
@article{abramo2011,
  title = {Evaluating Research: From Informed Peer Review to Bibliometrics},
  shorttitle = {Evaluating Research},
  author = {Abramo, Giovanni and D'Angelo, Ciriaco Andrea},
  year = {2011},
  month = jun,
  volume = {87},
  pages = {499--514},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-011-0352-7},
  abstract = {National research assessment exercises are becoming regular events in ever more countries. The present work contrasts the peer-review and bibliometrics approaches in the conduct of these exercises. The comparison is conducted in terms of the essential parameters of any measurement system: accuracy, robustness, validity, functionality, time and costs. Empirical evidence shows that for the natural and formal sciences, the bibliometric methodology is by far preferable to peer-review. Setting up national databases of publications by individual authors, derived from Web of Science or Scopus databases, would allow much better, cheaper and more frequent national research assessments.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2011/Abramo_D’Angelo_2011_Evaluating research.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {3}
}

@article{abramo2019,
  title = {Peer Review versus Bibliometrics: {{Which}} Method Better Predicts the Scholarly Impact of Publications?},
  shorttitle = {Peer Review versus Bibliometrics},
  author = {Abramo, Giovanni and D'Angelo, Ciriaco Andrea and Reale, Emanuela},
  year = {2019},
  month = oct,
  volume = {121},
  pages = {537--554},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-019-03184-y},
  abstract = {In this work, we try to answer the question of which method, peer review versus bibliometrics, better predicts the future overall scholarly impact of scientific publications. We measure the agreement between peer review evaluations of Web of Science indexed publications submitted to the first Italian research assessment exercise and long-term citations of the same publications. We do the same for an early citation-based indicator. We find that the latter shows stronger predictive power, i.e. it more reliably predicts late citations in all the disciplinary areas examined, and for any citation time window starting 1 year after publication.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2019/Abramo et al_2019_Peer review versus bibliometrics.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {1}
}

@article{belter2015,
  title = {Bibliometric Indicators: Opportunities and Limits},
  shorttitle = {Bibliometric Indicators},
  author = {Belter, Christopher W.},
  year = {2015},
  month = oct,
  volume = {103},
  pages = {219--221},
  issn = {1536-5050},
  doi = {10.3163/1536-5050.103.4.014},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of the Medical Library Association  JMLA/2015/Belter_2015_Bibliometric indicators.pdf},
  journal = {Journal of the Medical Library Association : JMLA},
  keywords = {fav},
  number = {4},
  pmcid = {PMC4613388},
  pmid = {26512227}
}

@article{butler2017,
  title = {The {{Evolution}} of {{Current Research Impact Metrics}}: {{From Bibliometrics}} to {{Altmetrics}}?},
  shorttitle = {The {{Evolution}} of {{Current Research Impact Metrics}}},
  author = {Butler, Joseph S. and Kaye, I. David and Sebastian, Arjun S. and Wagner, Scott C. and Morrissey, Patrick B. and Schroeder, Gregory D. and Kepler, Christopher K. and Vaccaro, Alexander R.},
  year = {2017},
  month = jun,
  volume = {30},
  pages = {226--228},
  issn = {2380-0186},
  doi = {10.1097/BSD.0000000000000531},
  abstract = {The prestige of publication has been based on traditional citation metrics, most commonly journal impact factor. However, the Internet has radically changed the speed, flow, and sharing of medical information. Furthermore, the explosion of social media, along with development of popular professional and scientific websites and blogs, has led to the need for alternative metrics, known as altmetrics, to quantify the wider impact of research. We explore the evolution of current research impact metrics and examine the evolving role of altmetrics in measuring the wider impact of research. We suggest that altmetrics used in research evaluation should be part of an informed peer-review process such as traditional metrics. Moreover, results based on altmetrics must not lead to direct decision making about research, but instead, should be used to assist experts in making decisions. Finally, traditional and alternative metrics should complement, not replace, each other in the peer-review process.},
  file = {/home/gabriel/Dropbox/zotero-library/Clinical Spine Surgery/2017/Butler et al_2017_The Evolution of Current Research Impact Metrics.pdf},
  journal = {Clinical Spine Surgery},
  language = {en-US},
  number = {5}
}

@article{cabanac2021,
  title = {Day-to-Day Discovery of Preprint\textendash Publication Links},
  author = {Cabanac, Guillaume and Oikonomidi, Theodora and Boutron, Isabelle},
  year = {2021},
  month = apr,
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03900-7},
  abstract = {Preprints promote the open and fast communication of non-peer reviewed work. Once a preprint is published in a peer-reviewed venue, the preprint server updates its web page: a prominent hyperlink leading to the newly published work is added. Linking preprints to publications is of utmost importance as it provides readers with the latest version of a now certified work. Yet leading preprint servers fail to identify all existing preprint\textendash publication links. This limitation calls for a more thorough approach to this critical information retrieval task: overlooking published evidence translates into partial and even inaccurate systematic reviews on health-related issues, for instance. We designed an algorithm leveraging the Crossref public and free source of bibliographic metadata to comb the literature for preprint\textendash publication links. We tested it on a reference preprint set identified and curated for a living systematic review on interventions for preventing and treating COVID-19 performed by international collaboration: the COVID-NMA initiative (covid-nma.com). The reference set comprised 343 preprints, 121 of which appeared as a publication in a peer-reviewed journal. While the preprint servers identified 39.7\% of the preprint\textendash publication links, our linker identified 90.9\% of the expected links with no clues taken from the preprint servers. The accuracy of the proposed linker is 91.5\% on this reference set, with 90.9\% sensitivity and 91.9\% specificity. This is a 16.26\% increase in accuracy compared to that of preprint servers. We release this software as supplementary material to foster its integration into preprint servers' workflows and enhance a daily preprint\textendash publication chase that is useful to all readers, including systematic reviewers. This preprint\textendash publication linker currently provides day-to-day updates to the biomedical experts of the COVID-NMA initiative.},
  file = {/home/gabriel/Dropbox/zotero-library/Cabanac et al_2021_Day-to-day discovery of preprint–publication links.pdf},
  journal = {Scientometrics},
  language = {en}
}

@article{cabezas-clavijo2013,
  title = {Reviewers' Ratings and Bibliometric Indicators: Hand in Hand When Assessing over Research Proposals?},
  shorttitle = {Reviewers' Ratings and Bibliometric Indicators},
  author = {{Cabezas-Clavijo}, Alvaro and {Robinson-Garc{\'i}a}, Nicol{\'a}s and Escabias, Manuel and {Jim{\'e}nez-Contreras}, Evaristo},
  year = {2013},
  volume = {8},
  pages = {e68258},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0068258},
  abstract = {BACKGROUND: The peer review system has been traditionally challenged due to its many limitations especially for allocating funding. Bibliometric indicators may well present themselves as a complement. OBJECTIVE: We analyze the relationship between peers' ratings and bibliometric indicators for Spanish researchers in the 2007 National R\&D Plan for 23 research fields. METHODS AND MATERIALS: We analyze peers' ratings for 2333 applications. We also gathered principal investigators' research output and impact and studied the differences between accepted and rejected applications. We used the Web of Science database and focused on the 2002-2006 period. First, we analyzed the distribution of granted and rejected proposals considering a given set of bibliometric indicators to test if there are significant differences. Then, we applied a multiple logistic regression analysis to determine if bibliometric indicators can explain by themselves the concession of grant proposals. RESULTS: 63.4\% of the applications were funded. Bibliometric indicators for accepted proposals showed a better previous performance than for those rejected; however the correlation between peer review and bibliometric indicators is very heterogeneous among most areas. The logistic regression analysis showed that the main bibliometric indicators that explain the granting of research proposals in most cases are the output (number of published articles) and the number of papers published in journals that belong to the first quartile ranking of the Journal Citations Report. DISCUSSION: Bibliometric indicators predict the concession of grant proposals at least as well as peer ratings. Social Sciences and Education are the only areas where no relation was found, although this may be due to the limitations of the Web of Science's coverage. These findings encourage the use of bibliometric indicators as a complement to peer review in most of the analyzed areas.},
  file = {/home/gabriel/Dropbox/zotero-library/PloS One/2013/Cabezas-Clavijo et al_2013_Reviewers' ratings and bibliometric indicators.pdf},
  journal = {PloS One},
  keywords = {Bibliometrics,Biomedical Research,Databases; Factual,fav,Financing; Organized,Humans,Publications,Publishing,Research Design,Research Personnel,Spain},
  language = {eng},
  number = {6},
  pmcid = {PMC3695904},
  pmid = {23840840}
}

@article{chahrour2020,
  title = {A {{Bibliometric Analysis}} of {{COVID}}-19 {{Research Activity}}: {{A Call}} for {{Increased Output}}},
  shorttitle = {A {{Bibliometric Analysis}} of {{COVID}}-19 {{Research Activity}}},
  author = {Chahrour, Mohamad and Assi, Sahar and Bejjani, Michael and Nasrallah, Ali A and Salhab, Hamza and Fares, Mohamad Y and Khachfe, Hussein H},
  year = {2020},
  month = mar,
  issn = {2168-8184},
  doi = {10.7759/cureus.7357},
  abstract = {Background: The novel coronavirus disease 2019 (COVID-19) has impacted many countries across all inhabited continents, and is now considered a global pandemic, due to its high rate of infectivity. Research related to this disease is pivotal for assessing pathogenic characteristics and formulating therapeutic strategies. The aim of this paper is to explore the activity and trends of COVID-19 research since its outbreak in December 2019. Methods: We explored the PubMed database and the World Health Organization (WHO) database for publications pertaining to COVID-19 since December 2019 up until March 18, 2020. Only relevant observational and interventional studies were included in our study. Data on COVID-19 incidence were extracted from the WHO situation reports. Research output was assessed with respect to gross domestic product (GDP) and population of each country. Results: Only 564 publications met our inclusion criteria. These articles came from 39 different countries, constituting 24\% of all affected countries. China produced the greatest number of publications with 377 publications (67\%). With respect to continental research activity, Asian countries had the highest research activity with 434 original publications (77\%). In terms of publications per million persons (PPMPs), Singapore had the highest number of publications with 1.069 PPMPs. In terms of publications per billion-dollar GDP, Mauritius ranked first with 0.075. Received 03/19/2020 Review began 03/20/2020 Review ended 03/20/2020 Published 03/21/2020 \textcopyright{} Copyright 2020 Chahrour et al. This is an open access article distributed under the terms of the Creative Commons Attribution License CC-BY 4.0., which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Conclusion: COVID-19 is a major disease that has impacted international public health on a global level. Observational studies and therapeutic trials pertaining to COVID-19 are essential for assessing pathogenic characteristics and developing novel treatment options.},
  file = {/home/gabriel/Dropbox/zotero-library/Cureus/2020/Chahrour et al_2020_A Bibliometric Analysis of COVID-19 Research Activity.pdf},
  journal = {Cureus},
  language = {en}
}

@article{craig2007,
  title = {Do Open Access Articles Have Greater Citation Impact?: {{A}} Critical Review of the Literature},
  shorttitle = {Do Open Access Articles Have Greater Citation Impact?},
  author = {Craig, Iain D. and Plume, Andrew M. and McVeigh, Marie E. and Pringle, James and Amin, Mayur},
  year = {2007},
  month = jul,
  volume = {1},
  pages = {239--248},
  issn = {1751-1577},
  doi = {10.1016/j.joi.2007.04.001},
  abstract = {The last few years have seen the emergence of several open access options in scholarly communication which can broadly be grouped into two areas referred to as `gold' and `green' open access (OA). In this article we review the literature examining the relationship between OA status and citation counts of scholarly articles. Early studies showed a correlation between the free online availability or OA status of articles and higher citation counts, and implied causality without due consideration of potential confounding factors. More recent investigations have dissected the nature of the relationship between article OA status and citations. Three non-exclusive postulates have been proposed to account for the observed citation differences between OA and non-OA articles: an open access postulate, a selection bias postulate, and an early view postulate. The most rigorous study to date (in condensed matter physics) showed that, after controlling for the early view postulate, the remaining difference in citation counts between OA and non-OA articles is explained by the selection bias postulate. No evidence was found to support the OA postulate per se; i.e. article OA status alone has little or no effect on citations. Further studies using a similarly rigorous approach are required to determine the generality of this finding.},
  journal = {Journal of Informetrics},
  keywords = {Citation analysis,Early view,Open access,Quality bias,todo},
  language = {en},
  number = {3},
  series = {The {{Hirsch Index}}}
}

@article{durieux2010,
  title = {Bibliometric {{Indicators}}: {{Quality Measurements}} of {{Scientific Publication}}},
  shorttitle = {Bibliometric {{Indicators}}},
  author = {Durieux, Val{\'e}rie and Gevenois, Pierre Alain},
  year = {2010},
  month = apr,
  volume = {255},
  pages = {342--351},
  publisher = {{Radiological Society of North America}},
  issn = {0033-8419},
  doi = {10.1148/radiol.09090626},
  abstract = {Bibliometrics is a set of mathematical and statistical methods used to analyze and measure the quantity and quality of books, articles, and other forms of publications. There are three types of bibliometric indicators: quantity indicators, which measure the productivity of a particular researcher; quality indicators, which measure the quality (or ``performance'') of a researcher's output; and structural indicators, which measure connections between publications, authors, and areas of research. Bibliometric indicators are especially important for researchers and organizations, as these measurements are often used in funding decisions, appointments, and promotions of researchers. As more and more scientific discoveries occur and published research results are read and then quoted by other researchers, bibliometric indicators are becoming increasingly important. This article provides an overview of the currently used bibliometric indicators and summarizes the critical elements and characteristics one should be aware of when evaluating the quantity and quality of scientific output.\textcopyright{} RSNA, 2010},
  file = {/home/gabriel/Dropbox/zotero-library/Radiology/2010/Durieux_Gevenois_2010_Bibliometric Indicators.pdf},
  journal = {Radiology},
  keywords = {fav},
  number = {2}
}

@article{eysenbach2006,
  title = {Citation {{Advantage}} of {{Open Access Articles}}},
  author = {Eysenbach, Gunther},
  year = {2006},
  month = may,
  volume = {4},
  pages = {e157},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0040157},
  abstract = {Open access (OA) to the research literature has the potential to accelerate recognition and dissemination of research findings, but its actual effects are controversial. This was a longitudinal bibliometric analysis of a cohort of OA and non-OA articles published between June 8, 2004, and December 20, 2004, in the same journal (PNAS: Proceedings of the National Academy of Sciences). Article characteristics were extracted, and citation data were compared between the two groups at three different points in time: at ``quasi-baseline'' (December 2004, 0\textendash 6 mo after publication), in April 2005 (4\textendash 10 mo after publication), and in October 2005 (10\textendash 16 mo after publication). Potentially confounding variables, including number of authors, authors' lifetime publication count and impact, submission track, country of corresponding author, funding organization, and discipline, were adjusted for in logistic and linear multiple regression models. A total of 1,492 original research articles were analyzed: 212 (14.2\% of all articles) were OA articles paid by the author, and 1,280 (85.8\%) were non-OA articles. In April 2005 (mean 206 d after publication), 627 (49.0\%) of the non-OA articles versus 78 (36.8\%) of the OA articles were not cited (relative risk = 1.3 [95\% Confidence Interval: 1.1\textendash 1.6]; p = 0.001). 6 mo later (mean 288 d after publication), non-OA articles were still more likely to be uncited (non-OA: 172 [13.6\%], OA: 11 [5.2\%]; relative risk = 2.6 [1.4\textendash 4.7]; p {$<$} 0.001). The average number of citations of OA articles was higher compared to non-OA articles (April 2005: 1.5 [SD = 2.5] versus 1.2 [SD = 2.0]; Z = 3.123; p = 0.002; October 2005: 6.4 [SD = 10.4] versus 4.5 [SD = 4.9]; Z = 4.058; p {$<$} 0.001). In a logistic regression model, controlling for potential confounders, OA articles compared to non-OA articles remained twice as likely to be cited (odds ratio = 2.1 [1.5\textendash 2.9]) in the first 4\textendash 10 mo after publication (April 2005), with the odds ratio increasing to 2.9 (1.5\textendash 5.5) 10\textendash 16 mo after publication (October 2005). Articles published as an immediate OA article on the journal site have higher impact than self-archived or otherwise openly accessible OA articles. We found strong evidence that, even in a journal that is widely available in research libraries, OA articles are more immediately recognized and cited by peers than non-OA articles published in the same journal. OA is likely to benefit science by accelerating dissemination and uptake of research findings.},
  file = {/home/gabriel/Dropbox/zotero-library/Eysenbach_2006_Citation Advantage of Open Access Articles.pdf},
  journal = {PLOS Biology},
  keywords = {Bibliometrics,Citation analysis,Institutional repositories,Internet,Open access publishing,Peer review,Scientific publishing,Scientists,todo},
  language = {en},
  number = {5}
}

@article{garner2018,
  title = {Bibliometric Indices: Defining Academic Productivity and Citation Rates of Researchers, Departments and Journals},
  shorttitle = {Bibliometric Indices},
  author = {Garner, Rebecca M. and Hirsch, Joshua A. and Albuquerque, Felipe C. and Fargen, Kyle M.},
  year = {2018},
  month = feb,
  volume = {10},
  pages = {102--106},
  issn = {1759-8486},
  doi = {10.1136/neurintsurg-2017-013265},
  abstract = {There has been an increasing focus on academic productivity for the purposes of promotion and funding within departments and institutions but also for comparison of individuals, institutions, specialties, and journals. A number of quantitative indices are used to investigate and compare academic productivity. These include various calculations attempting to analyze the number and citations of publications in order to capture both the quality and quantity of publications, such as the h index, the e index, impact factor, and Eigenfactor score. The indices have varying advantages and limitations and thus a basic knowledge is required in order to understand their potential utility within academic medicine. This article describes the various bibliometric indices and discusses recent applications of these metrics within the neurological sciences.},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of Neurointerventional Surgery/2018/Garner et al_2018_Bibliometric indices.pdf},
  journal = {Journal of Neurointerventional Surgery},
  keywords = {academic productivity,bibliometrics,Bibliometrics,Biomedical Research,citation analysis,Efficiency,fav,h index,Humans,Journal Impact Factor,Neurosciences,Periodicals as Topic},
  language = {eng},
  number = {2},
  pmid = {28824008}
}

@article{geuna2003,
  title = {University {{Research Evaluation}} and {{Funding}}: {{An International Comparison}}},
  shorttitle = {University {{Research Evaluation}} and {{Funding}}},
  author = {Geuna, Aldo and Martin, Ben R.},
  year = {2003},
  volume = {41},
  pages = {277--304},
  issn = {0026-4695},
  doi = {10.1023/B:MINE.0000005155.70870.bd},
  abstract = {Many countries have introduced evaluations of university research, reflecting global demands for greater accountability. This paper compares methods of evaluation used across twelve countries in Europe and the Asia-Pacific region. On the basis of this comparison, and focusing in particular on Britain, we examine the advantages and disadvantages of performance-based funding in comparison with other approaches to funding. Our analysis suggests that, while initial benefits may outweigh the costs, over time such a system seems to produce diminishing returns. This raises important questions about its continued use.},
  file = {/home/gabriel/Dropbox/zotero-library/Minerva/2003/Geuna_Martin_2003_University Research Evaluation and Funding.pdf},
  journal = {Minerva},
  language = {en},
  number = {4}
}

@book{gingras2016,
  title = {Bibliometrics and {{Research Evaluation}}: {{Uses}} and {{Abuses}}},
  shorttitle = {Bibliometrics and {{Research Evaluation}}},
  author = {Gingras, Yves},
  year = {2016},
  month = sep,
  publisher = {{MIT Press}},
  abstract = {Why bibliometrics is useful for understanding the global dynamics of science but generate perverse effects when applied inappropriately in research evaluation and university rankings.The research evaluation market is booming. ``Ranking,'' ``metrics,'' ``h-index,'' and ``impact factors'' are reigning buzzwords. Government and research administrators want to evaluate everything\textemdash teachers, professors, training programs, universities\textemdash using quantitative indicators. Among the tools used to measure ``research excellence,'' bibliometrics\textemdash aggregate data on publications and citations\textemdash has become dominant. Bibliometrics is hailed as an ``objective'' measure of research quality, a quantitative measure more useful than ``subjective'' and intuitive evaluation methods such as peer review that have been used since scientific papers were first published in the seventeenth century. In this book, Yves Gingras offers a spirited argument against an unquestioning reliance on bibliometrics as an indicator of research quality. Gingras shows that bibliometric rankings have no real scientific validity, rarely measuring what they pretend to.Although the study of publication and citation patterns, at the proper scales, can yield insights on the global dynamics of science over time, ill-defined quantitative indicators often generate perverse and unintended effects on the direction of research. Moreover, abuse of bibliometrics occurs when data is manipulated to boost rankings. Gingras looks at the politics of evaluation and argues that using numbers can be a way to control scientists and diminish their autonomy in the evaluation process. Proposing precise criteria for establishing the validity of indicators at a given scale of analysis, Gingras questions why universities are so eager to let invalid indicators influence their research strategy.},
  googlebooks = {0aExDQAAQBAJ},
  isbn = {978-0-262-33766-3},
  keywords = {Education / Evaluation \& Assessment,Language Arts \& Disciplines / Library \& Information Science / General},
  language = {en}
}

@article{glanzel2002,
  title = {Journal Impact Measures in Bibliometric Research},
  author = {Gl{\"a}nzel, Wolfgang and Moed, Henk F},
  year = {2002},
  pages = {23},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/2002/Glänzel_Moed_2002_Journal impact measures in bibliometric research.pdf},
  language = {en}
}

@article{glanzel2008,
  title = {Seven {{Myths}} in {{Bibliometrics About}} Facts and Fiction in Quantitative Science Studies},
  author = {Gl{\"a}nzel, Wolfgang},
  year = {2008},
  month = jun,
  volume = {2},
  pages = {9--17},
  issn = {0973-7766, 2168-930X},
  doi = {10.1080/09737766.2008.10700836},
  file = {/home/gabriel/Dropbox/zotero-library/Collnet Journal of Scientometrics and Information Management/2008/Glänzel_2008_Seven Myths in Bibliometrics About facts and fiction in quantitative science.pdf},
  journal = {Collnet Journal of Scientometrics and Information Management},
  keywords = {fav},
  language = {en},
  number = {1}
}

@article{gonzalez-betancor2019,
  title = {Publication Modalities `Article in Press' and `Open Access' in Relation to Journal Average Citation},
  author = {{Gonz{\'a}lez-Betancor}, Sara M. and {Dorta-Gonz{\'a}lez}, Pablo},
  year = {2019},
  month = sep,
  volume = {120},
  pages = {1209--1223},
  issn = {1588-2861},
  doi = {10.1007/s11192-019-03156-2},
  abstract = {There has been a generalization in the use of two publication practices by scientific journals during the past decade: (1) `article in press' or early view, which allows access to the accepted paper before its formal publication in an issue; (2) `open access', which allows readers to obtain it freely and free of charge. This paper studies the influence of both publication modalities on the average impact of the journal and its evolution over time. It tries to identify the separate effect of access on citation into two major parts: early view and selection effect, managing to provide some evidence of the positive effect of both. Scopus is used as the database and CiteScore as the measure of journal impact. The prevalence of both publication modalities is quantified. Differences in the average impact factor of group of journals, according to their publication modalities, are tested. The evolution over time of the citation influence, from 2011 to 2016, is also analysed. Finally, a linear regression to explain the correlation of these publication practices with the CiteScore in 2016, in a ceteris paribus context, is estimated. Our main findings show evidence of a positive correlation between average journal impact and advancing the publication of accepted articles, moreover this correlation increases over time. The open access modality, in a ceteris paribus context, also correlates positively with average journal impact.},
  file = {/home/gabriel/Dropbox/zotero-library/González-Betancor_Dorta-González_2019_Publication modalities ‘article in press’ and ‘open access’ in relation to.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {3}
}

@article{haeffner-cavaillon2009,
  title = {The Use of Bibliometric Indicators to Help Peer-Review Assessment},
  author = {{Haeffner-Cavaillon}, Nicole and {Graillot-Gak}, Claude},
  year = {2009 Jan-Feb},
  volume = {57},
  pages = {33--38},
  issn = {1661-4917},
  doi = {10.1007/s00005-009-0004-2},
  abstract = {Inserm is the only French public research institution entirely dedicated to human health. Inserm supports research across the biomedical spectrum in all major disease areas, from fundamental lab-based science to clinical trials. To translate its scientists' findings into tangible health benefits, Inserm has its own affiliated company, Inserm Transfert, which works with industry. Since 2001, Inserm has been setting up on-line file management software for the evaluation of researchers and laboratories, called EVA ( www.eva.inserm.fr ). EVA includes all grant applications, assessment reports, evaluation grading evaluation forms and includes automated bibliometric indicator software that enables calculating, for example, the number of publications, journal impact factors, number of citations, citation index, and number of the Top 1 publications for each researcher of the teams. The indicators take into account research fields, the year of publications, and the author's position among the participants. Bibliometrics is now considered a tool for science policy providing indicators to measure productivity and scientific quality, thereby supplying a basis for evaluating and orienting R\&D. It is also a potential tool for evaluation. It is neutral, allows comparative (national and international) assessment, and may select papers in the forefront in all fields. For each team, bibliometric indicators are calculated for all researchers with permanent or long-term positions (3-5 years). The use of bibliometric indicators requires great vigilance, but according to our experience they enrich the committee's debates without any doubt. We present an analysis of the data of 600 research teams evaluated in 2007-2008.},
  file = {/home/gabriel/Dropbox/zotero-library/Archivum Immunologiae Et Therapiae Experimentalis/2009/Haeffner-Cavaillon_Graillot-Gak_2009_The use of bibliometric indicators to help peer-review assessment.pdf},
  journal = {Archivum Immunologiae Et Therapiae Experimentalis},
  keywords = {Academies and Institutes,Bibliometrics,Biomedical Research,Evaluation Studies as Topic,fav,Financing; Organized,France,Health Care Sector,Information Management,Journal Impact Factor,Laboratories,Peer Review; Research,Periodicals as Topic,Policy Making,Publishing,Quality Control,Software},
  language = {eng},
  number = {1},
  pmcid = {PMC3957005},
  pmid = {19219530}
}

@article{hajjem2006,
  title = {Ten-{{Year Cross}}-{{Disciplinary Comparison}} of the {{Growth}} of {{Open Access}} and {{How}} It {{Increases Research Citation Impact}}},
  author = {Hajjem, C. and Harnad, S. and Gingras, Y.},
  year = {2006},
  month = aug,
  abstract = {Lawrence (2001)found computer science articles that were openly accessible (OA) on the Web were cited more. We replicated this in physics. We tested 1,307,038 articles published across 12 years (1992-2003) in 10 disciplines (Biology, Psychology, Sociology, Health, Political Science, Economics, Education, Law, Business, Management). A robot trawls the Web for full-texts using reference metadata ISI citation data (signal detectability d'=2.45; bias = 0.52). Percentage OA (relative to total OA + NOA) articles varies from 5\%-16\% (depending on discipline, year and country) and is slowly climbing annually (correlation r=.76, sample size N=12, probability p {$<$} 0.005). Comparing OA and NOA articles in the same journal/year, OA articles have consistently more citations, the advantage varying from 36\%-172\% by discipline and year. Comparing articles within six citation ranges (0, 1, 2-3, 4-7, 8-15, 16+ citations), the annual percentage of OA articles is growing significantly faster than NOA within every citation range (r {$>$} .90, N=12, p {$<$} .0005) and the effect is greater with the more highly cited articles (r = .98, N=6, p {$<$} .005). Causality cannot be determined from these data, but our prior finding of a similar pattern in physics, where percent OA is much higher (and even approaches 100\% in some subfields), makes it unlikely that the OA citation advantage is merely or mostly a self-selection bias (for making only one's better articles OA). Further research will analyze the effect's timing, causal components and relation to other variables.},
  archiveprefix = {arXiv},
  eprint = {cs/0606079},
  eprinttype = {arxiv},
  file = {/home/gabriel/Dropbox/zotero-library/Hajjem et al_2006_Ten-Year Cross-Disciplinary Comparison of the Growth of Open Access and How it.pdf},
  journal = {arXiv:cs/0606079},
  keywords = {Computer Science - Digital Libraries,todo}
}

@article{hammarfelt2018,
  title = {What Is a Discipline? {{The}} Conceptualization of Research Areas and Their Operationalization in Bibliometric Research},
  author = {Hammarfelt, Bj{\"o}rn},
  year = {2018},
  pages = {8},
  abstract = {This paper highlights disadvantages of conceptual impreciseness, and advocates further attention to the labels and concepts used when classifying clusters or groups based on bibliographic data. The main focus of the analysis is on the concept of `discipline' and how it is used in bibliometric research, but the implications concern a broader array of related terms.},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/2018/Hammarfelt_2018_What is a discipline.pdf},
  language = {en}
}

@article{hc,
  title = {Is the Journal Impact Factor a Valid Indicator of Scientific Value?},
  author = {Hc, Oh},
  pages = {3},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/undefined/Hc_Is the journal impact factor a valid indicator of scientific value.pdf},
  keywords = {fav},
  language = {en}
}

@article{jack2021,
  title = {Scientific Knowledge Production and Economic Catching-up: An Empirical Analysis},
  shorttitle = {Scientific Knowledge Production and Economic Catching-Up},
  author = {Jack, Pablo and Lachman, Jeremias and L{\'o}pez, Andr{\'e}s},
  year = {2021},
  month = apr,
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03973-4},
  abstract = {This paper aims to investigate the relationship between the production of scientific knowledge and level of income for a panel of 56 countries during the period 1996\textendash 2015. We argue that the accumulation of scientific knowledge is a key factor for the enhancement of educational and technological capabilities within an economy, and hence may have a positive impact on GDP per capita levels. We use academic publications in refereed journals (in all areas and specifically in engineering) as a proxy of scientific performance. As regards the impacts of scientific performance, we distinguish between high- and middle-income countries and, among the latter, between Asian and Latin America. The results show that academic publications are consistently and positively correlated with income per capita, for both middle and high-income countries. We also find non-linear effects in both groups. Those effects are lower for middle-income countries suggesting the presence of decreasing returns on academic performance. Finally, while Asian countries benefited from specialization in engineering research, no such effects were found for their Latin American peers.},
  file = {/home/gabriel/Dropbox/zotero-library/Jack et al_2021_Scientific knowledge production and economic catching-up.pdf},
  journal = {Scientometrics},
  language = {en}
}

@article{jappe2018,
  title = {Does Bibliometric Research Confer Legitimacy to Research Assessment Practice? {{A}} Sociological Study of Reputational Control, 1972-2016},
  shorttitle = {Does Bibliometric Research Confer Legitimacy to Research Assessment Practice?},
  author = {Jappe, Arlette and Pithan, David and Heinze, Thomas},
  editor = {Lozano, Sergi},
  year = {2018},
  month = jun,
  volume = {13},
  pages = {e0199031},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0199031},
  abstract = {The use of bibliometric measures in the evaluation of research has increased considerably based on expertise from the growing research field of evaluative citation analysis (ECA). However, mounting criticism of such metrics suggests that the professionalization of bibliometric expertise remains contested. This paper investigates why impact metrics, such as the journal impact factor and the h-index, proliferate even though their legitimacy as a means of professional research assessment is questioned. Our analysis is informed by two relevant sociological theories: Andrew Abbott's theory of professions and Richard Whitley's theory of scientific work. These complementary concepts are connected in order to demonstrate that ECA has failed so far to provide scientific authority for professional research assessment. This argument is based on an empirical investigation of the extent of reputational control in the relevant research area. Using three measures of reputational control that are computed from longitudinal inter-organizational networks in ECA (1972\textendash 2016), we show that peripheral and isolated actors contribute the same number of novel bibliometric indicators as central actors. In addition, the share of newcomers to the academic sector has remained high. These findings demonstrate that recent methodological debates in ECA have not been accompanied by the formation of an intellectual field in the sociological sense of a reputational organization. Therefore, we conclude that a growing gap exists between an academic sector with little capacity for collective action and increasing demand for routine performance assessment by research organizations and funding agencies. This gap has been filled by database providers. By selecting and distributing research metrics, these commercial providers have gained a powerful role in defining de-facto standards of research excellence without being challenged by expert authority.},
  file = {/home/gabriel/Dropbox/zotero-library/PLOS ONE/2018/Jappe et al_2018_Does bibliometric research confer legitimacy to research assessment practice.pdf},
  journal = {PLOS ONE},
  keywords = {fav},
  language = {en},
  number = {6}
}

@article{jarneving2005,
  title = {A Comparison of Two Bibliometric Methods for Mapping of the Research Front},
  author = {Jarneving, Bo},
  year = {2005},
  month = nov,
  volume = {65},
  pages = {245--263},
  issn = {1588-2861},
  doi = {10.1007/s11192-005-0270-7},
  abstract = {This paper builds on previous research concerned with the classification and specialty mapping of research fields. Two methods are put to test in order to decide if significant differences as to mapping results of the research front of a science field occur when compared. The first method was based on document co-citation analysis where papers citing co-citation clusters were assumed to reflect the research front. The second method was bibliographic coupling where likewise citing papers were assumed to reflect the research front. The application of these methods resulted in two different types of aggregations of papers: (1) groups of papers citing clusters of co-cited works and (2) clusters of bibliographically coupled papers. The comparision of the two methods as to mapping results was pursued by matching word profiles of groups of papers citing a particular co-citation cluster with word profiles of clusters of bibliographically coupled papers. Findings suggested that the research front was portrayed in two considerably different ways by the methods applied. It was concluded that the results in this study would support a further comparative study of these methods on a more detailed and qualitative ground. The original data set encompassed 73,379 articles from the fifty most cited environmental science journals listed in Journal Citation Report, science edition downloaded from the Science Citation Index on CD-ROM.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2005/Jarneving_2005_A comparison of two bibliometric methods for mapping of the research front.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {2}
}

@article{joshi2014,
  title = {Bibliometric Indicators for Evaluating the Quality of Scientifc Publications},
  author = {Joshi, Medha A.},
  year = {2014},
  month = mar,
  volume = {15},
  pages = {258--262},
  issn = {1526-3711},
  doi = {10.5005/jp-journals-10024-1525},
  abstract = {Evaluation of quality and quantity of publications can be done using a set of statistical and mathematical indices called bibliometric indicators. Two major categories of indicators are (1) quantitative indicators that measure the research productivity of a researcher and (2) performance indicators that evaluate the quality of publications. Bibliometric indicators are important for both the individual researcher and organizations. They are widely used to compare the performance of the individual researchers, journals and universities. Many of the appointments, promotions and allocation of research funds are based on these indicators. This review article describes some of the currently used bibliometric indicators such as journal impact factor, crown indicator, h-index and it's variants. It is suggested that for comparison of scientific impact and scientific output of researchers due consideration should be given to various factors affecting theses indicators.},
  file = {/home/gabriel/Dropbox/zotero-library/The Journal of Contemporary Dental Practice/2014/Joshi_2014_Bibliometric indicators for evaluating the quality of scientifc publications.pdf},
  journal = {The Journal of Contemporary Dental Practice},
  keywords = {Authorship,Bibliometrics,Dental Research,fav,Humans,Journal Impact Factor,Periodicals as Topic,Publishing},
  language = {eng},
  number = {2},
  pmid = {25095854}
}

@article{kamrani,
  title = {Do Researchers Know What the H-Index Is? {{And}} How Do They Estimate Its Importance?},
  author = {Kamrani, Pantea},
  pages = {20},
  abstract = {The h-index is a widely used scientometric indicator on the researcher level working with a simple combination of publication and citation counts. In this article, we pursue two goals, namely the collection of empirical data about researchers' personal estimations of the importance of the h-index for themselves as well as for their academic disciplines, and on the researchers' concrete knowledge on the h-index and the way of its calculation. We worked with an online survey (including a knowledge test on the calculation of the h-index), which was finished by 1081 German university professors. We distinguished between the results for all participants, and, additionally, the results by gender, generation, and field of knowledge. We found a clear binary division between the academic knowledge fields: For the sciences and medicine the h-index is important for the researchers themselves and for their disciplines, while for the humanities and social sciences, economics, and law the h-index is considerably less important. Two fifths of the professors do not know details on the h-index or wrongly deem to know what the h-index is and failed our test. The researchers' knowledge on the h-index is much smaller in the academic branches of the humanities and the social sciences. As the h-index is important for many researchers and as not all researchers are very knowledgeable about this author-specific indicator, it seems to be necessary to make researchers more aware of scholarly metrics literacy.},
  file = {/home/gabriel/Dropbox/zotero-library/Kamrani_Do researchers know what the h-index is.pdf},
  language = {en}
}

@article{karanatsiou,
  title = {Bibliometrics and {{Altmetrics}} Literature Review: {{Performance}} Indicators and Comparison Analysis},
  author = {Karanatsiou, Dimitra and Misirlis, Nikolaos and Vlachopoulou, Maro},
  pages = {21},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/undefined/Karanatsiou et al_Bibliometrics and Altmetrics literature review.pdf},
  language = {en}
}

@article{kumar2009,
  title = {Impact of the Impact Factor in Biomedical Research: Its Use and Misuse},
  shorttitle = {Impact of the Impact Factor in Biomedical Research},
  author = {Kumar, V. and Upadhyay, S. and Medhi, B.},
  year = {2009},
  month = aug,
  volume = {50},
  pages = {752--755},
  issn = {0037-5675},
  abstract = {The impact factor was created in the biomedical research field in order to measure a journal's value by calculating the average number of citations per article over a period of time. It was initially developed to help libraries decide which highly-cited journals to subscribe to. However, at present, it is being misused to judge the quality of a researcher or medical scientist as well as the quality of the work done. It contains serious sources of errors and flaws, resulting in strong biases against culture- and language-bound medical subspecialties. The present article is aimed to highlight the impact of the impact factor in the biomedical research, as well as its use and misuse.},
  file = {/home/gabriel/Dropbox/zotero-library/Singapore Medical Journal/2009/Kumar et al_2009_Impact of the impact factor in biomedical research.pdf},
  journal = {Singapore Medical Journal},
  keywords = {Bibliometrics,Biomedical Research,Databases; Bibliographic,fav,Journal Impact Factor,Periodicals as Topic,Publishing,Reproducibility of Results,Time Factors},
  language = {eng},
  number = {8},
  pmid = {19710969}
}

@article{leite2011,
  title = {A New Indicator for International Visibility: Exploring {{Brazilian}} Scientific Community},
  shorttitle = {A New Indicator for International Visibility},
  author = {Leite, Paula and Mugnaini, Rog{\'e}rio and Leta, Jacqueline},
  year = {2011},
  month = apr,
  volume = {88},
  pages = {311--319},
  publisher = {{Akad\'emiai Kiad\'o, co-published with Springer Science+Business Media B.V., Formerly Kluwer Academic Publishers B.V.}},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-011-0379-9},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d49e2"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Brazilian science has increased fast during the last decades. An example is the increasing in the country's share in the world's scientific publication within the main international databases. But what is the actual weight of international publications to the whole Brazilian productivity? In order to respond this question, we have elaborated a new indicator, the International Publication Ratio (IPR). The data source was Lattes Database, a database organized by one of the main Brazilian S\&amp;T funding agency, which encompasses publication data from 1997 to 2004 of about 51,000 Brazilian researchers. Influences of distinct parameters, such as sectors, fields, career age and gender, are analyzed. We hope the data presented may help S\&amp;T managers and other S\&amp;T interests to better understand the complexity under the concept scientific productivity, especially in peripheral countries in science, such as Brazil.{$<$}/p{$><$}/section{$>$}},
  chapter = {Scientometrics},
  journal = {Scientometrics},
  language = {en\_US},
  number = {1}
}

@article{lucas-dominguez2021,
  title = {The Sharing of Research Data Facing the {{COVID}}-19 Pandemic},
  author = {{Lucas-Dominguez}, Rut and {Alonso-Arroyo}, Adolfo and {Vidal-Infer}, Antonio and {Aleixandre-Benavent}, Rafael},
  year = {2021},
  month = apr,
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03971-6},
  abstract = {During the previous Ebola and Zika outbreaks, researchers shared their data, allowing many published epidemiological studies to be produced only from open research data, to speed up investigations and control of these infections. This study aims to evaluate the dissemination of the COVID-19 research data underlying scientific publications. Analysis of COVID-19 publications from December 1, 2019, to April 30, 2020, was conducted through the PubMed Central repository to evaluate the research data available through its publication as supplementary material or deposited in repositories. The PubMed Central search generated 5,905 records, of which 804 papers included complementary research data, especially as supplementary material (77.4\%). The most productive journals were The New England Journal of Medicine, The Lancet and The Lancet Infectious Diseases, the most frequent keyword was pneumonia, and the most used repositories were GitHub and GenBank. An expected growth in the number of published articles following the course of the pandemics is confirmed in this work, while the underlying research data are only 13.6\%. It can be deduced that data sharing is not a common practice, even in health emergencies, such as the present one. High-impact generalist journals have accounted for a large share of global publishing. The topics most often covered are related to epidemiological and public health concepts, genetics, virology and respiratory diseases, such as pneumonia. However, it is essential to interpret these data with caution following the evolution of publications and their funding in the coming months.},
  file = {/home/gabriel/Dropbox/zotero-library/Lucas-Dominguez et al_2021_The sharing of research data facing the COVID-19 pandemic.pdf},
  journal = {Scientometrics},
  language = {en}
}

@article{lyu2021,
  title = {The Classification of Citing Motivations: A Meta-Synthesis},
  shorttitle = {The Classification of Citing Motivations},
  author = {Lyu, Dongqing and Ruan, Xuanmin and Xie, Juan and Cheng, Ying},
  year = {2021},
  month = apr,
  volume = {126},
  pages = {3243--3264},
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03908-z},
  abstract = {Citation analysis has been a prevalent method in the field of information science, especially research on bibliometrics and evaluation, but its validity relies heavily on how the citations are treated. It is essential to study authors' citing motivations to identify citations with different values and significance. This study applied a meta-synthesis approach to establish a new holistic classification of citation motivations based on previous studies. First, we used a four-step search strategy to identify related articles on authors' citing motivations. Thirty-eight primary studies were included after the inclusion and exclusion criteria were applied and appraised using the Evidence-based Librarianship checklist. Next, we decoded and recoded the citing motivations found in the included studies, following the standard procedures of meta-synthesis. Thirty-five descriptive concepts of citation motivations emerged, which were then synthesized into 13 analytic themes. As a result, we proposed a comprehensive classification, including two main categories of citing reasons, i.e., ``scientific motivations'' and ``tactical motivations.'' Generally, the citations driven by scientific motivations serve as a rhetorical function, while tactical motivations are social or benefit-oriented and not easily captured through text-parsing. Our synthesis contributes to bibliometric and scientific evaluation theory. The synthesized classification also provides a comprehensive and unified annotation schema for citation classification and helps identify the useful mentions of a reference in a citing paper to optimize citation- based measurements.},
  file = {/home/gabriel/Dropbox/zotero-library/Lyu et al_2021_The classification of citing motivations.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {4}
}

@article{merigo2017,
  title = {A Bibliometric Analysis of Operations Research and Management Science},
  author = {Merig{\'o}, Jos{\'e} M. and Yang, Jian-Bo},
  year = {2017},
  month = dec,
  volume = {73},
  pages = {37--48},
  issn = {03050483},
  doi = {10.1016/j.omega.2016.12.004},
  abstract = {Bibliometric analysis is the quantitative study of bibliographic material. It provides a general picture of a research field that can be classified by papers, authors and journals. This paper presents a bibliometric overview of research published in operations research and management science in recent decades. The main objective of this study is to identify some of the most relevant research in this field and some of the newest trends according to the information found in the Web of Science database. Several classifications are made, including an analysis of the most influential journals, the two hundred most cited papers of all time and the most productive and influential authors. The results obtained are in accordance with the common wisdom, although some variations are found.},
  file = {/home/gabriel/Dropbox/zotero-library/Omega/2017/Merigó_Yang_2017_A bibliometric analysis of operations research and management science.pdf},
  journal = {Omega},
  language = {en}
}

@article{momeni2021,
  title = {What Happens When a Journal Converts to Open Access? {{A}} Bibliometric Analysis},
  shorttitle = {What Happens When a Journal Converts to Open Access?},
  author = {Momeni, Fakhri and Mayr, Philipp and Fraser, Nicholas and Peters, Isabella},
  year = {2021},
  month = apr,
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03972-5},
  abstract = {In recent years, increased stakeholder pressure to transition research to Open Access has led to many journals converting, or `flipping', from a closed access (CA) to an open access (OA) publishing model. Changing the publishing model can influence the decision of authors to submit their papers to a journal, and increased article accessibility may influence citation behaviour. In this paper we aimed to understand how flipping a journal to an OA model influences the journal's future publication volumes and citation impact. We analysed two independent sets of journals that had flipped to an OA model, one from the Directory of Open Access Journals (DOAJ) and one from the Open Access Directory (OAD), and compared their development with two respective control groups of similar journals. For bibliometric analyses, journals were matched to the Scopus database. We assessed changes in the number of articles published over time, as well as two citation metrics at the journal and article level: the normalised impact factor (IF) and the average relative citations (ARC), respectively. Our results show that overall, journals that flipped to an OA model increased their publication output compared to journals that remained closed. Mean normalised IF and ARC also generally increased following the flip to an OA model, at a greater rate than was observed in the control groups. However, the changes appear to vary largely by scientific discipline. Overall, these results indicate that flipping to an OA publishing model can bring positive changes to a journal.},
  file = {/home/gabriel/Dropbox/zotero-library/Momeni et al_2021_What happens when a journal converts to open access.pdf},
  journal = {Scientometrics},
  language = {en}
}

@article{moura2017,
  title = {Uses of {{Bibliometric Techniques}} in {{Public Health Research}}},
  author = {MOURA, Luana Kelle Batista and {de MESQUITA}, Rafael Fernandes and MOBIN, Mitra and MATOS, Francisca Tereza Coelho and MONTE, Thiago Lima and LAGO, Eliana Campelo and FALC{\~A}O, Carlos Alberto Monteiro and {de Ar{\^e}a Le{\~a}o FERRAZ}, Maria {\^A}ngela and SANTOS, Tanit Clementino and SOUSA, Laelson Rochelle Milan{\^e}s},
  year = {2017},
  month = oct,
  volume = {46},
  pages = {1435--1436},
  issn = {2251-6085},
  file = {/home/gabriel/Dropbox/zotero-library/Iranian Journal of Public Health/2017/MOURA et al_2017_Uses of Bibliometric Techniques in Public Health Research.pdf},
  journal = {Iranian Journal of Public Health},
  number = {10},
  pmcid = {PMC5750357},
  pmid = {29308389}
}

@article{mugnaini2004,
  title = {{Indicadores bibliom\'etricos da produ\c{c}\~ao cient\'ifica brasileira: uma an\'alise a partir da base Pascal}},
  shorttitle = {{Indicadores bibliom\'etricos da produ\c{c}\~ao cient\'ifica brasileira}},
  author = {Mugnaini, Rog{\'e}rio and Jannuzzi, Paulo de Martino and Quoniam, Luc},
  year = {2004},
  month = aug,
  volume = {33},
  pages = {123--131},
  publisher = {{Instituto Brasileiro de Informa\c{c}\~ao em{$<$}br{$>$}Ci\^encia e Tecnologia - IBICT}},
  issn = {0100-1965},
  doi = {10.1590/S0100-19652004000200013},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini et al_2004_Indicadores bibliométricos da produção científica brasileira.pdf},
  journal = {Ci\^encia da Informa\c{c}\~ao},
  language = {pt},
  number = {2}
}

@phdthesis{mugnaini2006,
  title = {{Caminhos para adequa\c{c}\~ao da avalia\c{c}\~ao da produ\c{c}\~ao cient\'ifica brasileira: impacto nacional versus internacional}},
  shorttitle = {{Caminhos para adequa\c{c}\~ao da avalia\c{c}\~ao da produ\c{c}\~ao cient\'ifica brasileira}},
  author = {Mugnaini, Rog{\'e}rio},
  year = {2006},
  month = nov,
  doi = {10.11606/T.27.2006.tde-11052007-091052},
  abstract = {Diversos indicadores bibliom\'etricos t\^em sido empregados na avalia\c{c}\~ao de desempenho de pesquisadores, universidades e pa\'ises. Indicadores de impacto, calculados a partir das cita\c{c}\~oes recebidas pelos artigos, t\^em sido objeto de muitos estudos constantes da pesquisa document\'aria. Dessa maneira, almeja-se apontar poss\'iveis formas de adequa\c{c}\~ao da an\'alise do impacto de revistas brasileiras com vistas ao aprimoramento dos crit\'erios de avalia\c{c}\~ao de produ\c{c}\~ao cient\'ifica no Brasil. Objetivos. A pesquisa foi conduzida de acordo com tr\^es objetivos: (1) Verificar se o uso exclusivo de indicadores internacionais deixa a pol\'itica cient\'ifica brasileira fora do contexto de sua realidade local, e se o acesso \`as revistas indexadas nas bases do ISI (Thomson Scientific) tem se justificado pelo uso ? o acesso gratuito aos textos completos \'e oferecido \`a comunidade cient\'ifica pela Capes (Coordena\c{c}\~ao de Aperfei\c{c}oamento de Pessoal de N\'ivel Superior). (2) Investigar se a base SciELO pode oferecer indicadores de impacto da produ\c{c}\~ao cient\'ifica brasileira com vistas ao aprimoramento da avalia\c{c}\~ao cient\'ifica nacional. (3) Buscar propor metodologias de indicadores mais adequadas \`a realidade da ci\^encia brasileira. Metodologia. Foi conduzido um estudo explorat\'orio quantitativo, baseados em caracter\'isticas qualitativas e quantitativas de revistas cient\'ificas provenientes de tr\^es fontes: revistas classificadas pela Avalia\c{c}\~ao Qualis (tri\^enio 2001/2003), revistas do Portal de Peri\'odicos da Capes e revistas indexadas na base SciELO (Scientific Electronic Library). Uma compara\c{c}\~ao do impacto nacional e internacional de um conjunto de revistas brasileiras indexadas na base SciELO foi realizada a partir das cita\c{c}\~oes recebidas pelas revistas em cada contexto (base SciELO e as bases do ISI). Uma metodologia de an\'alise de revistas foi apresentada aplicando-se t\'ecnicas de an\'alise estat\'istica multivariada a um conjunto de 42 indicadores. Resultados. A an\'alise da Avalia\c{c}\~ao Qualis mostrou que os crit\'erios definidos favorecem principalmente a publica\c{c}\~ao em revistas internacionais e fazem uso do Fator de Impacto do ISI. O Impacto P\'os-Portal, como foi denominado, indicou um efeito positivo, notado pelo aumento da m\'edia de cita\c{c}\~oes recebidas na SciELO, por aproximadamente 70\% das revistas da amostra (Ci\^encias da Vida), ap\'os o ano de publica\c{c}\~ao no Portal. A compara\c{c}\~ao do impacto nacional versus internacional das revistas SciELO mostrou que revistas indexadas tamb\'em no ISI s\~ao citadas com mais freq\"u\^encia naquela base, al\'em de receberem aproximadamente 72\% das cita\c{c}\~oes de revistas ISI de autores estrangeiros e terem os artigos em colabora\c{c}\~ao (nacional e internacional) mais citados que aqueles em autoria \'unica. Em rela\c{c}\~ao \`as revistas publicadas somente na SciELO, verificou-se que s\~ao citadas em quantidades similares naquela base e nas bases do ISI, recebem 68\% das cita\c{c}\~oes de revistas ISI de autores estrangeiros e t\^em seus artigos de autoria \'unica mais citados, seguidos daqueles em colabora\c{c}\~ao nacional. A an\'alise multivariada dos indicadores das revistas SciELO permitiu a identifica\c{c}\~ao de diferentes grupos de revistas, discriminados de acordo com as diferentes pr\'aticas de comunica\c{c}\~ao cient\'ifica. Conclus\~oes. A adequa\c{c}\~ao dos crit\'erios utilizados na avalia\c{c}\~ao da produ\c{c}\~ao cient\'ifica nacional pode ser conseguida considerando-se indicadores de impacto mensurados a partir de cita\c{c}\~oes provenientes das revistas nacionais, definindo crit\'erios que valorizem a publica\c{c}\~ao em revistas nacionais de qualidade reconhecida, o que permitir\'a a publica\c{c}\~ao de trabalhos importantes na l\'ingua portuguesa, e estimular\'a o processo de melhoria de qualidade das revistas nacionais.},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini_2006_Caminhos para adequação da avaliação da produção científica brasileira.pdf},
  language = {pt-br},
  school = {Universidade de S\~ao Paulo},
  type = {{text}}
}

@article{mugnaini2010,
  title = {Multidisciplinaridade e Especificidade Na Comunica\c{c}\~ao Cient\'ifica: Discuss\~ao Do Impacto Na Avalia\c{c}\~ao de Diferentes \'Areas},
  shorttitle = {Multidisciplinaridade e Especificidade Na Comunica\c{c}\~ao Cient\'ifica},
  author = {Mugnaini, Rog{\'e}rio and Poblaci{\'o}n, Dinah Apparecida de Melo Aguiar},
  year = {2010},
  volume = {4},
  issn = {1981-6278},
  doi = {10.3395/reciis.v4i5.533},
  abstract = {As refer\^encias bibliogr\'aficas podem revelar o perfil da ci\^encia publicada, oferecendo importantes informa\c{c}\~oes sobre a hist\'oria de uma revista. Ao identificar o impacto dos diferentes tipos de documentos citados por cinco revistas cient\'ificas de \'areas diversas, constatou-se que o livro \'e consideravelmente mais citado numa revista de Ci\^encias Sociais Aplicadas, enquanto a \'area de Sa\'ude Coletiva faz uso deste tipo de documento em propor\c{c}\~oes equipar\'aveis com os artigos cient\'ificos. Nas revistas de F\'isica e Medicina as cita\c{c}\~oes a revistas internacionais s\~ao muito mais prevalentes. E na revista de Veterin\'aria e de Ci\^encia da Informa\c{c}\~ao, destacam-se os anais e teses. Estas constata\c{c}\~oes s\~ao importantes para entender as culturas de comunica\c{c}\~ao cient\'ifica das \'areas, o que p\^ode ser observado tamb\'em ao analisar, tanto as classifica\c{c}\~oes das revistas no Qualis, quanto os crit\'erios constantes dos documentos de \'area. Indicadores bibliom\'etricos n\~ao restritos a um \'indice s\~ao capazes de oferecer par\^ametros para cooperar na defini\c{c}\~ao de crit\'erios para avalia\c{c}\~ao da produ\c{c}\~ao cient\'ifica brasileira, segundo as caracter\'isticas das diferentes \'areas do conhecimento.},
  copyright = {Direitos autorais},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini_Población_2010_Multidisciplinaridade e especificidade na comunicação científica.pdf},
  journal = {Revista Eletr\^onica de Comunica\c{c}\~ao, Informa\c{c}\~ao e Inova\c{c}\~ao em Sa\'ude},
  keywords = {Assessment,Avaliação,Bases de dados,Bibliometric indicators,Citação.,Citation,Comunicação científica,Database,Indicadores bibliométricos,Scientific publication},
  language = {en},
  number = {5}
}

@incollection{mugnaini2013,
  title = {40 Anos de {{Bibliometria}} No {{Brasil}}: Da Bibliografia Estat\'istica \`a Avalia\c{c}\~ao Da Produ\c{c}\~ao Cient\'ifica Nacional},
  shorttitle = {40 Anos de {{Bibliometria}} No {{Brasil}}},
  author = {Mugnaini, Rogerio},
  year = {2013},
  month = jan,
  pages = {37--58},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini_2013_40 anos de Bibliometria no Brasil.pdf},
  isbn = {978-85-7993-117-8},
  keywords = {done}
}

@article{mugnaini2014,
  title = {{Comunica\c{c}\~ao cient\'ifica no Brasil (1998-2012): indexa\c{c}\~ao, crescimento, fluxo e dispers\~ao}},
  shorttitle = {{Comunica\c{c}\~ao cient\'ifica no Brasil (1998-2012)}},
  author = {Mugnaini, Rog{\'e}rio and Digiampetri, Luciano Antonio and {Mena-Chalco}, Jes{\'u}s Pascual and Mugnaini, Rog{\'e}rio and Digiampetri, Luciano Antonio and {Mena-Chalco}, Jes{\'u}s Pascual},
  year = {2014},
  month = dec,
  volume = {26},
  pages = {239--252},
  publisher = {{Pontif\'icia Universidade Cat\'olica de Campinas}},
  issn = {0103-3786},
  doi = {10.1590/0103-3786201400030002},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini et al_2014_Comunicação científica no Brasil (1998-2012).pdf},
  journal = {Transinforma\c{c}\~ao},
  language = {pt},
  number = {3}
}

@article{mugnaini2017,
  title = {{Ciclo avaliativo de peri\'odicos no Brasil: caminho virtuoso ou colcha de retalhos?}},
  shorttitle = {{Ciclo avaliativo de peri\'odicos no Brasil}},
  author = {Mugnaini, Rogerio},
  year = {2017},
  month = mar,
  abstract = {This study investigated factors influencing the scholarly communication process, from the analysis of the quality criteria proposed between thematic areas in the exercise of evaluation ofscientific production in Brazil. Therefore, provided an overview based on the types of assessment criteria adopted in the Qualis-Peri\'odicos for the definition of strata A1, A2 and B1, from the documents of 2010-2012 period. It found that most hard sciences areas presents evaluation profile strictly based on bibliometric indicators, mainly JCR impact factor and in some cases, the h-index. Humanities, Social and Language, Literature and Arts areas, serve themselves believing at the selection process carried out by the databases, especially the citation indexes (Web of Science, Scopus and SciELO), and also in other databases as RedALyC and Latindex when defining criteria of journal indexing. Finally, periodic characteristics are primarily institutional diversity of authors and authors from foreign institutions, followed by journal periodicity and diversity of the editorial board. Thus configures itself an evaluation cycle, whose result appears to contribute to the improvement of the journals. On the other hand, although mounted, the process needs review, so that one can assess their effects on the scientific communication process.},
  annotation = {Accepted: 2017-03-06T20:28:32Z},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini_2017_Ciclo avaliativo de periódicos no Brasil.pdf},
  language = {pt\_BR}
}

@book{mugnaini2017a,
  title = {Bibliometria e Cientometria No {{Brasil}}: Infraestrutura Para Avalia\c{c}\~ao Da Pesquisa Cient\'ifica Na Era Do {{Big Data}} / {{Bibliometrics}} and Scientometrics in {{Brazil}}: Scientific Research Assessment Infrastructure in the Era of {{Big Data}}},
  shorttitle = {Bibliometria e Cientometria No {{Brasil}}},
  author = {Mugnaini, Rogerio and Fujino, Asa and Kobashi, Nair},
  year = {2017},
  month = mar,
  doi = {10.11606/9788572051705},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini et al_2017_Bibliometria e cientometria no Brasil.pdf},
  isbn = {978-85-7205-170-5},
  keywords = {fav,top5}
}

@article{mugnaini2021,
  title = {Openness Trends in {{Brazilian}} Citation Data: Factors Related to the Use of {{DOIs}}},
  shorttitle = {Openness Trends in {{Brazilian}} Citation Data},
  author = {Mugnaini, Rog{\'e}rio and Fraumann, Grischa and Tuesta, Esteban F. and Packer, Abel L.},
  year = {2021},
  month = mar,
  volume = {126},
  pages = {2523--2556},
  issn = {1588-2861},
  doi = {10.1007/s11192-020-03663-7},
  abstract = {Digital object identifiers (DOIs) are important metadata elements for indexing and interoperability, as well as for bibliometric studies in times of openness. This study analyses the use of DOIs in the cited references of articles by authors from Brazilian institutions, their possible influencing factors and differences among areas of knowledge. It measures the extent to which the citation datasets are open for reuse by others in terms of the availability of DOIs. 226,491 articles were retrieved from Web of Science (2012\textendash 2016), making a total of 8,707,120 cited references, 68\% of which include DOIs. The results showed that the hard sciences have higher percentages of DOIs in their cited references. The factor type of collaboration showed higher percentages when there is international collaboration, being significantly different from the other categories. However, when the analysis was conducted inside the areas, the international collaboration was found to be different particularly in the soft sciences and a couple of other areas. The articles with DOI attributed, as well as those with mention of research funding, had a significantly higher percentage, even in the interaction with the areas of knowledge. Among the open access routes the green routes showed the highest percentages, followed by golden (DOAJ and other) and Bronze, but green routes articles proved to be not significantly different from those not openly accessible. Finally, the principal collaborating countries also showed the greatest influence on the DOI attribution, with the exception of Peru and South Africa. Our findings provide evidence that studies on the availability and usability of DOIs can assist researchers, by underlining the importance of making greater use of this persistent identifier, as well as to provide consistency to citation analysis.},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini et al_2021_Openness trends in Brazilian citation data.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {3}
}

@article{oh2009,
  title = {Is the Journal Impact Factor a Valid Indicator of Scientific Value?},
  author = {Oh, H. C. and Lim, J. F.},
  year = {2009},
  month = aug,
  volume = {50},
  pages = {749--751},
  issn = {0037-5675},
  journal = {Singapore Medical Journal},
  keywords = {Bibliometrics,Biomedical Research,Databases; Bibliographic,Journal Impact Factor,Periodicals as Topic,Publishing},
  language = {eng},
  number = {8},
  pmid = {19710968}
}

@article{pendlebury2009,
  title = {The Use and Misuse of Journal Metrics and Other Citation Indicators},
  author = {Pendlebury, David A.},
  year = {2009},
  month = feb,
  volume = {57},
  pages = {1--11},
  issn = {0004-069X, 1661-4917},
  doi = {10.1007/s00005-009-0008-y},
  abstract = {This article reviews the nature and use of the journal impact factor and other common bibliometric measures for assessing research in the sciences and social sciences based on data compiled by Thomson Reuters. Journal impact factors are frequently misused to assess the influence of individual papers and authors, but such uses were never intended. Thomson Reuters also employs other measures of journal influence, which are contrasted with the impact factor. Finally, the author comments on the proper use of citation data in general, often as a supplement to peer review. This review may help government policymakers, university administrators, and individual researchers become better acquainted with the potential benefits and limitations of bibliometrics in the evaluation of research.},
  file = {/home/gabriel/Dropbox/zotero-library/Archivum Immunologiae et Therapiae Experimentalis/2009/Pendlebury_2009_The use and misuse of journal metrics and other citation indicators.pdf},
  journal = {Archivum Immunologiae et Therapiae Experimentalis},
  language = {en},
  number = {1}
}

@article{peoples2016,
  title = {Twitter {{Predicts Citation Rates}} of {{Ecological Research}}},
  author = {Peoples, Brandon K. and Midway, Stephen R. and Sackett, Dana and Lynch, Abigail and Cooney, Patrick B.},
  year = {2016},
  month = nov,
  volume = {11},
  pages = {e0166570},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0166570},
  abstract = {The relationship between traditional metrics of research impact (e.g., number of citations) and alternative metrics (altmetrics) such as Twitter activity are of great interest, but remain imprecisely quantified. We used generalized linear mixed modeling to estimate the relative effects of Twitter activity, journal impact factor, and time since publication on Web of Science citation rates of 1,599 primary research articles from 20 ecology journals published from 2012\textendash 2014. We found a strong positive relationship between Twitter activity (i.e., the number of unique tweets about an article) and number of citations. Twitter activity was a more important predictor of citation rates than 5-year journal impact factor. Moreover, Twitter activity was not driven by journal impact factor; the `highest-impact' journals were not necessarily the most discussed online. The effect of Twitter activity was only about a fifth as strong as time since publication; accounting for this confounding factor was critical for estimating the true effects of Twitter use. Articles in impactful journals can become heavily cited, but articles in journals with lower impact factors can generate considerable Twitter activity and also become heavily cited. Authors may benefit from establishing a strong social media presence, but should not expect research to become highly cited solely through social media promotion. Our research demonstrates that altmetrics and traditional metrics can be closely related, but not identical. We suggest that both altmetrics and traditional citation rates can be useful metrics of research impact.},
  file = {/home/gabriel/Dropbox/zotero-library/Peoples et al_2016_Twitter Predicts Citation Rates of Ecological Research.pdf},
  journal = {PLOS ONE},
  keywords = {Altmetrics,Bibliometrics,Citation analysis,Conservation science,Ecology,Scientific publishing,Social media,Twitter},
  language = {en},
  number = {11}
}

@article{roldan-valadez2019,
  title = {Current Concepts on Bibliometrics: A Brief Review about Impact Factor, {{Eigenfactor}} Score, {{CiteScore}}, {{SCImago Journal Rank}}, {{Source}}-{{Normalised Impact}} per {{Paper}}, {{H}}-Index, and Alternative Metrics},
  shorttitle = {Current Concepts on Bibliometrics},
  author = {{Roldan-Valadez}, Ernesto and {Salazar-Ruiz}, Shirley Yoselin and {Ibarra-Contreras}, Rafael and Rios, Camilo},
  year = {2019},
  month = aug,
  volume = {188},
  pages = {939--951},
  issn = {0021-1265, 1863-4362},
  doi = {10.1007/s11845-018-1936-5},
  abstract = {Background Understanding the impact of a publication by using bibliometric indices becomes an essential activity not only for universities and research institutes but also for individual academicians. This paper aims to provide a brief review of the current bibliometric tools used by authors and editors and proposes an algorithm to assess the relevance of the most common bibliometric tools to help the researchers select the fittest journal and know the trends of published submissions by using self-evaluation. Methods We present a narrative review answering at least two related consecutive questions triggered by the topics mentioned above. How prestigious is a journal based on its most recent bibliometrics, so authors may choose it to submit their next manuscript? And, how can they self-evaluate/understand the impact of their whole publishing scientific life? Results We presented the main relevant definitions of each bibliometrics and grouped them in those oriented to evaluated journals or individuals. Also, we share with our readers our algorithm to assess journals before manuscript submission. Conclusions Since there is a journal performance market and an article performance market, each one with its patterns, an integrative use of these metrics, rather than just the impact factor alone, might represent the fairest and most legitimate approach to assess the influence and importance of an acceptable research issue, and not only a sound journal in their respective disciplines.},
  file = {/home/gabriel/Dropbox/zotero-library/Irish Journal of Medical Science (1971 -)/2019/Roldan-Valadez et al_2019_Current concepts on bibliometrics.pdf},
  journal = {Irish Journal of Medical Science (1971 -)},
  keywords = {fav,top5},
  language = {en},
  number = {3}
}

@article{shelton2012,
  title = {Publish or Patent: {{Bibliometric}} Evidence for Empirical Trade-Offs in National Funding Strategies},
  shorttitle = {Publish or Patent},
  author = {Shelton, R. D. and Leydesdorff, Loet},
  year = {2012},
  volume = {63},
  pages = {498--511},
  issn = {1532-2890},
  doi = {10.1002/asi.21677},
  abstract = {Multivariate linear regression models suggest a trade-off in allocations of national research and development (R\&D). Government funding and spending in the higher education sector encourage publications as a long-term research benefit. Conversely, other components such as industrial funding and spending in the business sector encourage patenting. Our results help explain why the United States trails the European Union in publications: The focus in the United States is on industrial funding\textemdash some 70\% of its total R\&D investment. Likewise, our results also help explain why the European Union trails the United States in patenting, since its focus on government funding is less effective than industrial funding in predicting triadic patenting. Government funding contributes negatively to patenting in a multiple regression, and this relationship is significant in the case of triadic patenting. We provide new forecasts about the relationships of the United States, the European Union, and China for publishing; these results suggest much later dates for changes than previous forecasts because Chinese growth has been slowing down since 2003. Models for individual countries might be more successful than regression models whose parameters are averaged over a set of countries because nations can be expected to differ historically in terms of the institutional arrangements and funding schemes.},
  annotation = {\_eprint: https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.21677},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of the American Society for Information Science and Technology/2012/Shelton_Leydesdorff_2012_Publish or patent.pdf},
  journal = {Journal of the American Society for Information Science and Technology},
  language = {en},
  number = {3}
}

@article{subramanyam1983,
  title = {Bibliometric Studies of Research Collaboration: {{A}} Review},
  shorttitle = {Bibliometric Studies of Research Collaboration},
  author = {Subramanyam, K.},
  year = {1983},
  month = jan,
  volume = {6},
  pages = {33--38},
  issn = {0165-5515, 1741-6485},
  doi = {10.1177/016555158300600105},
  abstract = {Scientific research is becoming an increasingly collaborative endeavour. The nature and magnitude of collaboration vary from one discipline to another, and depend upon such factors as the nature of the research problem, the research environ ment, and demographic factors. Earlier studies have shown a high degree of correlation between collaboration and research productivity, and between collaboration and financial support for research. The extent of collaboration cannot be easily determined by traditional methods of survey and observation. Bibliometric methods offer a convenient and non-reactive tool for studying collaboration in research. In this paper, several types of collaboration have been identified, and earlier research on collaboration has been reviewed. Further research is needed to refine the methods of defining and assessing collaboration and its impact on the organization of research and communica tion in science.},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of Information Science/1983/Subramanyam_1983_Bibliometric studies of research collaboration.pdf},
  journal = {Journal of Information Science},
  keywords = {fav},
  language = {en},
  number = {1}
}

@article{tahamtan2016,
  title = {Factors Affecting Number of Citations: A Comprehensive Review of the Literature},
  shorttitle = {Factors Affecting Number of Citations},
  author = {Tahamtan, Iman and Safipour Afshar, Askar and Ahamdzadeh, Khadijeh},
  year = {2016},
  month = jun,
  volume = {107},
  pages = {1195--1225},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-016-1889-2},
  file = {/home/gabriel/Dropbox/zotero-library/Tahamtan et al_2016_Factors affecting number of citations.pdf},
  journal = {Scientometrics},
  keywords = {todo},
  language = {en},
  number = {3}
}

@article{thelwall2013,
  title = {Do {{Altmetrics Work}}? {{Twitter}} and {{Ten Other Social Web Services}}},
  shorttitle = {Do {{Altmetrics Work}}?},
  author = {Thelwall, Mike and Haustein, Stefanie and Larivi{\`e}re, Vincent and Sugimoto, Cassidy R.},
  year = {2013},
  month = may,
  volume = {8},
  pages = {e64841},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0064841},
  abstract = {Altmetric measurements derived from the social web are increasingly advocated and used as early indicators of article impact and usefulness. Nevertheless, there is a lack of systematic scientific evidence that altmetrics are valid proxies of either impact or utility although a few case studies have reported medium correlations between specific altmetrics and citation rates for individual journals or fields. To fill this gap, this study compares 11 altmetrics with Web of Science citations for 76 to 208,739 PubMed articles with at least one altmetric mention in each case and up to 1,891 journals per metric. It also introduces a simple sign test to overcome biases caused by different citation and usage windows. Statistically significant associations were found between higher metric scores and higher citations for articles with positive altmetric scores in all cases with sufficient evidence (Twitter, Facebook wall posts, research highlights, blogs, mainstream media and forums) except perhaps for Google+ posts. Evidence was insufficient for LinkedIn, Pinterest, question and answer sites, and Reddit, and no conclusions should be drawn about articles with zero altmetric scores or the strength of any correlation between altmetrics and citations. Nevertheless, comparisons between citations and metric values for articles published at different times, even within the same year, can remove or reverse this association and so publishers and scientometricians should consider the effect of time when using altmetrics to rank articles. Finally, the coverage of all the altmetrics except for Twitter seems to be low and so it is not clear if they are prevalent enough to be useful in practice.},
  file = {/home/gabriel/Dropbox/zotero-library/Thelwall et al_2013_Do Altmetrics Work.pdf},
  journal = {PLOS ONE},
  keywords = {Altmetrics,Bibliometrics,Citation analysis,Libraries,Medical journals,Scientific publishing,Social media,Twitter},
  language = {en},
  number = {5}
}

@article{torre,
  title = {{Nuovi indicatori bibliometrici nella letteratura scientifica: un panorama in continua evoluzione}},
  author = {Torre, G La and Sciarra, I and Chiappetta, M and Monteduro, A},
  pages = {7},
  abstract = {Introduction. Bibliometrics is a science which evaluates the impact of the scientific work of a journal or of an author, using mathematical and statistical tools. Impact Factor (IF) is the first bibliometric parameter created, and after it many others have been progressively conceived in order to go beyond its limits. Currently bibliometric indexes are used for academic purposes, among them to evaluate the eligibility of a researcher to compete for the National Scientific Qualification, in order to access to competitive exams to become professor. Objective. Aim of this study is to identify the most relevant bibliometric indexes and to summarized their characteristics. Methods. A revision of bibliometric indexes as been conducted, starting from the classic ones and completing with the most recent ones. Results. The two most used bibliometric indexes are the IF, which measures the scientific impact of a periodical and bases on Web of Science citation database, and the h-index, which measures the impact of the scientific work of a researcher, basing on Scopus database. Besides them other indexes have been created more recently, such as the SCImago Journal Rank Indicator (SJR), the Source Normalised Impact per Paper (SNIP) and the CiteScore index. They are all based on Scopus database and evaluate, in different ways, the citational impact of a periodic. The i10-index instead is provided from Google Scholar database and allows to evaluate the impact of the scientific production of a researcher. Recently two softwares have been introduced: the first one, Publish or Perish, allows to evaluate the scientific work of a researcher, through the assessment of many indexes; the second one, Altmetric, measure the use in the Web of the academic papers, instead of measuring citations, by means of alternative metrics respect to the traditional ones. Conclusions. Each analized index shows advantages but also criticalities. Therefore the combined use of more than one indexes, citational and not, should be preferred, in order to correctly evaluate the work of reserchers and to finally improve the quality and the development of scientific research. Clin Ter 2017; 168(2):e65-71. doi: 10.7417/CT.2017.1985},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/undefined/Torre et al_Nuovi indicatori bibliometrici nella letteratura scientifica.pdf},
  language = {it}
}

@article{tran2019,
  title = {Global {{Evolution}} of {{Research}} in {{Artificial Intelligence}} in {{Health}} and {{Medicine}}: {{A Bibliometric Study}}},
  shorttitle = {Global {{Evolution}} of {{Research}} in {{Artificial Intelligence}} in {{Health}} and {{Medicine}}},
  author = {Tran, Bach and Vu, Giang and Ha, Giang and Vuong, Quan-Hoang and Ho, Manh-Tung and Vuong, Thu-Trang and La, Viet-Phuong and Ho, Manh-Toan and Nghiem, Kien-Cuong and Nguyen, Huong and Latkin, Carl and Tam, Wilson and Cheung, Ngai-Man and Nguyen, Hong-Kong and Ho, Cyrus and Ho, Roger},
  year = {2019},
  month = mar,
  volume = {8},
  pages = {360},
  issn = {2077-0383},
  doi = {10.3390/jcm8030360},
  abstract = {The increasing application of Artificial Intelligence (AI) in health and medicine has attracted a great deal of research interest in recent decades. This study aims to provide a global and historical picture of research concerning AI in health and medicine. A total of 27,451 papers that were published between 1977 and 2018 (84.6\% were dated 2008\textendash 2018) were retrieved from the Web of Science platform. The descriptive analysis examined the publication volume, and authors and countries collaboration. A global network of authors' keywords and content analysis of related scientific literature highlighted major techniques, including Robotic, Machine learning, Artificial neural network, Artificial intelligence, Natural language process, and their most frequent applications in Clinical Prediction and Treatment. The number of cancer-related publications was the highest, followed by Heart Diseases and Stroke, Vision impairment, Alzheimer's, and Depression. Moreover, the shortage in the research of AI application to some high burden diseases suggests future directions in AI research. This study offers a first and comprehensive picture of the global efforts directed towards this increasingly important and prolific field of research and suggests the development of global and national protocols and regulations on the justification and adaptation of medical AI products.},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of Clinical Medicine/2019/Tran et al_2019_Global Evolution of Research in Artificial Intelligence in Health and Medicine.pdf},
  journal = {Journal of Clinical Medicine},
  language = {en},
  number = {3}
}

@article{vanleeuwen2001,
  title = {The Use of Combined Bibliometric Methods in Research Funding Policy},
  author = {{van Leeuwen}, T N and {van der Wurff}, L J and {van Raan}, A F J},
  year = {2001},
  month = dec,
  volume = {10},
  pages = {195--201},
  issn = {09582029, 14715449},
  doi = {10.3152/147154401781777015},
  file = {/home/gabriel/Dropbox/zotero-library/Research Evaluation/2001/van Leeuwen et al_2001_The use of combined bibliometric methods in research funding policy.pdf},
  journal = {Research Evaluation},
  language = {en},
  number = {3}
}

@article{wallin2005,
  title = {Bibliometric {{Methods}}: {{Pitfalls}} and {{Possibilities}}},
  shorttitle = {Bibliometric {{Methods}}},
  author = {Wallin, Johan A.},
  year = {2005},
  volume = {97},
  pages = {261--275},
  issn = {1742-7843},
  doi = {10.1111/j.1742-7843.2005.pto_139.x},
  abstract = {Abstract: Bibliometric studies are increasingly being used for research assessment. Bibliometric indicators are strongly methodology-dependent but for all of them, various types of data normalization are an indispensable requirement. Bibliometric studies have many pitfalls; technical skill, critical sense and a precise knowledge about the examined scientific domain are required to carry out and interpret bibliometric investigations correctly.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1742-7843.2005.pto\_139.x},
  file = {/home/gabriel/Dropbox/zotero-library/Basic & Clinical Pharmacology & Toxicology/2005/Wallin_2005_Bibliometric Methods.pdf},
  journal = {Basic \& Clinical Pharmacology \& Toxicology},
  keywords = {fav},
  language = {en},
  number = {5}
}

@article{wang2019,
  title = {Which Can Better Predict the Future Success of Articles? {{Bibliometric}} Indices or Alternative Metrics},
  shorttitle = {Which Can Better Predict the Future Success of Articles?},
  author = {Wang, Mingyang and Wang, Zhenyu and Chen, Guangsheng},
  year = {2019},
  month = jun,
  volume = {119},
  pages = {1575--1595},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-019-03052-9},
  abstract = {In this paper, we made a survey on the prediction capability of bibliometric indices and alternative metrics on the future success of articles by establishing a machine learning framework. Twenty-three bibliometric and alternative indices were collected to establish the feature space for the predication task. In order to eliminate the possible redundancy in feature space, three feature selection techniques of Relief-F, principal component analysis and entropy weighted method were used to rank the features according to their contribution to the original data set. Combining the fractal dimension of the data set, the intrinsic features which can better represent the original feature space were extracted. Three classifiers of Na\"ive Bayes, KNN and random forest were performed to detect the classification performance of these features. Experimental results show that both bibliometric indices and alternative metrics are beneficial to articles' growth. Early citation features, early Web usage statistics, as well as the reputation of the first author are the most valuable indicators in making an article more influential in the future.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2019/Wang et al_2019_Which can better predict the future success of articles.pdf},
  journal = {Scientometrics},
  keywords = {done},
  language = {en},
  number = {3}
}

@article{wang2021,
  title = {Identifying 'seed' Papers in Sciences},
  author = {Wang, Jean J. and Shao, Sarah X. and Ye, Fred Y.},
  year = {2021},
  month = apr,
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03980-5},
  abstract = {A concise quantitative method is established for identifying `seed' papers in sciences. The method is set up following h-type metrics based on co-citation network analysis. With defining original-seed (O-Seed) and dominant-seed (D-Seed) by measurable h-strength and second-order h-type degree centrality, O-seed resembles to be a `root' and D-seed develops to become `stem'. Using dataset from Web of Science (WoS), the `seed' papers in research fields of graphene, genome editing, and h-set studies are identified. Graphene D-Seed paper and genome editing D-Seed paper are representative outputs of the 2010 Nobel Prize in Physics and the 2020 Nobel Prize in Chemistry respectively. H-set O-Seed and D-Seed are the same paper that first proposed the concept of h-index. The `seed' papers are characterized by not only high citations, but also network structure and core function in sciences.},
  file = {/home/gabriel/Dropbox/zotero-library/Wang et al_2021_Identifying 'seed' papers in sciences.pdf},
  journal = {Scientometrics},
  keywords = {todo},
  language = {en}
}

@article{zancanaro2015,
  title = {A Bibliometric Mapping of Open Educational Resources},
  author = {Zancanaro, Airton and Todesco, Jos{\'e} Leomar and Ramos, Fernando},
  year = {2015},
  month = jan,
  volume = {16},
  issn = {1492-3831},
  doi = {10.19173/irrodl.v16i1.1960},
  abstract = {Open educational resources (OER) is a topic that has aroused increasing interest by researchers as a powerful contribution to improve the educational system quality and openness, both in face to face and distance education. The goal of this research is to map publications related to OER, dating from 2002 to 2013, and available through the Web of Science and Scopus scientific databases as well as in the OER Knowledge Cloud open repository. Data were used to explore relevant aspects related to the scientific production in OER, such as: (i) number of publications per year; (ii) most cited publications; (iii) authors with higher number of publications; (iv) institutions and countries with more publications and (v) most referenced bibliography by the authors. The analysis has included 544 papers, written by 843 authors, from 338 institutions, from 61 different countries. Moreover, the analysis has included the publications referenced and the author's keywords, considering 6,355 different publications and 929 different keywords. Besides presenting a bibliographic mapping of the research on OER, this paper also intends to contribute to consolidate the idea that OER is a promising field for researchers, in line with the spreading of the Open movement.},
  file = {/home/gabriel/Dropbox/zotero-library/The International Review of Research in Open and Distributed Learning/2015/Zancanaro et al_2015_A bibliometric mapping of open educational resources.pdf},
  journal = {The International Review of Research in Open and Distributed Learning},
  language = {en},
  number = {1}
}

@article{zhang2021,
  title = {Does Open Data Boost Journal Impact: Evidence from {{Chinese}} Economics},
  shorttitle = {Does Open Data Boost Journal Impact},
  author = {Zhang, Liwei and Ma, Liang},
  year = {2021},
  month = apr,
  volume = {126},
  pages = {3393--3419},
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03897-z},
  abstract = {To encourage research transparency and replication, more and more journals have been requiring authors to share original datasets and analytic procedures supporting their publications. Does open data boost journal impact? In this article, we report one of the first empirical studies to assess the effects of open data on journal impact. China Industrial Economics (CIE) mandated authors to open their research data in the end of 2016, which is the first to embrace open data among Chinese journals and provides a natural experiment for policy evaluation. We use the data of 37 Chinese economics journals from 2001 to 2019 and apply synthetic control method to causally estimate the effects of open data, and our results show that open data has significantly increased the citations of journal articles. On average, the current- and second-year citations of articles published with CIE have increased by 1\,\textasciitilde\,4 times, and articles published before the open data policy also benefited from the spillover effect. Our findings suggest that journals can leverage compulsory open data to develop reputation and amplify academic impacts.},
  file = {/home/gabriel/Dropbox/zotero-library/Zhang_Ma_2021_Does open data boost journal impact.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {4}
}

@misc{zotero-835,
  title = {Do {{Altmetrics Work}}? {{Twitter}} and {{Ten Other Social Web Services}}},
  howpublished = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0064841}
}

@misc{zotero-839,
  title = {Twitter {{Predicts Citation Rates}} of {{Ecological Research}}},
  howpublished = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0166570}
}


