
@article{abramo2011,
  title = {Evaluating Research: From Informed Peer Review to Bibliometrics},
  shorttitle = {Evaluating Research},
  author = {Abramo, Giovanni and D'Angelo, Ciriaco Andrea},
  year = {2011},
  month = jun,
  volume = {87},
  pages = {499--514},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-011-0352-7},
  abstract = {National research assessment exercises are becoming regular events in ever more countries. The present work contrasts the peer-review and bibliometrics approaches in the conduct of these exercises. The comparison is conducted in terms of the essential parameters of any measurement system: accuracy, robustness, validity, functionality, time and costs. Empirical evidence shows that for the natural and formal sciences, the bibliometric methodology is by far preferable to peer-review. Setting up national databases of publications by individual authors, derived from Web of Science or Scopus databases, would allow much better, cheaper and more frequent national research assessments.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2011/Abramo_Dâ€™Angelo_2011_Evaluating research.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {3}
}

@article{abramo2019,
  title = {Peer Review versus Bibliometrics: {{Which}} Method Better Predicts the Scholarly Impact of Publications?},
  shorttitle = {Peer Review versus Bibliometrics},
  author = {Abramo, Giovanni and D'Angelo, Ciriaco Andrea and Reale, Emanuela},
  year = {2019},
  month = oct,
  volume = {121},
  pages = {537--554},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-019-03184-y},
  abstract = {In this work, we try to answer the question of which method, peer review versus bibliometrics, better predicts the future overall scholarly impact of scientific publications. We measure the agreement between peer review evaluations of Web of Science indexed publications submitted to the first Italian research assessment exercise and long-term citations of the same publications. We do the same for an early citation-based indicator. We find that the latter shows stronger predictive power, i.e. it more reliably predicts late citations in all the disciplinary areas examined, and for any citation time window starting 1 year after publication.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2019/Abramo et al_2019_Peer review versus bibliometrics.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {1}
}

@article{belter2015,
  title = {Bibliometric Indicators: Opportunities and Limits},
  shorttitle = {Bibliometric Indicators},
  author = {Belter, Christopher W.},
  year = {2015},
  month = oct,
  volume = {103},
  pages = {219--221},
  issn = {1536-5050},
  doi = {10.3163/1536-5050.103.4.014},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of the Medical Library Association  JMLA/2015/Belter_2015_Bibliometric indicators.pdf},
  journal = {Journal of the Medical Library Association : JMLA},
  keywords = {fav},
  number = {4},
  pmcid = {PMC4613388},
  pmid = {26512227}
}

@article{butler2017,
  title = {The {{Evolution}} of {{Current Research Impact Metrics}}: {{From Bibliometrics}} to {{Altmetrics}}?},
  shorttitle = {The {{Evolution}} of {{Current Research Impact Metrics}}},
  author = {Butler, Joseph S. and Kaye, I. David and Sebastian, Arjun S. and Wagner, Scott C. and Morrissey, Patrick B. and Schroeder, Gregory D. and Kepler, Christopher K. and Vaccaro, Alexander R.},
  year = {2017},
  month = jun,
  volume = {30},
  pages = {226--228},
  issn = {2380-0186},
  doi = {10.1097/BSD.0000000000000531},
  abstract = {The prestige of publication has been based on traditional citation metrics, most commonly journal impact factor. However, the Internet has radically changed the speed, flow, and sharing of medical information. Furthermore, the explosion of social media, along with development of popular professional and scientific websites and blogs, has led to the need for alternative metrics, known as altmetrics, to quantify the wider impact of research. We explore the evolution of current research impact metrics and examine the evolving role of altmetrics in measuring the wider impact of research. We suggest that altmetrics used in research evaluation should be part of an informed peer-review process such as traditional metrics. Moreover, results based on altmetrics must not lead to direct decision making about research, but instead, should be used to assist experts in making decisions. Finally, traditional and alternative metrics should complement, not replace, each other in the peer-review process.},
  file = {/home/gabriel/Dropbox/zotero-library/Clinical Spine Surgery/2017/Butler et al_2017_The Evolution of Current Research Impact Metrics.pdf},
  journal = {Clinical Spine Surgery},
  language = {en-US},
  number = {5}
}

@article{cabezas-clavijo2013,
  title = {Reviewers' Ratings and Bibliometric Indicators: Hand in Hand When Assessing over Research Proposals?},
  shorttitle = {Reviewers' Ratings and Bibliometric Indicators},
  author = {{Cabezas-Clavijo}, Alvaro and {Robinson-Garc{\'i}a}, Nicol{\'a}s and Escabias, Manuel and {Jim{\'e}nez-Contreras}, Evaristo},
  year = {2013},
  volume = {8},
  pages = {e68258},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0068258},
  abstract = {BACKGROUND: The peer review system has been traditionally challenged due to its many limitations especially for allocating funding. Bibliometric indicators may well present themselves as a complement. OBJECTIVE: We analyze the relationship between peers' ratings and bibliometric indicators for Spanish researchers in the 2007 National R\&D Plan for 23 research fields. METHODS AND MATERIALS: We analyze peers' ratings for 2333 applications. We also gathered principal investigators' research output and impact and studied the differences between accepted and rejected applications. We used the Web of Science database and focused on the 2002-2006 period. First, we analyzed the distribution of granted and rejected proposals considering a given set of bibliometric indicators to test if there are significant differences. Then, we applied a multiple logistic regression analysis to determine if bibliometric indicators can explain by themselves the concession of grant proposals. RESULTS: 63.4\% of the applications were funded. Bibliometric indicators for accepted proposals showed a better previous performance than for those rejected; however the correlation between peer review and bibliometric indicators is very heterogeneous among most areas. The logistic regression analysis showed that the main bibliometric indicators that explain the granting of research proposals in most cases are the output (number of published articles) and the number of papers published in journals that belong to the first quartile ranking of the Journal Citations Report. DISCUSSION: Bibliometric indicators predict the concession of grant proposals at least as well as peer ratings. Social Sciences and Education are the only areas where no relation was found, although this may be due to the limitations of the Web of Science's coverage. These findings encourage the use of bibliometric indicators as a complement to peer review in most of the analyzed areas.},
  file = {/home/gabriel/Dropbox/zotero-library/PloS One/2013/Cabezas-Clavijo et al_2013_Reviewers' ratings and bibliometric indicators.pdf},
  journal = {PloS One},
  keywords = {Bibliometrics,Biomedical Research,Databases; Factual,fav,Financing; Organized,Humans,Publications,Publishing,Research Design,Research Personnel,Spain},
  language = {eng},
  number = {6},
  pmcid = {PMC3695904},
  pmid = {23840840}
}

@article{chahrour2020,
  title = {A {{Bibliometric Analysis}} of {{COVID}}-19 {{Research Activity}}: {{A Call}} for {{Increased Output}}},
  shorttitle = {A {{Bibliometric Analysis}} of {{COVID}}-19 {{Research Activity}}},
  author = {Chahrour, Mohamad and Assi, Sahar and Bejjani, Michael and Nasrallah, Ali A and Salhab, Hamza and Fares, Mohamad Y and Khachfe, Hussein H},
  year = {2020},
  month = mar,
  issn = {2168-8184},
  doi = {10.7759/cureus.7357},
  abstract = {Background: The novel coronavirus disease 2019 (COVID-19) has impacted many countries across all inhabited continents, and is now considered a global pandemic, due to its high rate of infectivity. Research related to this disease is pivotal for assessing pathogenic characteristics and formulating therapeutic strategies. The aim of this paper is to explore the activity and trends of COVID-19 research since its outbreak in December 2019. Methods: We explored the PubMed database and the World Health Organization (WHO) database for publications pertaining to COVID-19 since December 2019 up until March 18, 2020. Only relevant observational and interventional studies were included in our study. Data on COVID-19 incidence were extracted from the WHO situation reports. Research output was assessed with respect to gross domestic product (GDP) and population of each country. Results: Only 564 publications met our inclusion criteria. These articles came from 39 different countries, constituting 24\% of all affected countries. China produced the greatest number of publications with 377 publications (67\%). With respect to continental research activity, Asian countries had the highest research activity with 434 original publications (77\%). In terms of publications per million persons (PPMPs), Singapore had the highest number of publications with 1.069 PPMPs. In terms of publications per billion-dollar GDP, Mauritius ranked first with 0.075. Received 03/19/2020 Review began 03/20/2020 Review ended 03/20/2020 Published 03/21/2020 \textcopyright{} Copyright 2020 Chahrour et al. This is an open access article distributed under the terms of the Creative Commons Attribution License CC-BY 4.0., which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Conclusion: COVID-19 is a major disease that has impacted international public health on a global level. Observational studies and therapeutic trials pertaining to COVID-19 are essential for assessing pathogenic characteristics and developing novel treatment options.},
  file = {/home/gabriel/Dropbox/zotero-library/Cureus/2020/Chahrour et al_2020_A Bibliometric Analysis of COVID-19 Research Activity.pdf},
  journal = {Cureus},
  language = {en}
}

@article{durieux2010,
  title = {Bibliometric {{Indicators}}: {{Quality Measurements}} of {{Scientific Publication}}},
  shorttitle = {Bibliometric {{Indicators}}},
  author = {Durieux, Val{\'e}rie and Gevenois, Pierre Alain},
  year = {2010},
  month = apr,
  volume = {255},
  pages = {342--351},
  publisher = {{Radiological Society of North America}},
  issn = {0033-8419},
  doi = {10.1148/radiol.09090626},
  abstract = {Bibliometrics is a set of mathematical and statistical methods used to analyze and measure the quantity and quality of books, articles, and other forms of publications. There are three types of bibliometric indicators: quantity indicators, which measure the productivity of a particular researcher; quality indicators, which measure the quality (or ``performance'') of a researcher's output; and structural indicators, which measure connections between publications, authors, and areas of research. Bibliometric indicators are especially important for researchers and organizations, as these measurements are often used in funding decisions, appointments, and promotions of researchers. As more and more scientific discoveries occur and published research results are read and then quoted by other researchers, bibliometric indicators are becoming increasingly important. This article provides an overview of the currently used bibliometric indicators and summarizes the critical elements and characteristics one should be aware of when evaluating the quantity and quality of scientific output.\textcopyright{} RSNA, 2010},
  file = {/home/gabriel/Dropbox/zotero-library/Radiology/2010/Durieux_Gevenois_2010_Bibliometric Indicators.pdf},
  journal = {Radiology},
  keywords = {fav},
  number = {2}
}

@article{garner2018,
  title = {Bibliometric Indices: Defining Academic Productivity and Citation Rates of Researchers, Departments and Journals},
  shorttitle = {Bibliometric Indices},
  author = {Garner, Rebecca M. and Hirsch, Joshua A. and Albuquerque, Felipe C. and Fargen, Kyle M.},
  year = {2018},
  month = feb,
  volume = {10},
  pages = {102--106},
  issn = {1759-8486},
  doi = {10.1136/neurintsurg-2017-013265},
  abstract = {There has been an increasing focus on academic productivity for the purposes of promotion and funding within departments and institutions but also for comparison of individuals, institutions, specialties, and journals. A number of quantitative indices are used to investigate and compare academic productivity. These include various calculations attempting to analyze the number and citations of publications in order to capture both the quality and quantity of publications, such as the h index, the e index, impact factor, and Eigenfactor score. The indices have varying advantages and limitations and thus a basic knowledge is required in order to understand their potential utility within academic medicine. This article describes the various bibliometric indices and discusses recent applications of these metrics within the neurological sciences.},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of Neurointerventional Surgery/2018/Garner et al_2018_Bibliometric indices.pdf},
  journal = {Journal of Neurointerventional Surgery},
  keywords = {academic productivity,bibliometrics,Bibliometrics,Biomedical Research,citation analysis,Efficiency,fav,h index,Humans,Journal Impact Factor,Neurosciences,Periodicals as Topic},
  language = {eng},
  number = {2},
  pmid = {28824008}
}

@article{geuna2003,
  title = {University {{Research Evaluation}} and {{Funding}}: {{An International Comparison}}},
  shorttitle = {University {{Research Evaluation}} and {{Funding}}},
  author = {Geuna, Aldo and Martin, Ben R.},
  year = {2003},
  volume = {41},
  pages = {277--304},
  issn = {0026-4695},
  doi = {10.1023/B:MINE.0000005155.70870.bd},
  abstract = {Many countries have introduced evaluations of university research, reflecting global demands for greater accountability. This paper compares methods of evaluation used across twelve countries in Europe and the Asia-Pacific region. On the basis of this comparison, and focusing in particular on Britain, we examine the advantages and disadvantages of performance-based funding in comparison with other approaches to funding. Our analysis suggests that, while initial benefits may outweigh the costs, over time such a system seems to produce diminishing returns. This raises important questions about its continued use.},
  file = {/home/gabriel/Dropbox/zotero-library/Minerva/2003/Geuna_Martin_2003_University Research Evaluation and Funding.pdf},
  journal = {Minerva},
  language = {en},
  number = {4}
}

@book{gingras2016,
  title = {Bibliometrics and {{Research Evaluation}}: {{Uses}} and {{Abuses}}},
  shorttitle = {Bibliometrics and {{Research Evaluation}}},
  author = {Gingras, Yves},
  year = {2016},
  month = sep,
  publisher = {{MIT Press}},
  abstract = {Why bibliometrics is useful for understanding the global dynamics of science but generate perverse effects when applied inappropriately in research evaluation and university rankings.The research evaluation market is booming. ``Ranking,'' ``metrics,'' ``h-index,'' and ``impact factors'' are reigning buzzwords. Government and research administrators want to evaluate everything\textemdash teachers, professors, training programs, universities\textemdash using quantitative indicators. Among the tools used to measure ``research excellence,'' bibliometrics\textemdash aggregate data on publications and citations\textemdash has become dominant. Bibliometrics is hailed as an ``objective'' measure of research quality, a quantitative measure more useful than ``subjective'' and intuitive evaluation methods such as peer review that have been used since scientific papers were first published in the seventeenth century. In this book, Yves Gingras offers a spirited argument against an unquestioning reliance on bibliometrics as an indicator of research quality. Gingras shows that bibliometric rankings have no real scientific validity, rarely measuring what they pretend to.Although the study of publication and citation patterns, at the proper scales, can yield insights on the global dynamics of science over time, ill-defined quantitative indicators often generate perverse and unintended effects on the direction of research. Moreover, abuse of bibliometrics occurs when data is manipulated to boost rankings. Gingras looks at the politics of evaluation and argues that using numbers can be a way to control scientists and diminish their autonomy in the evaluation process. Proposing precise criteria for establishing the validity of indicators at a given scale of analysis, Gingras questions why universities are so eager to let invalid indicators influence their research strategy.},
  googlebooks = {0aExDQAAQBAJ},
  isbn = {978-0-262-33766-3},
  keywords = {Education / Evaluation \& Assessment,Language Arts \& Disciplines / Library \& Information Science / General},
  language = {en}
}

@article{glanzel2002,
  title = {Journal Impact Measures in Bibliometric Research},
  author = {Gl{\"a}nzel, Wolfgang and Moed, Henk F},
  year = {2002},
  pages = {23},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/2002/GlÃ¤nzel_Moed_2002_Journal impact measures in bibliometric research.pdf},
  language = {en}
}

@article{glanzel2008,
  title = {Seven {{Myths}} in {{Bibliometrics About}} Facts and Fiction in Quantitative Science Studies},
  author = {Gl{\"a}nzel, Wolfgang},
  year = {2008},
  month = jun,
  volume = {2},
  pages = {9--17},
  issn = {0973-7766, 2168-930X},
  doi = {10.1080/09737766.2008.10700836},
  file = {/home/gabriel/Dropbox/zotero-library/Collnet Journal of Scientometrics and Information Management/2008/GlÃ¤nzel_2008_Seven Myths in Bibliometrics About facts and fiction in quantitative science.pdf},
  journal = {Collnet Journal of Scientometrics and Information Management},
  keywords = {fav},
  language = {en},
  number = {1}
}

@article{haeffner-cavaillon2009,
  title = {The Use of Bibliometric Indicators to Help Peer-Review Assessment},
  author = {{Haeffner-Cavaillon}, Nicole and {Graillot-Gak}, Claude},
  year = {2009 Jan-Feb},
  volume = {57},
  pages = {33--38},
  issn = {1661-4917},
  doi = {10.1007/s00005-009-0004-2},
  abstract = {Inserm is the only French public research institution entirely dedicated to human health. Inserm supports research across the biomedical spectrum in all major disease areas, from fundamental lab-based science to clinical trials. To translate its scientists' findings into tangible health benefits, Inserm has its own affiliated company, Inserm Transfert, which works with industry. Since 2001, Inserm has been setting up on-line file management software for the evaluation of researchers and laboratories, called EVA ( www.eva.inserm.fr ). EVA includes all grant applications, assessment reports, evaluation grading evaluation forms and includes automated bibliometric indicator software that enables calculating, for example, the number of publications, journal impact factors, number of citations, citation index, and number of the Top 1 publications for each researcher of the teams. The indicators take into account research fields, the year of publications, and the author's position among the participants. Bibliometrics is now considered a tool for science policy providing indicators to measure productivity and scientific quality, thereby supplying a basis for evaluating and orienting R\&D. It is also a potential tool for evaluation. It is neutral, allows comparative (national and international) assessment, and may select papers in the forefront in all fields. For each team, bibliometric indicators are calculated for all researchers with permanent or long-term positions (3-5 years). The use of bibliometric indicators requires great vigilance, but according to our experience they enrich the committee's debates without any doubt. We present an analysis of the data of 600 research teams evaluated in 2007-2008.},
  file = {/home/gabriel/Dropbox/zotero-library/Archivum Immunologiae Et Therapiae Experimentalis/2009/Haeffner-Cavaillon_Graillot-Gak_2009_The use of bibliometric indicators to help peer-review assessment.pdf},
  journal = {Archivum Immunologiae Et Therapiae Experimentalis},
  keywords = {Academies and Institutes,Bibliometrics,Biomedical Research,Evaluation Studies as Topic,fav,Financing; Organized,France,Health Care Sector,Information Management,Journal Impact Factor,Laboratories,Peer Review; Research,Periodicals as Topic,Policy Making,Publishing,Quality Control,Software},
  language = {eng},
  number = {1},
  pmcid = {PMC3957005},
  pmid = {19219530}
}

@article{hammarfelt2018,
  title = {What Is a Discipline? {{The}} Conceptualization of Research Areas and Their Operationalization in Bibliometric Research},
  author = {Hammarfelt, Bj{\"o}rn},
  year = {2018},
  pages = {8},
  abstract = {This paper highlights disadvantages of conceptual impreciseness, and advocates further attention to the labels and concepts used when classifying clusters or groups based on bibliographic data. The main focus of the analysis is on the concept of `discipline' and how it is used in bibliometric research, but the implications concern a broader array of related terms.},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/2018/Hammarfelt_2018_What is a discipline.pdf},
  language = {en}
}

@article{hc,
  title = {Is the Journal Impact Factor a Valid Indicator of Scientific Value?},
  author = {Hc, Oh},
  pages = {3},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/undefined/Hc_Is the journal impact factor a valid indicator of scientific value.pdf},
  keywords = {fav},
  language = {en}
}

@article{jappe2018,
  title = {Does Bibliometric Research Confer Legitimacy to Research Assessment Practice? {{A}} Sociological Study of Reputational Control, 1972-2016},
  shorttitle = {Does Bibliometric Research Confer Legitimacy to Research Assessment Practice?},
  author = {Jappe, Arlette and Pithan, David and Heinze, Thomas},
  editor = {Lozano, Sergi},
  year = {2018},
  month = jun,
  volume = {13},
  pages = {e0199031},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0199031},
  abstract = {The use of bibliometric measures in the evaluation of research has increased considerably based on expertise from the growing research field of evaluative citation analysis (ECA). However, mounting criticism of such metrics suggests that the professionalization of bibliometric expertise remains contested. This paper investigates why impact metrics, such as the journal impact factor and the h-index, proliferate even though their legitimacy as a means of professional research assessment is questioned. Our analysis is informed by two relevant sociological theories: Andrew Abbott's theory of professions and Richard Whitley's theory of scientific work. These complementary concepts are connected in order to demonstrate that ECA has failed so far to provide scientific authority for professional research assessment. This argument is based on an empirical investigation of the extent of reputational control in the relevant research area. Using three measures of reputational control that are computed from longitudinal inter-organizational networks in ECA (1972\textendash 2016), we show that peripheral and isolated actors contribute the same number of novel bibliometric indicators as central actors. In addition, the share of newcomers to the academic sector has remained high. These findings demonstrate that recent methodological debates in ECA have not been accompanied by the formation of an intellectual field in the sociological sense of a reputational organization. Therefore, we conclude that a growing gap exists between an academic sector with little capacity for collective action and increasing demand for routine performance assessment by research organizations and funding agencies. This gap has been filled by database providers. By selecting and distributing research metrics, these commercial providers have gained a powerful role in defining de-facto standards of research excellence without being challenged by expert authority.},
  file = {/home/gabriel/Dropbox/zotero-library/PLOS ONE/2018/Jappe et al_2018_Does bibliometric research confer legitimacy to research assessment practice.pdf},
  journal = {PLOS ONE},
  keywords = {fav},
  language = {en},
  number = {6}
}

@article{jarneving2005,
  title = {A Comparison of Two Bibliometric Methods for Mapping of the Research Front},
  author = {Jarneving, Bo},
  year = {2005},
  month = nov,
  volume = {65},
  pages = {245--263},
  issn = {1588-2861},
  doi = {10.1007/s11192-005-0270-7},
  abstract = {This paper builds on previous research concerned with the classification and specialty mapping of research fields. Two methods are put to test in order to decide if significant differences as to mapping results of the research front of a science field occur when compared. The first method was based on document co-citation analysis where papers citing co-citation clusters were assumed to reflect the research front. The second method was bibliographic coupling where likewise citing papers were assumed to reflect the research front. The application of these methods resulted in two different types of aggregations of papers: (1) groups of papers citing clusters of co-cited works and (2) clusters of bibliographically coupled papers. The comparision of the two methods as to mapping results was pursued by matching word profiles of groups of papers citing a particular co-citation cluster with word profiles of clusters of bibliographically coupled papers. Findings suggested that the research front was portrayed in two considerably different ways by the methods applied. It was concluded that the results in this study would support a further comparative study of these methods on a more detailed and qualitative ground. The original data set encompassed 73,379 articles from the fifty most cited environmental science journals listed in Journal Citation Report, science edition downloaded from the Science Citation Index on CD-ROM.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2005/Jarneving_2005_A comparison of two bibliometric methods for mapping of the research front.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {2}
}

@article{joshi2014,
  title = {Bibliometric Indicators for Evaluating the Quality of Scientifc Publications},
  author = {Joshi, Medha A.},
  year = {2014},
  month = mar,
  volume = {15},
  pages = {258--262},
  issn = {1526-3711},
  doi = {10.5005/jp-journals-10024-1525},
  abstract = {Evaluation of quality and quantity of publications can be done using a set of statistical and mathematical indices called bibliometric indicators. Two major categories of indicators are (1) quantitative indicators that measure the research productivity of a researcher and (2) performance indicators that evaluate the quality of publications. Bibliometric indicators are important for both the individual researcher and organizations. They are widely used to compare the performance of the individual researchers, journals and universities. Many of the appointments, promotions and allocation of research funds are based on these indicators. This review article describes some of the currently used bibliometric indicators such as journal impact factor, crown indicator, h-index and it's variants. It is suggested that for comparison of scientific impact and scientific output of researchers due consideration should be given to various factors affecting theses indicators.},
  file = {/home/gabriel/Dropbox/zotero-library/The Journal of Contemporary Dental Practice/2014/Joshi_2014_Bibliometric indicators for evaluating the quality of scientifc publications.pdf},
  journal = {The Journal of Contemporary Dental Practice},
  keywords = {Authorship,Bibliometrics,Dental Research,fav,Humans,Journal Impact Factor,Periodicals as Topic,Publishing},
  language = {eng},
  number = {2},
  pmid = {25095854}
}

@article{karanatsiou,
  title = {Bibliometrics and {{Altmetrics}} Literature Review: {{Performance}} Indicators and Comparison Analysis},
  author = {Karanatsiou, Dimitra and Misirlis, Nikolaos and Vlachopoulou, Maro},
  pages = {21},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/undefined/Karanatsiou et al_Bibliometrics and Altmetrics literature review.pdf},
  language = {en}
}

@article{kumar2009,
  title = {Impact of the Impact Factor in Biomedical Research: Its Use and Misuse},
  shorttitle = {Impact of the Impact Factor in Biomedical Research},
  author = {Kumar, V. and Upadhyay, S. and Medhi, B.},
  year = {2009},
  month = aug,
  volume = {50},
  pages = {752--755},
  issn = {0037-5675},
  abstract = {The impact factor was created in the biomedical research field in order to measure a journal's value by calculating the average number of citations per article over a period of time. It was initially developed to help libraries decide which highly-cited journals to subscribe to. However, at present, it is being misused to judge the quality of a researcher or medical scientist as well as the quality of the work done. It contains serious sources of errors and flaws, resulting in strong biases against culture- and language-bound medical subspecialties. The present article is aimed to highlight the impact of the impact factor in the biomedical research, as well as its use and misuse.},
  file = {/home/gabriel/Dropbox/zotero-library/Singapore Medical Journal/2009/Kumar et al_2009_Impact of the impact factor in biomedical research.pdf},
  journal = {Singapore Medical Journal},
  keywords = {Bibliometrics,Biomedical Research,Databases; Bibliographic,fav,Journal Impact Factor,Periodicals as Topic,Publishing,Reproducibility of Results,Time Factors},
  language = {eng},
  number = {8},
  pmid = {19710969}
}

@article{merigo2017,
  title = {A Bibliometric Analysis of Operations Research and Management Science},
  author = {Merig{\'o}, Jos{\'e} M. and Yang, Jian-Bo},
  year = {2017},
  month = dec,
  volume = {73},
  pages = {37--48},
  issn = {03050483},
  doi = {10.1016/j.omega.2016.12.004},
  abstract = {Bibliometric analysis is the quantitative study of bibliographic material. It provides a general picture of a research field that can be classified by papers, authors and journals. This paper presents a bibliometric overview of research published in operations research and management science in recent decades. The main objective of this study is to identify some of the most relevant research in this field and some of the newest trends according to the information found in the Web of Science database. Several classifications are made, including an analysis of the most influential journals, the two hundred most cited papers of all time and the most productive and influential authors. The results obtained are in accordance with the common wisdom, although some variations are found.},
  file = {/home/gabriel/Dropbox/zotero-library/Omega/2017/MerigÃ³_Yang_2017_A bibliometric analysis of operations research and management science.pdf},
  journal = {Omega},
  language = {en}
}

@article{moura2017,
  title = {Uses of {{Bibliometric Techniques}} in {{Public Health Research}}},
  author = {MOURA, Luana Kelle Batista and {de MESQUITA}, Rafael Fernandes and MOBIN, Mitra and MATOS, Francisca Tereza Coelho and MONTE, Thiago Lima and LAGO, Eliana Campelo and FALC{\~A}O, Carlos Alberto Monteiro and {de Ar{\^e}a Le{\~a}o FERRAZ}, Maria {\^A}ngela and SANTOS, Tanit Clementino and SOUSA, Laelson Rochelle Milan{\^e}s},
  year = {2017},
  month = oct,
  volume = {46},
  pages = {1435--1436},
  issn = {2251-6085},
  file = {/home/gabriel/Dropbox/zotero-library/Iranian Journal of Public Health/2017/MOURA et al_2017_Uses of Bibliometric Techniques in Public Health Research.pdf},
  journal = {Iranian Journal of Public Health},
  number = {10},
  pmcid = {PMC5750357},
  pmid = {29308389}
}

@article{oh2009,
  title = {Is the Journal Impact Factor a Valid Indicator of Scientific Value?},
  author = {Oh, H. C. and Lim, J. F.},
  year = {2009},
  month = aug,
  volume = {50},
  pages = {749--751},
  issn = {0037-5675},
  journal = {Singapore Medical Journal},
  keywords = {Bibliometrics,Biomedical Research,Databases; Bibliographic,Journal Impact Factor,Periodicals as Topic,Publishing},
  language = {eng},
  number = {8},
  pmid = {19710968}
}

@article{pendlebury2009,
  title = {The Use and Misuse of Journal Metrics and Other Citation Indicators},
  author = {Pendlebury, David A.},
  year = {2009},
  month = feb,
  volume = {57},
  pages = {1--11},
  issn = {0004-069X, 1661-4917},
  doi = {10.1007/s00005-009-0008-y},
  abstract = {This article reviews the nature and use of the journal impact factor and other common bibliometric measures for assessing research in the sciences and social sciences based on data compiled by Thomson Reuters. Journal impact factors are frequently misused to assess the influence of individual papers and authors, but such uses were never intended. Thomson Reuters also employs other measures of journal influence, which are contrasted with the impact factor. Finally, the author comments on the proper use of citation data in general, often as a supplement to peer review. This review may help government policymakers, university administrators, and individual researchers become better acquainted with the potential benefits and limitations of bibliometrics in the evaluation of research.},
  file = {/home/gabriel/Dropbox/zotero-library/Archivum Immunologiae et Therapiae Experimentalis/2009/Pendlebury_2009_The use and misuse of journal metrics and other citation indicators.pdf},
  journal = {Archivum Immunologiae et Therapiae Experimentalis},
  language = {en},
  number = {1}
}

@article{roldan-valadez2019,
  title = {Current Concepts on Bibliometrics: A Brief Review about Impact Factor, {{Eigenfactor}} Score, {{CiteScore}}, {{SCImago Journal Rank}}, {{Source}}-{{Normalised Impact}} per {{Paper}}, {{H}}-Index, and Alternative Metrics},
  shorttitle = {Current Concepts on Bibliometrics},
  author = {{Roldan-Valadez}, Ernesto and {Salazar-Ruiz}, Shirley Yoselin and {Ibarra-Contreras}, Rafael and Rios, Camilo},
  year = {2019},
  month = aug,
  volume = {188},
  pages = {939--951},
  issn = {0021-1265, 1863-4362},
  doi = {10.1007/s11845-018-1936-5},
  abstract = {Background Understanding the impact of a publication by using bibliometric indices becomes an essential activity not only for universities and research institutes but also for individual academicians. This paper aims to provide a brief review of the current bibliometric tools used by authors and editors and proposes an algorithm to assess the relevance of the most common bibliometric tools to help the researchers select the fittest journal and know the trends of published submissions by using self-evaluation. Methods We present a narrative review answering at least two related consecutive questions triggered by the topics mentioned above. How prestigious is a journal based on its most recent bibliometrics, so authors may choose it to submit their next manuscript? And, how can they self-evaluate/understand the impact of their whole publishing scientific life? Results We presented the main relevant definitions of each bibliometrics and grouped them in those oriented to evaluated journals or individuals. Also, we share with our readers our algorithm to assess journals before manuscript submission. Conclusions Since there is a journal performance market and an article performance market, each one with its patterns, an integrative use of these metrics, rather than just the impact factor alone, might represent the fairest and most legitimate approach to assess the influence and importance of an acceptable research issue, and not only a sound journal in their respective disciplines.},
  file = {/home/gabriel/Dropbox/zotero-library/Irish Journal of Medical Science (1971 -)/2019/Roldan-Valadez et al_2019_Current concepts on bibliometrics.pdf},
  journal = {Irish Journal of Medical Science (1971 -)},
  language = {en},
  number = {3}
}

@article{shelton2012,
  title = {Publish or Patent: {{Bibliometric}} Evidence for Empirical Trade-Offs in National Funding Strategies},
  shorttitle = {Publish or Patent},
  author = {Shelton, R. D. and Leydesdorff, Loet},
  year = {2012},
  volume = {63},
  pages = {498--511},
  issn = {1532-2890},
  doi = {10.1002/asi.21677},
  abstract = {Multivariate linear regression models suggest a trade-off in allocations of national research and development (R\&D). Government funding and spending in the higher education sector encourage publications as a long-term research benefit. Conversely, other components such as industrial funding and spending in the business sector encourage patenting. Our results help explain why the United States trails the European Union in publications: The focus in the United States is on industrial funding\textemdash some 70\% of its total R\&D investment. Likewise, our results also help explain why the European Union trails the United States in patenting, since its focus on government funding is less effective than industrial funding in predicting triadic patenting. Government funding contributes negatively to patenting in a multiple regression, and this relationship is significant in the case of triadic patenting. We provide new forecasts about the relationships of the United States, the European Union, and China for publishing; these results suggest much later dates for changes than previous forecasts because Chinese growth has been slowing down since 2003. Models for individual countries might be more successful than regression models whose parameters are averaged over a set of countries because nations can be expected to differ historically in terms of the institutional arrangements and funding schemes.},
  annotation = {\_eprint: https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.21677},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of the American Society for Information Science and Technology/2012/Shelton_Leydesdorff_2012_Publish or patent.pdf},
  journal = {Journal of the American Society for Information Science and Technology},
  language = {en},
  number = {3}
}

@article{subramanyam1983,
  title = {Bibliometric Studies of Research Collaboration: {{A}} Review},
  shorttitle = {Bibliometric Studies of Research Collaboration},
  author = {Subramanyam, K.},
  year = {1983},
  month = jan,
  volume = {6},
  pages = {33--38},
  issn = {0165-5515, 1741-6485},
  doi = {10.1177/016555158300600105},
  abstract = {Scientific research is becoming an increasingly collaborative endeavour. The nature and magnitude of collaboration vary from one discipline to another, and depend upon such factors as the nature of the research problem, the research environ ment, and demographic factors. Earlier studies have shown a high degree of correlation between collaboration and research productivity, and between collaboration and financial support for research. The extent of collaboration cannot be easily determined by traditional methods of survey and observation. Bibliometric methods offer a convenient and non-reactive tool for studying collaboration in research. In this paper, several types of collaboration have been identified, and earlier research on collaboration has been reviewed. Further research is needed to refine the methods of defining and assessing collaboration and its impact on the organization of research and communica tion in science.},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of Information Science/1983/Subramanyam_1983_Bibliometric studies of research collaboration.pdf},
  journal = {Journal of Information Science},
  keywords = {fav},
  language = {en},
  number = {1}
}

@article{torre,
  title = {{Nuovi indicatori bibliometrici nella letteratura scientifica: un panorama in continua evoluzione}},
  author = {Torre, G La and Sciarra, I and Chiappetta, M and Monteduro, A},
  pages = {7},
  abstract = {Introduction. Bibliometrics is a science which evaluates the impact of the scientific work of a journal or of an author, using mathematical and statistical tools. Impact Factor (IF) is the first bibliometric parameter created, and after it many others have been progressively conceived in order to go beyond its limits. Currently bibliometric indexes are used for academic purposes, among them to evaluate the eligibility of a researcher to compete for the National Scientific Qualification, in order to access to competitive exams to become professor. Objective. Aim of this study is to identify the most relevant bibliometric indexes and to summarized their characteristics. Methods. A revision of bibliometric indexes as been conducted, starting from the classic ones and completing with the most recent ones. Results. The two most used bibliometric indexes are the IF, which measures the scientific impact of a periodical and bases on Web of Science citation database, and the h-index, which measures the impact of the scientific work of a researcher, basing on Scopus database. Besides them other indexes have been created more recently, such as the SCImago Journal Rank Indicator (SJR), the Source Normalised Impact per Paper (SNIP) and the CiteScore index. They are all based on Scopus database and evaluate, in different ways, the citational impact of a periodic. The i10-index instead is provided from Google Scholar database and allows to evaluate the impact of the scientific production of a researcher. Recently two softwares have been introduced: the first one, Publish or Perish, allows to evaluate the scientific work of a researcher, through the assessment of many indexes; the second one, Altmetric, measure the use in the Web of the academic papers, instead of measuring citations, by means of alternative metrics respect to the traditional ones. Conclusions. Each analized index shows advantages but also criticalities. Therefore the combined use of more than one indexes, citational and not, should be preferred, in order to correctly evaluate the work of reserchers and to finally improve the quality and the development of scientific research. Clin Ter 2017; 168(2):e65-71. doi: 10.7417/CT.2017.1985},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/undefined/Torre et al_Nuovi indicatori bibliometrici nella letteratura scientifica.pdf},
  language = {it}
}

@article{tran2019,
  title = {Global {{Evolution}} of {{Research}} in {{Artificial Intelligence}} in {{Health}} and {{Medicine}}: {{A Bibliometric Study}}},
  shorttitle = {Global {{Evolution}} of {{Research}} in {{Artificial Intelligence}} in {{Health}} and {{Medicine}}},
  author = {Tran, Bach and Vu, Giang and Ha, Giang and Vuong, Quan-Hoang and Ho, Manh-Tung and Vuong, Thu-Trang and La, Viet-Phuong and Ho, Manh-Toan and Nghiem, Kien-Cuong and Nguyen, Huong and Latkin, Carl and Tam, Wilson and Cheung, Ngai-Man and Nguyen, Hong-Kong and Ho, Cyrus and Ho, Roger},
  year = {2019},
  month = mar,
  volume = {8},
  pages = {360},
  issn = {2077-0383},
  doi = {10.3390/jcm8030360},
  abstract = {The increasing application of Artificial Intelligence (AI) in health and medicine has attracted a great deal of research interest in recent decades. This study aims to provide a global and historical picture of research concerning AI in health and medicine. A total of 27,451 papers that were published between 1977 and 2018 (84.6\% were dated 2008\textendash 2018) were retrieved from the Web of Science platform. The descriptive analysis examined the publication volume, and authors and countries collaboration. A global network of authors' keywords and content analysis of related scientific literature highlighted major techniques, including Robotic, Machine learning, Artificial neural network, Artificial intelligence, Natural language process, and their most frequent applications in Clinical Prediction and Treatment. The number of cancer-related publications was the highest, followed by Heart Diseases and Stroke, Vision impairment, Alzheimer's, and Depression. Moreover, the shortage in the research of AI application to some high burden diseases suggests future directions in AI research. This study offers a first and comprehensive picture of the global efforts directed towards this increasingly important and prolific field of research and suggests the development of global and national protocols and regulations on the justification and adaptation of medical AI products.},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of Clinical Medicine/2019/Tran et al_2019_Global Evolution of Research in Artificial Intelligence in Health and Medicine.pdf},
  journal = {Journal of Clinical Medicine},
  language = {en},
  number = {3}
}

@article{vanleeuwen2001,
  title = {The Use of Combined Bibliometric Methods in Research Funding Policy},
  author = {{van Leeuwen}, T N and {van der Wurff}, L J and {van Raan}, A F J},
  year = {2001},
  month = dec,
  volume = {10},
  pages = {195--201},
  issn = {09582029, 14715449},
  doi = {10.3152/147154401781777015},
  file = {/home/gabriel/Dropbox/zotero-library/Research Evaluation/2001/van Leeuwen et al_2001_The use of combined bibliometric methods in research funding policy.pdf},
  journal = {Research Evaluation},
  language = {en},
  number = {3}
}

@article{wallin2005,
  title = {Bibliometric {{Methods}}: {{Pitfalls}} and {{Possibilities}}},
  shorttitle = {Bibliometric {{Methods}}},
  author = {Wallin, Johan A.},
  year = {2005},
  volume = {97},
  pages = {261--275},
  issn = {1742-7843},
  doi = {10.1111/j.1742-7843.2005.pto_139.x},
  abstract = {Abstract: Bibliometric studies are increasingly being used for research assessment. Bibliometric indicators are strongly methodology-dependent but for all of them, various types of data normalization are an indispensable requirement. Bibliometric studies have many pitfalls; technical skill, critical sense and a precise knowledge about the examined scientific domain are required to carry out and interpret bibliometric investigations correctly.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1742-7843.2005.pto\_139.x},
  file = {/home/gabriel/Dropbox/zotero-library/Basic & Clinical Pharmacology & Toxicology/2005/Wallin_2005_Bibliometric Methods.pdf},
  journal = {Basic \& Clinical Pharmacology \& Toxicology},
  keywords = {fav},
  language = {en},
  number = {5}
}

@article{wang2019,
  title = {Which Can Better Predict the Future Success of Articles? {{Bibliometric}} Indices or Alternative Metrics},
  shorttitle = {Which Can Better Predict the Future Success of Articles?},
  author = {Wang, Mingyang and Wang, Zhenyu and Chen, Guangsheng},
  year = {2019},
  month = jun,
  volume = {119},
  pages = {1575--1595},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-019-03052-9},
  abstract = {In this paper, we made a survey on the prediction capability of bibliometric indices and alternative metrics on the future success of articles by establishing a machine learning framework. Twenty-three bibliometric and alternative indices were collected to establish the feature space for the predication task. In order to eliminate the possible redundancy in feature space, three feature selection techniques of Relief-F, principal component analysis and entropy weighted method were used to rank the features according to their contribution to the original data set. Combining the fractal dimension of the data set, the intrinsic features which can better represent the original feature space were extracted. Three classifiers of Na\"ive Bayes, KNN and random forest were performed to detect the classification performance of these features. Experimental results show that both bibliometric indices and alternative metrics are beneficial to articles' growth. Early citation features, early Web usage statistics, as well as the reputation of the first author are the most valuable indicators in making an article more influential in the future.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2019/Wang et al_2019_Which can better predict the future success of articles.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {3}
}

@article{zancanaro2015,
  title = {A Bibliometric Mapping of Open Educational Resources},
  author = {Zancanaro, Airton and Todesco, Jos{\'e} Leomar and Ramos, Fernando},
  year = {2015},
  month = jan,
  volume = {16},
  issn = {1492-3831},
  doi = {10.19173/irrodl.v16i1.1960},
  abstract = {Open educational resources (OER) is a topic that has aroused increasing interest by researchers as a powerful contribution to improve the educational system quality and openness, both in face to face and distance education. The goal of this research is to map publications related to OER, dating from 2002 to 2013, and available through the Web of Science and Scopus scientific databases as well as in the OER Knowledge Cloud open repository. Data were used to explore relevant aspects related to the scientific production in OER, such as: (i) number of publications per year; (ii) most cited publications; (iii) authors with higher number of publications; (iv) institutions and countries with more publications and (v) most referenced bibliography by the authors. The analysis has included 544 papers, written by 843 authors, from 338 institutions, from 61 different countries. Moreover, the analysis has included the publications referenced and the author's keywords, considering 6,355 different publications and 929 different keywords. Besides presenting a bibliographic mapping of the research on OER, this paper also intends to contribute to consolidate the idea that OER is a promising field for researchers, in line with the spreading of the Open movement.},
  file = {/home/gabriel/Dropbox/zotero-library/The International Review of Research in Open and Distributed Learning/2015/Zancanaro et al_2015_A bibliometric mapping of open educational resources.pdf},
  journal = {The International Review of Research in Open and Distributed Learning},
  language = {en},
  number = {1}
}


