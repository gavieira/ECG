
@article{abramo2011,
  title = {Evaluating Research: From Informed Peer Review to Bibliometrics},
  shorttitle = {Evaluating Research},
  author = {Abramo, Giovanni and D'Angelo, Ciriaco Andrea},
  year = {2011},
  month = jun,
  volume = {87},
  pages = {499--514},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-011-0352-7},
  abstract = {National research assessment exercises are becoming regular events in ever more countries. The present work contrasts the peer-review and bibliometrics approaches in the conduct of these exercises. The comparison is conducted in terms of the essential parameters of any measurement system: accuracy, robustness, validity, functionality, time and costs. Empirical evidence shows that for the natural and formal sciences, the bibliometric methodology is by far preferable to peer-review. Setting up national databases of publications by individual authors, derived from Web of Science or Scopus databases, would allow much better, cheaper and more frequent national research assessments.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2011/Abramo_D’Angelo_2011_Evaluating research.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {3}
}

@article{abramo2019,
  title = {Peer Review versus Bibliometrics: {{Which}} Method Better Predicts the Scholarly Impact of Publications?},
  shorttitle = {Peer Review versus Bibliometrics},
  author = {Abramo, Giovanni and D'Angelo, Ciriaco Andrea and Reale, Emanuela},
  year = {2019},
  month = oct,
  volume = {121},
  pages = {537--554},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-019-03184-y},
  abstract = {In this work, we try to answer the question of which method, peer review versus bibliometrics, better predicts the future overall scholarly impact of scientific publications. We measure the agreement between peer review evaluations of Web of Science indexed publications submitted to the first Italian research assessment exercise and long-term citations of the same publications. We do the same for an early citation-based indicator. We find that the latter shows stronger predictive power, i.e. it more reliably predicts late citations in all the disciplinary areas examined, and for any citation time window starting 1 year after publication.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2019/Abramo et al_2019_Peer review versus bibliometrics.pdf},
  journal = {Scientometrics},
  keywords = {todo},
  language = {en},
  number = {1}
}

@book{andres2009,
  title = {Measuring Academic Research: How to Undertake a Bibliometric Study},
  shorttitle = {Measuring Academic Research},
  author = {Andr{\'e}s, Ana},
  year = {2009},
  publisher = {{Chandos Publishing}},
  address = {{Oxford}},
  abstract = {Many analyses have been applied in relation to bibliometric studies, but few have shown how to actually carry out the analysis. This book shows how to undertake a bibliometric study, providing a guide from the first step in which the study has to be set, to the analysis and interpretation},
  annotation = {OCLC: ocn370609754},
  file = {/home/gabriel/Zotero/storage/EEWHWQQ9/Measuring Academic Research. How to Undertake a Bi.pdf},
  isbn = {978-1-84334-528-2 978-1-84334-529-9},
  keywords = {Bibliometría,Bibliometrics,Bibliometrie,Methode,Methodology,Onderzoek},
  lccn = {Z669.8 .A65 2009}
}

@article{asongu2016,
  title = {A {{Brief Future}} of {{Time}} in the {{Monopoly}} of {{Scientific Knowledge}}},
  author = {Asongu, Simplice A. and Nwachukwu, Jacinta C.},
  year = {2016},
  month = dec,
  volume = {58},
  pages = {638--671},
  issn = {1478-3320},
  doi = {10.1057/s41294-016-0008-y},
  abstract = {This paper provides global empirical evidence on cross-country differences in scientific and technical publications. Its purpose is to model the future of scientific knowledge monopoly in order to understand whether the impressive growth experienced by latecomers in the industry has been accompanied by a similar catch-up in scientific capabilities and knowledge contribution. The empirical evidence for the period 1994\textendash 2010 is based on 41 panels which together consist of 99 countries. The large dataset allows us to disaggregate countries into fundamental characteristics based on income levels (high-income, lower-middle-income, upper-middle-income and low-income), legal origins (English common-law, French civil-law, German civil-law and Scandinavian civil-law) and regional proximity (South Asia, Europe and Central Asia; East Asia and the Pacific; Middle East and North Africa; Latin America and the Caribbean and Sub-Saharan Africa). Three main issues are investigated: the presence or not of catch-up processes, the speed of the catch-up processes and the time needed for a complete elimination of country differences in scientific and technical publications. The findings based on absolute and conditional catch-up patterns broadly show that advanced countries will continue to dominate in scientific knowledge contribution. Policy implications are discussed.},
  file = {/home/gabriel/Dropbox/zotero-library/Asongu_Nwachukwu_2016_A Brief Future of Time in the Monopoly of Scientific Knowledge.pdf},
  journal = {Comparative Economic Studies},
  keywords = {todo},
  language = {en},
  number = {4}
}

@book{ball2017,
  title = {An {{Introduction}} to {{Bibliometrics}}: {{New Development}} and {{Trends}}},
  shorttitle = {An {{Introduction}} to {{Bibliometrics}}},
  author = {Ball, Rafael},
  year = {2017},
  month = sep,
  publisher = {{Chandos Publishing}},
  abstract = {An Introduction to Bibliometrics: New Development and Trends provides a comprehensible, readable and easy to read introduction to bibliometrics. Importantly, the book surveys the latest developments of bibliometrics (such as altmetrics, etc.) and how the field is likely to change over the next decade. In the literature, bibliometrics is generally discussed from one of two perspectives: (1) Purely mathematical/statistical or (2) Its sociological implications. Both approaches are very far from how most users want to apply bibliometrics. This book fills that need by providing tactics on how bibliometrics can be applied to their sphere of scientific activity.Provides readers with an understanding of bibliometric indicators, including their background and significance, classification in quantitative performance, and an evaluation of science and researchIncludes an overview of the most important indicators, their areas of application, and where and when they should and should not be usedDiscusses future trends in the quantitative performance evaluation of scientific research},
  googlebooks = {wrlvDgAAQBAJ},
  isbn = {978-0-08-102151-4},
  keywords = {Computers / Business \& Productivity Software / General,Social Science / General},
  language = {en}
}

@article{belter2015,
  title = {Bibliometric Indicators: Opportunities and Limits},
  shorttitle = {Bibliometric Indicators},
  author = {Belter, Christopher W.},
  year = {2015},
  month = oct,
  volume = {103},
  pages = {219--221},
  issn = {1536-5050},
  doi = {10.3163/1536-5050.103.4.014},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of the Medical Library Association  JMLA/2015/Belter_2015_Bibliometric indicators.pdf},
  journal = {Journal of the Medical Library Association : JMLA},
  keywords = {fav,top5},
  number = {4},
  pmcid = {PMC4613388},
  pmid = {26512227}
}

@article{blumel2020,
  title = {Studying Review Articles in Scientometrics and beyond: A Research Agenda},
  shorttitle = {Studying Review Articles in Scientometrics and Beyond},
  author = {Bl{\"u}mel, Clemens and Schniedermann, Alexander},
  year = {2020},
  month = jul,
  volume = {124},
  pages = {711--728},
  issn = {1588-2861},
  doi = {10.1007/s11192-020-03431-7},
  abstract = {Review articles are an often neglected genre in scholarly communication. Though there was intense discussion about review articles in scientometrics in the 1970s and 1980s, we find less studies devoted to this genre within the last 20~years. Yet, recent discussions in other fields, such as linguistics, sociology or medicine imply that review articles are part of important debates about problems of research in academia, such as research quality or transparency. Against that background, the purpose of this paper is to review recent developments for the study of review articles in scientometrics and beyond, to discuss theoretical, conceptual and empirical accounts of how review articles can be defined, and to identify major methodological and conceptual challenges for studying review articles. Based on reviewing work and inputs received from of a workshop conducted at a Conference of the International Society of Informetrics in September 2019, we propose a research agenda for the study of review articles. We have identified six realms of study in this area: (1) the study of methodological caveats resulting from the usage of scholarly databases, (2) the study of field specific patterns of reception and usage of review articles, (3) the study of argumentative and textual structures of review articles, (4) the exploration of organizations and infrastructures for review articles, (5) the study of epistemic roles of review articles, and (6) the analysis of authorship patterns in review articles.},
  file = {/home/gabriel/Dropbox/zotero-library/Blümel_Schniedermann_2020_Studying review articles in scientometrics and beyond.pdf},
  journal = {Scientometrics},
  keywords = {todo},
  language = {en},
  number = {1}
}

@article{bornmann2016a,
  title = {To What Extent Does the {{Leiden}} Manifesto Also Apply to Altmetrics? {{A}} Discussion of the Manifesto against the Background of Research into Altmetrics},
  shorttitle = {To What Extent Does the {{Leiden}} Manifesto Also Apply to Altmetrics?},
  author = {Bornmann, Lutz and Haunschild, Robin},
  year = {2016},
  month = aug,
  volume = {40},
  pages = {529--543},
  doi = {10.1108/OIR-09-2015-0314},
  abstract = {Purpose \textendash{} Hicks et al. (2015) have formulated the so-called Leiden manifesto, in which they have assembled the ten principles for a meaningful evaluation of research on the basis of bibliometric data. The paper aims to discuss this issue. Design/methodology/approach \textendash{} In this work the attempt is made to indicate the relevance of the Leiden manifesto for altmetrics. Findings \textendash{} As shown by the discussion of the ten principles against the background of the knowledge about and the research into altmetrics, the principles also have a great importance for altmetrics and should be taken into account in their application. Originality/value \textendash{} Altmetrics is already frequently used in the area of research evaluation. Thus, it is important that the user of altmetrics data knows the relevance of the Leiden manifesto also in this area.},
  file = {/home/gabriel/Dropbox/zotero-library/Bornmann_Haunschild_2016_To what extent does the Leiden manifesto also apply to altmetrics.pdf},
  journal = {Online Information Review},
  keywords = {todo}
}

@article{bornmann2018,
  title = {Critical Rationalism and the Search for Standard (Field-Normalized) Indicators in Bibliometrics},
  author = {Bornmann, Lutz and Marx, Werner},
  year = {2018},
  month = aug,
  volume = {12},
  pages = {598--604},
  issn = {1751-1577},
  doi = {10.1016/j.joi.2018.05.002},
  abstract = {Bibliometrics plays an increasingly important role in research evaluation. However, no gold standard exists for a set of reliable and valid (field-normalized) impact indicators in research evaluation. This opinion paper recommends that bibliometricians develop and analyze these impact indicators against the backdrop of Popper's critical rationalism. The studies critically investigating the indicators should publish the results in such a way that they can be included in meta-analyses. The results of meta-analyses give guidance on which indicators can then be part of a set of indicators used as standard in bibliometrics. The generation and continuous revision of the standard set could be handled by the International Society for Informetrics and Scientometrics (ISSI).},
  file = {/home/gabriel/Dropbox/zotero-library/Bornmann_Marx_2018_Critical rationalism and the search for standard (field-normalized) indicators.pdf},
  journal = {Journal of Informetrics},
  keywords = {Bibliometrics,Field-normalized indicators,Standards},
  language = {en},
  number = {3}
}

@article{butler2017,
  title = {The {{Evolution}} of {{Current Research Impact Metrics}}: {{From Bibliometrics}} to {{Altmetrics}}?},
  shorttitle = {The {{Evolution}} of {{Current Research Impact Metrics}}},
  author = {Butler, Joseph S. and Kaye, I. David and Sebastian, Arjun S. and Wagner, Scott C. and Morrissey, Patrick B. and Schroeder, Gregory D. and Kepler, Christopher K. and Vaccaro, Alexander R.},
  year = {2017},
  month = jun,
  volume = {30},
  pages = {226--228},
  issn = {2380-0186},
  doi = {10.1097/BSD.0000000000000531},
  abstract = {The prestige of publication has been based on traditional citation metrics, most commonly journal impact factor. However, the Internet has radically changed the speed, flow, and sharing of medical information. Furthermore, the explosion of social media, along with development of popular professional and scientific websites and blogs, has led to the need for alternative metrics, known as altmetrics, to quantify the wider impact of research. We explore the evolution of current research impact metrics and examine the evolving role of altmetrics in measuring the wider impact of research. We suggest that altmetrics used in research evaluation should be part of an informed peer-review process such as traditional metrics. Moreover, results based on altmetrics must not lead to direct decision making about research, but instead, should be used to assist experts in making decisions. Finally, traditional and alternative metrics should complement, not replace, each other in the peer-review process.},
  file = {/home/gabriel/Dropbox/zotero-library/Clinical Spine Surgery/2017/Butler et al_2017_The Evolution of Current Research Impact Metrics.pdf},
  journal = {Clinical Spine Surgery},
  language = {en-US},
  number = {5}
}

@article{cabanac2021,
  title = {Day-to-Day Discovery of Preprint\textendash Publication Links},
  author = {Cabanac, Guillaume and Oikonomidi, Theodora and Boutron, Isabelle},
  year = {2021},
  month = apr,
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03900-7},
  abstract = {Preprints promote the open and fast communication of non-peer reviewed work. Once a preprint is published in a peer-reviewed venue, the preprint server updates its web page: a prominent hyperlink leading to the newly published work is added. Linking preprints to publications is of utmost importance as it provides readers with the latest version of a now certified work. Yet leading preprint servers fail to identify all existing preprint\textendash publication links. This limitation calls for a more thorough approach to this critical information retrieval task: overlooking published evidence translates into partial and even inaccurate systematic reviews on health-related issues, for instance. We designed an algorithm leveraging the Crossref public and free source of bibliographic metadata to comb the literature for preprint\textendash publication links. We tested it on a reference preprint set identified and curated for a living systematic review on interventions for preventing and treating COVID-19 performed by international collaboration: the COVID-NMA initiative (covid-nma.com). The reference set comprised 343 preprints, 121 of which appeared as a publication in a peer-reviewed journal. While the preprint servers identified 39.7\% of the preprint\textendash publication links, our linker identified 90.9\% of the expected links with no clues taken from the preprint servers. The accuracy of the proposed linker is 91.5\% on this reference set, with 90.9\% sensitivity and 91.9\% specificity. This is a 16.26\% increase in accuracy compared to that of preprint servers. We release this software as supplementary material to foster its integration into preprint servers' workflows and enhance a daily preprint\textendash publication chase that is useful to all readers, including systematic reviewers. This preprint\textendash publication linker currently provides day-to-day updates to the biomedical experts of the COVID-NMA initiative.},
  file = {/home/gabriel/Dropbox/zotero-library/Cabanac et al_2021_Day-to-day discovery of preprint–publication links.pdf},
  journal = {Scientometrics},
  language = {en}
}

@article{cabezas-clavijo2013,
  title = {Reviewers' Ratings and Bibliometric Indicators: Hand in Hand When Assessing over Research Proposals?},
  shorttitle = {Reviewers' Ratings and Bibliometric Indicators},
  author = {{Cabezas-Clavijo}, Alvaro and {Robinson-Garc{\'i}a}, Nicol{\'a}s and Escabias, Manuel and {Jim{\'e}nez-Contreras}, Evaristo},
  year = {2013},
  volume = {8},
  pages = {e68258},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0068258},
  abstract = {BACKGROUND: The peer review system has been traditionally challenged due to its many limitations especially for allocating funding. Bibliometric indicators may well present themselves as a complement. OBJECTIVE: We analyze the relationship between peers' ratings and bibliometric indicators for Spanish researchers in the 2007 National R\&D Plan for 23 research fields. METHODS AND MATERIALS: We analyze peers' ratings for 2333 applications. We also gathered principal investigators' research output and impact and studied the differences between accepted and rejected applications. We used the Web of Science database and focused on the 2002-2006 period. First, we analyzed the distribution of granted and rejected proposals considering a given set of bibliometric indicators to test if there are significant differences. Then, we applied a multiple logistic regression analysis to determine if bibliometric indicators can explain by themselves the concession of grant proposals. RESULTS: 63.4\% of the applications were funded. Bibliometric indicators for accepted proposals showed a better previous performance than for those rejected; however the correlation between peer review and bibliometric indicators is very heterogeneous among most areas. The logistic regression analysis showed that the main bibliometric indicators that explain the granting of research proposals in most cases are the output (number of published articles) and the number of papers published in journals that belong to the first quartile ranking of the Journal Citations Report. DISCUSSION: Bibliometric indicators predict the concession of grant proposals at least as well as peer ratings. Social Sciences and Education are the only areas where no relation was found, although this may be due to the limitations of the Web of Science's coverage. These findings encourage the use of bibliometric indicators as a complement to peer review in most of the analyzed areas.},
  file = {/home/gabriel/Dropbox/zotero-library/PloS One/2013/Cabezas-Clavijo et al_2013_Reviewers' ratings and bibliometric indicators.pdf},
  journal = {PloS One},
  keywords = {Bibliometrics,Biomedical Research,Databases; Factual,fav,Financing; Organized,Humans,Publications,Publishing,Research Design,Research Personnel,Spain},
  language = {eng},
  number = {6},
  pmcid = {PMC3695904},
  pmid = {23840840}
}

@book{cantu-ortiz2017,
  title = {Research {{Analytics}}: {{Boosting University Productivity}} and {{Competitiveness}} through {{Scientometrics}}},
  shorttitle = {Research {{Analytics}}},
  author = {{Cantu-Ortiz}, Francisco J.},
  year = {2017},
  month = oct,
  publisher = {{CRC Press}},
  abstract = {The growth of machines and users of the Internet has led to the proliferation of all sorts of data concerning individuals, institutions, companies, governments, universities, and all kinds of known objects and events happening everywhere in daily life. Scientific knowledge is not an exception to the data boom. The phenomenon of data growth in science pushes forth as the number of scientific papers published doubles every 9\textendash 15 years, and the need for methods and tools to understand what is reported in scientific literature becomes evident.  As the number of academicians and innovators swells, so do the number of publications of all types, yielding outlets of documents and depots of authors and institutions that need to be found in Bibliometric databases. These databases are dug into and treated to hand over metrics of research performance by means of Scientometrics that analyze the toil of individuals, institutions, journals, countries, and even regions of the world.  The objective of this book is to assist students, professors, university managers, government, industry, and stakeholders in general, understand which are the main Bibliometric databases, what are the key research indicators, and who are the main players in university rankings and the methodologies and approaches that they employ in producing ranking tables.  The book is divided into two sections. The first looks at Scientometric databases, including Scopus and Google Scholar as well as institutional repositories. The second section examines the application of Scientometrics to world-class universities and the role that Scientometrics can play in competition among them. It looks at university rankings and the methodologies used to create these rankings. Individual chapters examine specific rankings that include:   QS World University Scimago Institutions Webometrics U-Multirank U.S. News \& World Report The book concludes with a discussion of university performance in the age of research analytics.},
  file = {/home/gabriel/Dropbox/zotero-library/Cantu-Ortiz_2017_Research Analytics.pdf},
  googlebooks = {lD0PEAAAQBAJ},
  isbn = {978-1-4987-8638-6},
  keywords = {Business \& Economics / Government \& Business,Computers / Data Science / Data Analytics,Computers / Information Technology,Mathematics / Probability \& Statistics / General},
  language = {en}
}

@article{chahrour2020,
  title = {A {{Bibliometric Analysis}} of {{COVID}}-19 {{Research Activity}}: {{A Call}} for {{Increased Output}}},
  shorttitle = {A {{Bibliometric Analysis}} of {{COVID}}-19 {{Research Activity}}},
  author = {Chahrour, Mohamad and Assi, Sahar and Bejjani, Michael and Nasrallah, Ali A and Salhab, Hamza and Fares, Mohamad Y and Khachfe, Hussein H},
  year = {2020},
  month = mar,
  issn = {2168-8184},
  doi = {10.7759/cureus.7357},
  abstract = {Background: The novel coronavirus disease 2019 (COVID-19) has impacted many countries across all inhabited continents, and is now considered a global pandemic, due to its high rate of infectivity. Research related to this disease is pivotal for assessing pathogenic characteristics and formulating therapeutic strategies. The aim of this paper is to explore the activity and trends of COVID-19 research since its outbreak in December 2019. Methods: We explored the PubMed database and the World Health Organization (WHO) database for publications pertaining to COVID-19 since December 2019 up until March 18, 2020. Only relevant observational and interventional studies were included in our study. Data on COVID-19 incidence were extracted from the WHO situation reports. Research output was assessed with respect to gross domestic product (GDP) and population of each country. Results: Only 564 publications met our inclusion criteria. These articles came from 39 different countries, constituting 24\% of all affected countries. China produced the greatest number of publications with 377 publications (67\%). With respect to continental research activity, Asian countries had the highest research activity with 434 original publications (77\%). In terms of publications per million persons (PPMPs), Singapore had the highest number of publications with 1.069 PPMPs. In terms of publications per billion-dollar GDP, Mauritius ranked first with 0.075. Received 03/19/2020 Review began 03/20/2020 Review ended 03/20/2020 Published 03/21/2020 \textcopyright{} Copyright 2020 Chahrour et al. This is an open access article distributed under the terms of the Creative Commons Attribution License CC-BY 4.0., which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Conclusion: COVID-19 is a major disease that has impacted international public health on a global level. Observational studies and therapeutic trials pertaining to COVID-19 are essential for assessing pathogenic characteristics and developing novel treatment options.},
  file = {/home/gabriel/Dropbox/zotero-library/Cureus/2020/Chahrour et al_2020_A Bibliometric Analysis of COVID-19 Research Activity.pdf},
  journal = {Cureus},
  language = {en}
}

@article{coelho2019,
  title = {Decentralising Scientific Publishing: Can the Blockchain Improve Science Communication?},
  shorttitle = {Decentralising Scientific Publishing},
  author = {Coelho, Fl{\'a}vio Code{\c c}o and Brand{\~a}o, Adeilton},
  year = {2019},
  volume = {114},
  pages = {e190257},
  issn = {1678-8060, 0074-0276},
  doi = {10.1590/0074-02760190257},
  file = {/home/gabriel/Dropbox/zotero-library/Coelho_Brandão_2019_Decentralising scientific publishing.pdf},
  journal = {Mem\'orias do Instituto Oswaldo Cruz},
  keywords = {todo},
  language = {en}
}

@article{craig2007,
  title = {Do {{Open Access Articles Have Greater Citation Impact}}? {{A Critical Review}} of the {{Literature}}},
  shorttitle = {Do {{Open Access Articles Have Greater Citation Impact}}?},
  author = {Craig, Iain and Plume, Andrew and Mcveigh, Marie and Pringle, James},
  year = {2007},
  month = jul,
  volume = {1},
  pages = {239--248},
  doi = {10.1016/j.joi.2007.04.001},
  abstract = {The last few years have seen the emergence of several open access options in scholarly communication which can broadly be grouped into two areas referred to as `gold' and `green' open access (OA). In this article we review the literature examining the relationship between OA status and citation counts of scholarly articles. Early studies showed a correlation between the free online availability or OA status of articles and higher citation counts, and implied causality without due consideration of potential confounding factors. More recent investigations have dissected the nature of the relationship between article OA status and citations. Three non-exclusive postulates have been proposed to account for the observed citation differences between OA and non-OA articles: an open access postulate, a selection bias postulate, and an early view postulate. The most rigorous study to date (in condensed matter physics) showed that, after controlling for the early view postulate, the remaining difference in citation counts between OA and non-OA articles is explained by the selection bias postulate. No evidence was found to support the OA postulate per se; i.e. article OA status alone has little or no effect on citations. Further studies using a similarly rigorous approach are required to determine the generality of this finding.},
  file = {/home/gabriel/Dropbox/zotero-library/Craig et al_2007_Do Open Access Articles Have Greater Citation Impact.pdf},
  journal = {J. Informetrics},
  keywords = {todo}
}

@article{das2017,
  title = {Citation Impact: {{Manipulation}} and Monopoly},
  shorttitle = {Citation Impact},
  author = {Das, KusalK},
  year = {2017},
  volume = {2},
  pages = {67},
  issn = {2468-838X},
  doi = {10.4103/bjhs.bjhs_34_17},
  file = {/home/gabriel/Dropbox/zotero-library/Das_2017_Citation impact.pdf},
  journal = {BLDE University Journal of Health Sciences},
  keywords = {todo},
  language = {en},
  number = {2}
}

@article{durieux2010,
  title = {Bibliometric {{Indicators}}: {{Quality Measurements}} of {{Scientific Publication}}},
  shorttitle = {Bibliometric {{Indicators}}},
  author = {Durieux, Val{\'e}rie and Gevenois, Pierre Alain},
  year = {2010},
  month = apr,
  volume = {255},
  pages = {342--351},
  publisher = {{Radiological Society of North America}},
  issn = {0033-8419},
  doi = {10.1148/radiol.09090626},
  abstract = {Bibliometrics is a set of mathematical and statistical methods used to analyze and measure the quantity and quality of books, articles, and other forms of publications. There are three types of bibliometric indicators: quantity indicators, which measure the productivity of a particular researcher; quality indicators, which measure the quality (or ``performance'') of a researcher's output; and structural indicators, which measure connections between publications, authors, and areas of research. Bibliometric indicators are especially important for researchers and organizations, as these measurements are often used in funding decisions, appointments, and promotions of researchers. As more and more scientific discoveries occur and published research results are read and then quoted by other researchers, bibliometric indicators are becoming increasingly important. This article provides an overview of the currently used bibliometric indicators and summarizes the critical elements and characteristics one should be aware of when evaluating the quantity and quality of scientific output.\textcopyright{} RSNA, 2010},
  file = {/home/gabriel/Dropbox/zotero-library/Radiology/2010/Durieux_Gevenois_2010_Bibliometric Indicators.pdf},
  journal = {Radiology},
  keywords = {fav,top5},
  number = {2}
}

@article{eysenbach2006,
  title = {Citation {{Advantage}} of {{Open Access Articles}}},
  author = {Eysenbach, Gunther},
  year = {2006},
  month = may,
  volume = {4},
  pages = {e157},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0040157},
  abstract = {Open access (OA) to the research literature has the potential to accelerate recognition and dissemination of research findings, but its actual effects are controversial. This was a longitudinal bibliometric analysis of a cohort of OA and non-OA articles published between June 8, 2004, and December 20, 2004, in the same journal (PNAS: Proceedings of the National Academy of Sciences). Article characteristics were extracted, and citation data were compared between the two groups at three different points in time: at ``quasi-baseline'' (December 2004, 0\textendash 6 mo after publication), in April 2005 (4\textendash 10 mo after publication), and in October 2005 (10\textendash 16 mo after publication). Potentially confounding variables, including number of authors, authors' lifetime publication count and impact, submission track, country of corresponding author, funding organization, and discipline, were adjusted for in logistic and linear multiple regression models. A total of 1,492 original research articles were analyzed: 212 (14.2\% of all articles) were OA articles paid by the author, and 1,280 (85.8\%) were non-OA articles. In April 2005 (mean 206 d after publication), 627 (49.0\%) of the non-OA articles versus 78 (36.8\%) of the OA articles were not cited (relative risk = 1.3 [95\% Confidence Interval: 1.1\textendash 1.6]; p = 0.001). 6 mo later (mean 288 d after publication), non-OA articles were still more likely to be uncited (non-OA: 172 [13.6\%], OA: 11 [5.2\%]; relative risk = 2.6 [1.4\textendash 4.7]; p {$<$} 0.001). The average number of citations of OA articles was higher compared to non-OA articles (April 2005: 1.5 [SD = 2.5] versus 1.2 [SD = 2.0]; Z = 3.123; p = 0.002; October 2005: 6.4 [SD = 10.4] versus 4.5 [SD = 4.9]; Z = 4.058; p {$<$} 0.001). In a logistic regression model, controlling for potential confounders, OA articles compared to non-OA articles remained twice as likely to be cited (odds ratio = 2.1 [1.5\textendash 2.9]) in the first 4\textendash 10 mo after publication (April 2005), with the odds ratio increasing to 2.9 (1.5\textendash 5.5) 10\textendash 16 mo after publication (October 2005). Articles published as an immediate OA article on the journal site have higher impact than self-archived or otherwise openly accessible OA articles. We found strong evidence that, even in a journal that is widely available in research libraries, OA articles are more immediately recognized and cited by peers than non-OA articles published in the same journal. OA is likely to benefit science by accelerating dissemination and uptake of research findings.},
  file = {/home/gabriel/Dropbox/zotero-library/Eysenbach_2006_Citation Advantage of Open Access Articles.pdf},
  journal = {PLOS Biology},
  keywords = {Bibliometrics,Citation analysis,Institutional repositories,Internet,Open access publishing,Peer review,Scientific publishing,Scientists},
  language = {en},
  number = {5}
}

@misc{fiormonte,
  title = {Knowledge {{Monopolies}} and {{Global Academic Publishing}}},
  author = {Fiormonte, Domenico and {University of Roma Tre, Italy} and Priego, Ernesto and {City University London, UK}},
  doi = {10.15200/winn.147220.00404},
  file = {/home/gabriel/Dropbox/zotero-library/Fiormonte et al_Knowledge Monopolies and Global Academic Publishing.pdf},
  keywords = {todo},
  language = {en}
}

@article{fortunato2018,
  title = {Science of Science},
  author = {Fortunato, Santo and Bergstrom, Carl T. and B{\"o}rner, Katy and Evans, James A. and Helbing, Dirk and Milojevi{\'c}, Sta{\v s}a and Petersen, Alexander M. and Radicchi, Filippo and Sinatra, Roberta and Uzzi, Brian and Vespignani, Alessandro and Waltman, Ludo and Wang, Dashun and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2018},
  month = mar,
  volume = {359},
  issn = {0036-8075},
  doi = {10.1126/science.aao0185},
  abstract = {The complexity of science. Science can be seen as an expanding and evolving network of ideas, scholars and papers. SciSci searches for universal and domain-specific laws underlying the structure and dynamics of science.,},
  file = {/home/gabriel/Dropbox/zotero-library/Fortunato et al_2018_Science of science.pdf},
  journal = {Science (New York, N.Y.)},
  number = {6379},
  pmcid = {PMC5949209},
  pmid = {29496846}
}

@article{garner2018,
  title = {Bibliometric Indices: Defining Academic Productivity and Citation Rates of Researchers, Departments and Journals},
  shorttitle = {Bibliometric Indices},
  author = {Garner, Rebecca M. and Hirsch, Joshua A. and Albuquerque, Felipe C. and Fargen, Kyle M.},
  year = {2018},
  month = feb,
  volume = {10},
  pages = {102--106},
  issn = {1759-8486},
  doi = {10.1136/neurintsurg-2017-013265},
  abstract = {There has been an increasing focus on academic productivity for the purposes of promotion and funding within departments and institutions but also for comparison of individuals, institutions, specialties, and journals. A number of quantitative indices are used to investigate and compare academic productivity. These include various calculations attempting to analyze the number and citations of publications in order to capture both the quality and quantity of publications, such as the h index, the e index, impact factor, and Eigenfactor score. The indices have varying advantages and limitations and thus a basic knowledge is required in order to understand their potential utility within academic medicine. This article describes the various bibliometric indices and discusses recent applications of these metrics within the neurological sciences.},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of Neurointerventional Surgery/2018/Garner et al_2018_Bibliometric indices.pdf},
  journal = {Journal of Neurointerventional Surgery},
  keywords = {academic productivity,bibliometrics,Bibliometrics,Biomedical Research,citation analysis,Efficiency,fav,h index,Humans,Journal Impact Factor,Neurosciences,Periodicals as Topic,top5},
  language = {eng},
  number = {2},
  pmid = {28824008}
}

@article{geuna2003,
  title = {University {{Research Evaluation}} and {{Funding}}: {{An International Comparison}}},
  shorttitle = {University {{Research Evaluation}} and {{Funding}}},
  author = {Geuna, Aldo and Martin, Ben R.},
  year = {2003},
  volume = {41},
  pages = {277--304},
  issn = {0026-4695},
  doi = {10.1023/B:MINE.0000005155.70870.bd},
  abstract = {Many countries have introduced evaluations of university research, reflecting global demands for greater accountability. This paper compares methods of evaluation used across twelve countries in Europe and the Asia-Pacific region. On the basis of this comparison, and focusing in particular on Britain, we examine the advantages and disadvantages of performance-based funding in comparison with other approaches to funding. Our analysis suggests that, while initial benefits may outweigh the costs, over time such a system seems to produce diminishing returns. This raises important questions about its continued use.},
  file = {/home/gabriel/Dropbox/zotero-library/Minerva/2003/Geuna_Martin_2003_University Research Evaluation and Funding.pdf},
  journal = {Minerva},
  language = {en},
  number = {4}
}

@book{gingras2016,
  title = {Bibliometrics and {{Research Evaluation}}: {{Uses}} and {{Abuses}}},
  shorttitle = {Bibliometrics and {{Research Evaluation}}},
  author = {Gingras, Yves},
  year = {2016},
  month = sep,
  publisher = {{MIT Press}},
  abstract = {Why bibliometrics is useful for understanding the global dynamics of science but generate perverse effects when applied inappropriately in research evaluation and university rankings.The research evaluation market is booming. ``Ranking,'' ``metrics,'' ``h-index,'' and ``impact factors'' are reigning buzzwords. Government and research administrators want to evaluate everything\textemdash teachers, professors, training programs, universities\textemdash using quantitative indicators. Among the tools used to measure ``research excellence,'' bibliometrics\textemdash aggregate data on publications and citations\textemdash has become dominant. Bibliometrics is hailed as an ``objective'' measure of research quality, a quantitative measure more useful than ``subjective'' and intuitive evaluation methods such as peer review that have been used since scientific papers were first published in the seventeenth century. In this book, Yves Gingras offers a spirited argument against an unquestioning reliance on bibliometrics as an indicator of research quality. Gingras shows that bibliometric rankings have no real scientific validity, rarely measuring what they pretend to.Although the study of publication and citation patterns, at the proper scales, can yield insights on the global dynamics of science over time, ill-defined quantitative indicators often generate perverse and unintended effects on the direction of research. Moreover, abuse of bibliometrics occurs when data is manipulated to boost rankings. Gingras looks at the politics of evaluation and argues that using numbers can be a way to control scientists and diminish their autonomy in the evaluation process. Proposing precise criteria for establishing the validity of indicators at a given scale of analysis, Gingras questions why universities are so eager to let invalid indicators influence their research strategy.},
  googlebooks = {0aExDQAAQBAJ},
  isbn = {978-0-262-33766-3},
  keywords = {Education / Evaluation \& Assessment,Language Arts \& Disciplines / Library \& Information Science / General},
  language = {en}
}

@book{gingras2016a,
  title = {Bibliometrics and {{Research Evaluation}}: {{Uses}} and {{Abuses}}},
  shorttitle = {Bibliometrics and {{Research Evaluation}}},
  author = {Gingras, Yves},
  year = {2016},
  month = sep,
  publisher = {{MIT Press}},
  abstract = {Why bibliometrics is useful for understanding the global dynamics of science but generate perverse effects when applied inappropriately in research evaluation and university rankings.The research evaluation market is booming. ``Ranking,'' ``metrics,'' ``h-index,'' and ``impact factors'' are reigning buzzwords. Government and research administrators want to evaluate everything\textemdash teachers, professors, training programs, universities\textemdash using quantitative indicators. Among the tools used to measure ``research excellence,'' bibliometrics\textemdash aggregate data on publications and citations\textemdash has become dominant. Bibliometrics is hailed as an ``objective'' measure of research quality, a quantitative measure more useful than ``subjective'' and intuitive evaluation methods such as peer review that have been used since scientific papers were first published in the seventeenth century. In this book, Yves Gingras offers a spirited argument against an unquestioning reliance on bibliometrics as an indicator of research quality. Gingras shows that bibliometric rankings have no real scientific validity, rarely measuring what they pretend to.Although the study of publication and citation patterns, at the proper scales, can yield insights on the global dynamics of science over time, ill-defined quantitative indicators often generate perverse and unintended effects on the direction of research. Moreover, abuse of bibliometrics occurs when data is manipulated to boost rankings. Gingras looks at the politics of evaluation and argues that using numbers can be a way to control scientists and diminish their autonomy in the evaluation process. Proposing precise criteria for establishing the validity of indicators at a given scale of analysis, Gingras questions why universities are so eager to let invalid indicators influence their research strategy.},
  file = {/home/gabriel/Zotero/storage/UBMPC2N6/Bibliometrics and Research Evaluation Uses and Ab.pdf},
  googlebooks = {0aExDQAAQBAJ},
  isbn = {978-0-262-33766-3},
  keywords = {Education / Evaluation \& Assessment,Language Arts \& Disciplines / Library \& Information Science / General},
  language = {en}
}

@article{glanzel2002,
  title = {Journal Impact Measures in Bibliometric Research},
  author = {Gl{\"a}nzel, Wolfgang and Moed, Henk F},
  year = {2002},
  pages = {23},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/2002/Glänzel_Moed_2002_Journal impact measures in bibliometric research.pdf},
  keywords = {fav,top5},
  language = {en}
}

@article{glanzel2008,
  title = {Seven {{Myths}} in {{Bibliometrics About}} Facts and Fiction in Quantitative Science Studies},
  author = {Gl{\"a}nzel, Wolfgang},
  year = {2008},
  month = jun,
  volume = {2},
  pages = {9--17},
  issn = {0973-7766, 2168-930X},
  doi = {10.1080/09737766.2008.10700836},
  file = {/home/gabriel/Dropbox/zotero-library/Collnet Journal of Scientometrics and Information Management/2008/Glänzel_2008_Seven Myths in Bibliometrics About facts and fiction in quantitative science.pdf},
  journal = {Collnet Journal of Scientometrics and Information Management},
  keywords = {fav,top5},
  language = {en},
  number = {1}
}

@article{godin2006,
  title = {On the Origins of Bibliometrics},
  author = {Godin, Beno{\^i}t},
  year = {2006},
  month = jul,
  volume = {68},
  pages = {109--133},
  publisher = {{Akad\'emiai Kiad\'o, co-published with Springer Science+Business Media B.V., Formerly Kluwer Academic Publishers B.V.}},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-006-0086-0},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d801795152e136"{$>$}Summary~~{$<$}/h2{$><$}p{$>$}Among the many statistics on science, called scientometrics, bibliometrics holds a privileged place. Bibliometrics is one  of the few subfields concerned with measuring the output side of science. According to most ``histories'', bibliometrics owes  its systematic development mainly to D.J.D. Price and Eugene Garfield, as founders. The few works conducted before the 1950s  are usually relegated to prehistory. This paper documents how the systematic counting of publications originated with psychologists.  In the early 1900s, psychologists began collecting statistics on their discipline. Publications came to be counted in addresses,  reviews and histories of psychology for several decades. The aim was to contribute to the advancement of psychology. Far from  being a negligible output of a prehistoric type, both the volume and the systematicness of these efforts are witnesses to  what should be considered as pioneering work, and their authors considered as forerunners to bibliometrics.  {$<$}/p{$><$}/section{$>$}},
  chapter = {Scientometrics},
  journal = {Scientometrics},
  language = {en\_US},
  number = {1}
}

@book{goldfinch2012,
  title = {Prometheus {{Assessed}}?: {{Research Measurement}}, {{Peer Review}}, and {{Citation Analysis}}},
  shorttitle = {Prometheus {{Assessed}}?},
  author = {Goldfinch, Shaun and Yamamoto, Kiyoshi},
  year = {2012},
  month = apr,
  publisher = {{Elsevier}},
  abstract = {This book examines the problems, pitfalls and opportunities of different models of assessing research quality, drawing on studies from around the world. Aimed at academics, education officials and public servants, key features include an overview of the argument of whether research should be assessed and how research quality should be determined. Prometheus Assessed? offers a survey of research assessment models in the US, UK, Japan and New Zealand and includes an examination of citation analysis and comparison between the different models.Should research be assessed and what is research quality?Survey of research assessment models in US, UK, Japan and New ZealandExamination of citation analysis},
  file = {/home/gabriel/Zotero/storage/99Q3CYSF/Prometheus Assessed. Research Measurement, Peer R.pdf},
  googlebooks = {h19EAgAAQBAJ},
  isbn = {978-1-78063-301-5},
  keywords = {Language Arts \& Disciplines / Library \& Information Science / General},
  language = {en}
}

@article{gonzalez-betancor2019,
  title = {Publication Modalities `Article in Press' and `Open Access' in Relation to Journal Average Citation},
  author = {{Gonz{\'a}lez-Betancor}, Sara M. and {Dorta-Gonz{\'a}lez}, Pablo},
  year = {2019},
  month = sep,
  volume = {120},
  pages = {1209--1223},
  issn = {1588-2861},
  doi = {10.1007/s11192-019-03156-2},
  abstract = {There has been a generalization in the use of two publication practices by scientific journals during the past decade: (1) `article in press' or early view, which allows access to the accepted paper before its formal publication in an issue; (2) `open access', which allows readers to obtain it freely and free of charge. This paper studies the influence of both publication modalities on the average impact of the journal and its evolution over time. It tries to identify the separate effect of access on citation into two major parts: early view and selection effect, managing to provide some evidence of the positive effect of both. Scopus is used as the database and CiteScore as the measure of journal impact. The prevalence of both publication modalities is quantified. Differences in the average impact factor of group of journals, according to their publication modalities, are tested. The evolution over time of the citation influence, from 2011 to 2016, is also analysed. Finally, a linear regression to explain the correlation of these publication practices with the CiteScore in 2016, in a ceteris paribus context, is estimated. Our main findings show evidence of a positive correlation between average journal impact and advancing the publication of accepted articles, moreover this correlation increases over time. The open access modality, in a ceteris paribus context, also correlates positively with average journal impact.},
  file = {/home/gabriel/Dropbox/zotero-library/González-Betancor_Dorta-González_2019_Publication modalities ‘article in press’ and ‘open access’ in relation to.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {3}
}

@book{guedon2010,
  title = {In {{Oldenburg\'s}} Long Shadow: Librarians, Research Scientists, Publishers, and the Control of Scientific Publishing},
  shorttitle = {In {{Oldenburg\'s}} Long Shadow},
  author = {Gu{\'e}don, Jean-Claude},
  year = {2010},
  publisher = {{Association of Research Libraries}},
  address = {{Washington, D.C}},
  annotation = {OCLC: 255735567},
  file = {/home/gabriel/Dropbox/zotero-library/Guédon_2010_In Oldenburgś long shadow.pdf},
  isbn = {978-1-59407-829-3 978-0-918006-81-3},
  keywords = {todo},
  language = {en}
}

@article{haeffner-cavaillon2009,
  title = {The Use of Bibliometric Indicators to Help Peer-Review Assessment},
  author = {{Haeffner-Cavaillon}, Nicole and {Graillot-Gak}, Claude},
  year = {2009 Jan-Feb},
  volume = {57},
  pages = {33--38},
  issn = {1661-4917},
  doi = {10.1007/s00005-009-0004-2},
  abstract = {Inserm is the only French public research institution entirely dedicated to human health. Inserm supports research across the biomedical spectrum in all major disease areas, from fundamental lab-based science to clinical trials. To translate its scientists' findings into tangible health benefits, Inserm has its own affiliated company, Inserm Transfert, which works with industry. Since 2001, Inserm has been setting up on-line file management software for the evaluation of researchers and laboratories, called EVA ( www.eva.inserm.fr ). EVA includes all grant applications, assessment reports, evaluation grading evaluation forms and includes automated bibliometric indicator software that enables calculating, for example, the number of publications, journal impact factors, number of citations, citation index, and number of the Top 1 publications for each researcher of the teams. The indicators take into account research fields, the year of publications, and the author's position among the participants. Bibliometrics is now considered a tool for science policy providing indicators to measure productivity and scientific quality, thereby supplying a basis for evaluating and orienting R\&D. It is also a potential tool for evaluation. It is neutral, allows comparative (national and international) assessment, and may select papers in the forefront in all fields. For each team, bibliometric indicators are calculated for all researchers with permanent or long-term positions (3-5 years). The use of bibliometric indicators requires great vigilance, but according to our experience they enrich the committee's debates without any doubt. We present an analysis of the data of 600 research teams evaluated in 2007-2008.},
  file = {/home/gabriel/Dropbox/zotero-library/Archivum Immunologiae Et Therapiae Experimentalis/2009/Haeffner-Cavaillon_Graillot-Gak_2009_The use of bibliometric indicators to help peer-review assessment.pdf},
  journal = {Archivum Immunologiae Et Therapiae Experimentalis},
  keywords = {Academies and Institutes,Bibliometrics,Biomedical Research,Evaluation Studies as Topic,fav,Financing; Organized,France,Health Care Sector,Information Management,Journal Impact Factor,Laboratories,Peer Review; Research,Periodicals as Topic,Policy Making,Publishing,Quality Control,Software},
  language = {eng},
  number = {1},
  pmcid = {PMC3957005},
  pmid = {19219530}
}

@article{hajjem2006,
  title = {Ten-{{Year Cross}}-{{Disciplinary Comparison}} of the {{Growth}} of {{Open Access}} and {{How}} It {{Increases Research Citation Impact}}},
  author = {Hajjem, C. and Harnad, S. and Gingras, Y.},
  year = {2006},
  month = aug,
  abstract = {Lawrence (2001)found computer science articles that were openly accessible (OA) on the Web were cited more. We replicated this in physics. We tested 1,307,038 articles published across 12 years (1992-2003) in 10 disciplines (Biology, Psychology, Sociology, Health, Political Science, Economics, Education, Law, Business, Management). A robot trawls the Web for full-texts using reference metadata ISI citation data (signal detectability d'=2.45; bias = 0.52). Percentage OA (relative to total OA + NOA) articles varies from 5\%-16\% (depending on discipline, year and country) and is slowly climbing annually (correlation r=.76, sample size N=12, probability p {$<$} 0.005). Comparing OA and NOA articles in the same journal/year, OA articles have consistently more citations, the advantage varying from 36\%-172\% by discipline and year. Comparing articles within six citation ranges (0, 1, 2-3, 4-7, 8-15, 16+ citations), the annual percentage of OA articles is growing significantly faster than NOA within every citation range (r {$>$} .90, N=12, p {$<$} .0005) and the effect is greater with the more highly cited articles (r = .98, N=6, p {$<$} .005). Causality cannot be determined from these data, but our prior finding of a similar pattern in physics, where percent OA is much higher (and even approaches 100\% in some subfields), makes it unlikely that the OA citation advantage is merely or mostly a self-selection bias (for making only one's better articles OA). Further research will analyze the effect's timing, causal components and relation to other variables.},
  archiveprefix = {arXiv},
  eprint = {cs/0606079},
  eprinttype = {arxiv},
  file = {/home/gabriel/Dropbox/zotero-library/Hajjem et al_2006_Ten-Year Cross-Disciplinary Comparison of the Growth of Open Access and How it.pdf},
  journal = {arXiv:cs/0606079},
  keywords = {Computer Science - Digital Libraries}
}

@article{hammarfelt2018,
  title = {What Is a Discipline? {{The}} Conceptualization of Research Areas and Their Operationalization in Bibliometric Research},
  author = {Hammarfelt, Bj{\"o}rn},
  year = {2018},
  pages = {8},
  abstract = {This paper highlights disadvantages of conceptual impreciseness, and advocates further attention to the labels and concepts used when classifying clusters or groups based on bibliographic data. The main focus of the analysis is on the concept of `discipline' and how it is used in bibliometric research, but the implications concern a broader array of related terms.},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/2018/Hammarfelt_2018_What is a discipline.pdf},
  language = {en}
}

@book{harzing2010,
  title = {The {{Publish Or Perish Book}}: {{Your Guide}} to {{Effective}} and {{Responsible Citation Analysis}}},
  shorttitle = {The {{Publish Or Perish Book}}},
  author = {Harzing, Anne-Wil},
  year = {2010},
  publisher = {{Tarma Software Research Pty Limited}},
  abstract = {The first chapter provides a brief introduction to citation analysis as well as an overview of the most popular data sources and metrics in use. Part 1: How to use Publish or Perish more effectively The first part provides step-by-step instructions on how to use Publish or Perish more effectively. An introduction to the main features of the software is first provided in Chapter 2. Chapters 3 and 4 subsequently provide detailed instructions on how to conduct effect Author and Journal Queries. Chapter 5 is devoted to the broader applications of the General Citation search that can be used to find particular papers, conduct advanced author and journal queries, compare institutional performance and conduct a literature review. Finally, Chapter 6 discusses how the Multi-query Center can be used to effectively store and manage queries for future use. Part 2: Day-to-day uses of Publish or Perish citation analysis In Part 2, I present the most common day-to-day uses of the Publish or Perish software. Chapter 7 provides tips and tricks for academics that need to make their case for tenure or promotion. I discuss the importance of reference groups as well as several ways to show your citation record to its best advantage. In Chapter 8, I discuss how to evaluate other academics. The examples in this chapter vary from a 5-minute preparation before meeting someone you don't know, to evaluating editorial board members or prospective PhD supervisors, from writing up tributes (or laudations) and eulogies to deciding on publication awards and preparing for a job interview. Chapter 9 turns the tables and looks at citation analysis for Deans and other academic administrators. It includes four topics: the need to accept Google Scholar as an alternative data source, the myths about self-citation, the inappropriateness of citation analysis at early career stages, and the differences in citation impact across disciplines.},
  file = {/home/gabriel/Zotero/storage/GUFWMGZ9/The Publish or Perish Book Your guide to effectiv.pdf},
  isbn = {978-0-9808485-2-6},
  language = {en}
}

@incollection{haustein2015,
  title = {The {{Use}} of {{Bibliometrics}} for {{Assessing Research}}: {{Possibilities}}, {{Limitations}} and {{Adverse Effects}}},
  shorttitle = {The {{Use}} of {{Bibliometrics}} for {{Assessing Research}}},
  booktitle = {Incentives and {{Performance}}},
  author = {Haustein, Stefanie and Larivi{\`e}re, Vincent},
  editor = {Welpe, Isabell M. and Wollersheim, Jutta and Ringelhan, Stefanie and Osterloh, Margit},
  year = {2015},
  pages = {121--139},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09785-5_8},
  abstract = {Researchers are used to being evaluated: publications, hiring, tenure and funding decisions are all based on the evaluation of research. Traditionally, this evaluation relied on judgement of peers but, in the light of limited resources and increased bureaucratization of science, peer review is getting more and more replaced or complemented with bibliometric methods. Central to the introduction of bibliometrics in research evaluation was the creation of the Science Citation Index (SCI) in the 1960s, a citation database initially developed for the retrieval of scientific information. Embedded in this database was the Impact Factor, first used as a tool for the selection of journals to cover in the SCI, which then became a synonym for journal quality and academic prestige. Over the last 10 years, this indicator became powerful enough to influence researchers' publication patterns in so far as it became one of the most important criteria to select a publication venue. Regardless of its many flaws as a journal metric and its inadequacy as a predictor of citations on the paper level, it became the go-to indicator of research quality and was used and misused by authors, editors, publishers and research policy makers alike. The h-index, introduced as an indicator of both output and impact combined in one simple number, has experienced a similar fate, mainly due to simplicity and availability. Despite their massive use, these measures are too simple to capture the complexity and multiple dimensions of research output and impact. This chapter provides an overview of bibliometric methods, from the development of citation indexing as a tool for information retrieval to its application in research evaluation, and discusses their misuse and effects on researchers' scholarly communication behavior.},
  file = {/home/gabriel/Dropbox/zotero-library/Haustein_Larivière_2015_The Use of Bibliometrics for Assessing Research.pdf},
  isbn = {978-3-319-09784-8 978-3-319-09785-5},
  language = {en}
}

@article{hlavcheva2019,
  title = {A {{Survey}} of {{Informetric Methods}} and {{Technologies}}},
  author = {Hlavcheva, Yu. M. and Kanishcheva, O. V. and Borysova, N. V.},
  year = {2019},
  month = may,
  volume = {55},
  pages = {503--513},
  issn = {1573-8337},
  doi = {10.1007/s10559-019-00158-z},
  abstract = {A survey of informetric methods and technologies is presented. Problems and directions of informetrics are defined. The interrelations between the concepts such as scientometrics, bibliometrics, informetrics, webometrics, and altmetrics are shown. The existing informetric models and methods and also topical problems of informetrics are analyzed. Available analytic-and-search scientometric databases and systems are considered and their drawbacks and advantages are revealed. Characteristics of scientometric systems, their components, and factors that affect scientometric indices are determined. Based on the conducted research, promising directions for developing scientometric systems are formulated.},
  file = {/home/gabriel/Dropbox/zotero-library/Hlavcheva et al_2019_A Survey of Informetric Methods and Technologies.pdf},
  journal = {Cybernetics and Systems Analysis},
  keywords = {todo},
  language = {en},
  number = {3}
}

@article{jack2021,
  title = {Scientific Knowledge Production and Economic Catching-up: An Empirical Analysis},
  shorttitle = {Scientific Knowledge Production and Economic Catching-Up},
  author = {Jack, Pablo and Lachman, Jeremias and L{\'o}pez, Andr{\'e}s},
  year = {2021},
  month = apr,
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03973-4},
  abstract = {This paper aims to investigate the relationship between the production of scientific knowledge and level of income for a panel of 56 countries during the period 1996\textendash 2015. We argue that the accumulation of scientific knowledge is a key factor for the enhancement of educational and technological capabilities within an economy, and hence may have a positive impact on GDP per capita levels. We use academic publications in refereed journals (in all areas and specifically in engineering) as a proxy of scientific performance. As regards the impacts of scientific performance, we distinguish between high- and middle-income countries and, among the latter, between Asian and Latin America. The results show that academic publications are consistently and positively correlated with income per capita, for both middle and high-income countries. We also find non-linear effects in both groups. Those effects are lower for middle-income countries suggesting the presence of decreasing returns on academic performance. Finally, while Asian countries benefited from specialization in engineering research, no such effects were found for their Latin American peers.},
  file = {/home/gabriel/Dropbox/zotero-library/Jack et al_2021_Scientific knowledge production and economic catching-up.pdf},
  journal = {Scientometrics},
  keywords = {todo},
  language = {en}
}

@article{jappe2018,
  title = {Does Bibliometric Research Confer Legitimacy to Research Assessment Practice? {{A}} Sociological Study of Reputational Control, 1972-2016},
  shorttitle = {Does Bibliometric Research Confer Legitimacy to Research Assessment Practice?},
  author = {Jappe, Arlette and Pithan, David and Heinze, Thomas},
  editor = {Lozano, Sergi},
  year = {2018},
  month = jun,
  volume = {13},
  pages = {e0199031},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0199031},
  abstract = {The use of bibliometric measures in the evaluation of research has increased considerably based on expertise from the growing research field of evaluative citation analysis (ECA). However, mounting criticism of such metrics suggests that the professionalization of bibliometric expertise remains contested. This paper investigates why impact metrics, such as the journal impact factor and the h-index, proliferate even though their legitimacy as a means of professional research assessment is questioned. Our analysis is informed by two relevant sociological theories: Andrew Abbott's theory of professions and Richard Whitley's theory of scientific work. These complementary concepts are connected in order to demonstrate that ECA has failed so far to provide scientific authority for professional research assessment. This argument is based on an empirical investigation of the extent of reputational control in the relevant research area. Using three measures of reputational control that are computed from longitudinal inter-organizational networks in ECA (1972\textendash 2016), we show that peripheral and isolated actors contribute the same number of novel bibliometric indicators as central actors. In addition, the share of newcomers to the academic sector has remained high. These findings demonstrate that recent methodological debates in ECA have not been accompanied by the formation of an intellectual field in the sociological sense of a reputational organization. Therefore, we conclude that a growing gap exists between an academic sector with little capacity for collective action and increasing demand for routine performance assessment by research organizations and funding agencies. This gap has been filled by database providers. By selecting and distributing research metrics, these commercial providers have gained a powerful role in defining de-facto standards of research excellence without being challenged by expert authority.},
  file = {/home/gabriel/Dropbox/zotero-library/PLOS ONE/2018/Jappe et al_2018_Does bibliometric research confer legitimacy to research assessment practice.pdf},
  journal = {PLOS ONE},
  keywords = {fav},
  language = {en},
  number = {6}
}

@article{jarneving2005,
  title = {A Comparison of Two Bibliometric Methods for Mapping of the Research Front},
  author = {Jarneving, Bo},
  year = {2005},
  month = nov,
  volume = {65},
  pages = {245--263},
  issn = {1588-2861},
  doi = {10.1007/s11192-005-0270-7},
  abstract = {This paper builds on previous research concerned with the classification and specialty mapping of research fields. Two methods are put to test in order to decide if significant differences as to mapping results of the research front of a science field occur when compared. The first method was based on document co-citation analysis where papers citing co-citation clusters were assumed to reflect the research front. The second method was bibliographic coupling where likewise citing papers were assumed to reflect the research front. The application of these methods resulted in two different types of aggregations of papers: (1) groups of papers citing clusters of co-cited works and (2) clusters of bibliographically coupled papers. The comparision of the two methods as to mapping results was pursued by matching word profiles of groups of papers citing a particular co-citation cluster with word profiles of clusters of bibliographically coupled papers. Findings suggested that the research front was portrayed in two considerably different ways by the methods applied. It was concluded that the results in this study would support a further comparative study of these methods on a more detailed and qualitative ground. The original data set encompassed 73,379 articles from the fifty most cited environmental science journals listed in Journal Citation Report, science edition downloaded from the Science Citation Index on CD-ROM.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2005/Jarneving_2005_A comparison of two bibliometric methods for mapping of the research front.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {2}
}

@article{joshi2014,
  title = {Bibliometric Indicators for Evaluating the Quality of Scientifc Publications},
  author = {Joshi, Medha A.},
  year = {2014},
  month = mar,
  volume = {15},
  pages = {258--262},
  issn = {1526-3711},
  doi = {10.5005/jp-journals-10024-1525},
  abstract = {Evaluation of quality and quantity of publications can be done using a set of statistical and mathematical indices called bibliometric indicators. Two major categories of indicators are (1) quantitative indicators that measure the research productivity of a researcher and (2) performance indicators that evaluate the quality of publications. Bibliometric indicators are important for both the individual researcher and organizations. They are widely used to compare the performance of the individual researchers, journals and universities. Many of the appointments, promotions and allocation of research funds are based on these indicators. This review article describes some of the currently used bibliometric indicators such as journal impact factor, crown indicator, h-index and it's variants. It is suggested that for comparison of scientific impact and scientific output of researchers due consideration should be given to various factors affecting theses indicators.},
  file = {/home/gabriel/Dropbox/zotero-library/The Journal of Contemporary Dental Practice/2014/Joshi_2014_Bibliometric indicators for evaluating the quality of scientifc publications.pdf},
  journal = {The Journal of Contemporary Dental Practice},
  keywords = {Authorship,Bibliometrics,Dental Research,fav,Humans,Journal Impact Factor,Periodicals as Topic,Publishing},
  language = {eng},
  number = {2},
  pmid = {25095854}
}

@article{kamrani,
  title = {Do Researchers Know What the H-Index Is? {{And}} How Do They Estimate Its Importance?},
  author = {Kamrani, Pantea},
  pages = {20},
  abstract = {The h-index is a widely used scientometric indicator on the researcher level working with a simple combination of publication and citation counts. In this article, we pursue two goals, namely the collection of empirical data about researchers' personal estimations of the importance of the h-index for themselves as well as for their academic disciplines, and on the researchers' concrete knowledge on the h-index and the way of its calculation. We worked with an online survey (including a knowledge test on the calculation of the h-index), which was finished by 1081 German university professors. We distinguished between the results for all participants, and, additionally, the results by gender, generation, and field of knowledge. We found a clear binary division between the academic knowledge fields: For the sciences and medicine the h-index is important for the researchers themselves and for their disciplines, while for the humanities and social sciences, economics, and law the h-index is considerably less important. Two fifths of the professors do not know details on the h-index or wrongly deem to know what the h-index is and failed our test. The researchers' knowledge on the h-index is much smaller in the academic branches of the humanities and the social sciences. As the h-index is important for many researchers and as not all researchers are very knowledgeable about this author-specific indicator, it seems to be necessary to make researchers more aware of scholarly metrics literacy.},
  file = {/home/gabriel/Dropbox/zotero-library/Kamrani_Do researchers know what the h-index is.pdf},
  language = {en}
}

@article{karanatsiou,
  title = {Bibliometrics and {{Altmetrics}} Literature Review: {{Performance}} Indicators and Comparison Analysis},
  author = {Karanatsiou, Dimitra and Misirlis, Nikolaos and Vlachopoulou, Maro},
  pages = {21},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/undefined/Karanatsiou et al_Bibliometrics and Altmetrics literature review.pdf},
  language = {en}
}

@article{kokol,
  title = {Application of Bibliometrics in Medicine: A Historical Bibliometrics Analysis},
  shorttitle = {Application of Bibliometrics in Medicine},
  author = {Kokol, Peter and Vo{\v s}ner, Helena Bla{\v z}un and Zavr{\v s}nik, Jernej},
  volume = {n/a},
  issn = {1471-1842},
  doi = {10.1111/hir.12295},
  abstract = {Background The application of bibliometrics in medicine enables one to analyse vast amounts of publications and their production patterns on macroscopic and microscopic levels. Objectives The aim of the study was to analyse the historical perspective of research literature production regarding application of bibliometrics in medicine. Methods Publications related to application of bibliometrics in medicine from 1970 to 2018 were harvested from the Scopus bibliographic database. Reference Publication Year Spectroscopy was triangulated with the VOSViewer to identify historical roots and evolution of topics and clinical areas. Results The search resulted in 6557 publications. The literature production trend was positive. Historical roots analysis identified 33 historical roots and 16 clinical areas where bibliometrics was applied. Discussion The increase in productivity in application of bibliometrics in medicine might be attributed to increased use of quantitative metrics in research evaluation, publish or perish phenomenon and the increased use of evidence-based medicine. Conclusion The trend of the literature production was positive. Medicine was in the forefront of knowledge development in bibliometrics. reference publication year spectroscopy proved to be an accurate method which was able to identify most of the historical roots.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/hir.12295},
  copyright = {\textcopyright{} 2020 Health Libraries Group},
  file = {/home/gabriel/Dropbox/zotero-library/Kokol et al_Application of bibliometrics in medicine.pdf},
  journal = {Health Information \& Libraries Journal},
  keywords = {bibliometrics,biomedical,citation analysis,evidence-based medicine (EBM),research},
  language = {en},
  number = {n/a}
}

@article{kostenko2015,
  title = {Scientometrics: {{A Tool}} for {{Monitoring}} and {{Support}} of {{Research}}},
  author = {Kostenko, L and Zhabin, A and Kuznetsov, A and Lukashevich, T and Kukharchuk, E and Simonenko, T},
  year = {2015},
  pages = {7},
  abstract = {The origins of scientometrics (research metrics) are discussed. The approaches to research evaluation are reviewed, and the tendency to replacing formal quantitative indicators by expert review based on bibliometric indicators is emphasized. The principles of ``Leiden Manifesto of Scientometrics'' are set out, providing for transparent monitoring and support of research and encouraging constructive dialog between the scientific community and the public. The methodological framework and the peculiarities of implementation of the information and analytical system ``Bibliometryka Ukrayinskoyi Nauky'' (``Bibliometrics of the Ukrainian Science''), constructed by the Vernadsky National Library of Ukraine, are shown. The proposals on creating advisory councils, responsible for formulating conclusions on the research effectiveness of institutions, are given. The feasibility of building a common platform for expert evaluation of research for the Eastern Partnership Countries by launching similar bibliometric projects in these countries and their further convergence is considered.},
  file = {/home/gabriel/Dropbox/zotero-library/Kostenko et al_2015_Scientometrics.pdf},
  keywords = {todo},
  language = {en}
}

@article{kumar2009,
  title = {Impact of the Impact Factor in Biomedical Research: Its Use and Misuse},
  shorttitle = {Impact of the Impact Factor in Biomedical Research},
  author = {Kumar, V. and Upadhyay, S. and Medhi, B.},
  year = {2009},
  month = aug,
  volume = {50},
  pages = {752--755},
  issn = {0037-5675},
  abstract = {The impact factor was created in the biomedical research field in order to measure a journal's value by calculating the average number of citations per article over a period of time. It was initially developed to help libraries decide which highly-cited journals to subscribe to. However, at present, it is being misused to judge the quality of a researcher or medical scientist as well as the quality of the work done. It contains serious sources of errors and flaws, resulting in strong biases against culture- and language-bound medical subspecialties. The present article is aimed to highlight the impact of the impact factor in the biomedical research, as well as its use and misuse.},
  file = {/home/gabriel/Dropbox/zotero-library/Singapore Medical Journal/2009/Kumar et al_2009_Impact of the impact factor in biomedical research.pdf},
  journal = {Singapore Medical Journal},
  keywords = {Bibliometrics,Biomedical Research,Databases; Bibliographic,fav,Journal Impact Factor,Periodicals as Topic,Publishing,Reproducibility of Results,Time Factors},
  language = {eng},
  number = {8},
  pmid = {19710969}
}

@article{kumar2015,
  title = {Popular {{Scientometric Analysis}}, {{Mapping}} and {{Visualisation Softwares}}: {{An Overview}}},
  author = {Kumar, Ashok and Shivarama, J and Choukimath, Puttaraj A},
  year = {2015},
  pages = {14},
  abstract = {Measurement of scientific productivity has been regarded as main indicator of ascertaining impact of research over scientific community. To showcase the impact of research using science mapping and visualisation, social network analysis has been developed over the period of time. These methods help researchers to understand the structural, temporal and dynamic development of a discipline. The present paper provides a comprehensive overview of widely used softwares used for scientometric analysis, mapping and visualisation.},
  file = {/home/gabriel/Dropbox/zotero-library/Kumar et al_2015_Popular Scientometric Analysis, Mapping and Visualisation Softwares.pdf},
  keywords = {todo},
  language = {en}
}

@article{leite2011,
  title = {A New Indicator for International Visibility: Exploring {{Brazilian}} Scientific Community},
  shorttitle = {A New Indicator for International Visibility},
  author = {Leite, Paula and Mugnaini, Rog{\'e}rio and Leta, Jacqueline},
  year = {2011},
  month = apr,
  volume = {88},
  pages = {311--319},
  publisher = {{Akad\'emiai Kiad\'o, co-published with Springer Science+Business Media B.V., Formerly Kluwer Academic Publishers B.V.}},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-011-0379-9},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d49e2"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Brazilian science has increased fast during the last decades. An example is the increasing in the country's share in the world's scientific publication within the main international databases. But what is the actual weight of international publications to the whole Brazilian productivity? In order to respond this question, we have elaborated a new indicator, the International Publication Ratio (IPR). The data source was Lattes Database, a database organized by one of the main Brazilian S\&amp;T funding agency, which encompasses publication data from 1997 to 2004 of about 51,000 Brazilian researchers. Influences of distinct parameters, such as sectors, fields, career age and gender, are analyzed. We hope the data presented may help S\&amp;T managers and other S\&amp;T interests to better understand the complexity under the concept scientific productivity, especially in peripheral countries in science, such as Brazil.{$<$}/p{$><$}/section{$>$}},
  chapter = {Scientometrics},
  journal = {Scientometrics},
  language = {en\_US},
  number = {1}
}

@article{liao2018,
  title = {A {{Bibliometric Analysis}} and {{Visualization}} of {{Medical Big Data Research}}},
  author = {Liao, Huchang and Tang, Ming and Luo, Li and Li, Chunyang and Chiclana, Francisco and Zeng, Xiao-Jun},
  year = {2018},
  month = jan,
  volume = {10},
  pages = {166},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/su10010166},
  abstract = {With the rapid development of ``Internet plus'', medical care has entered the era of big data. However, there is little research on medical big data (MBD) from the perspectives of bibliometrics and visualization. The substantive research on the basic aspects of MBD itself is also rare. This study aims to explore the current status of medical big data through visualization analysis on the journal papers related to MBD. We analyze a total of 988 references which were downloaded from the Science Citation Index Expanded and the Social Science Citation Index databases from Web of Science and the time span was defined as ``all years''. The GraphPad Prism 5, VOSviewer and CiteSpace softwares are used for analysis. Many results concerning the annual trends, the top players in terms of journal and institute levels, the citations and H-index in terms of country level, the keywords distribution, the highly cited papers, the co-authorship status and the most influential journals and authors are presented in this paper. This study points out the development status and trends on MBD. It can help people in the medical profession to get comprehensive understanding on the state of the art of MBD. It also has reference values for the research and application of the MBD visualization methods.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/home/gabriel/Dropbox/zotero-library/Liao et al_2018_A Bibliometric Analysis and Visualization of Medical Big Data Research.pdf},
  journal = {Sustainability},
  keywords = {bibliometric analysis,co-authorship analysis,co-citation analysis,medical big data,visualization},
  language = {en},
  number = {1}
}

@article{lucas-dominguez2021,
  title = {The Sharing of Research Data Facing the {{COVID}}-19 Pandemic},
  author = {{Lucas-Dominguez}, Rut and {Alonso-Arroyo}, Adolfo and {Vidal-Infer}, Antonio and {Aleixandre-Benavent}, Rafael},
  year = {2021},
  month = apr,
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03971-6},
  abstract = {During the previous Ebola and Zika outbreaks, researchers shared their data, allowing many published epidemiological studies to be produced only from open research data, to speed up investigations and control of these infections. This study aims to evaluate the dissemination of the COVID-19 research data underlying scientific publications. Analysis of COVID-19 publications from December 1, 2019, to April 30, 2020, was conducted through the PubMed Central repository to evaluate the research data available through its publication as supplementary material or deposited in repositories. The PubMed Central search generated 5,905 records, of which 804 papers included complementary research data, especially as supplementary material (77.4\%). The most productive journals were The New England Journal of Medicine, The Lancet and The Lancet Infectious Diseases, the most frequent keyword was pneumonia, and the most used repositories were GitHub and GenBank. An expected growth in the number of published articles following the course of the pandemics is confirmed in this work, while the underlying research data are only 13.6\%. It can be deduced that data sharing is not a common practice, even in health emergencies, such as the present one. High-impact generalist journals have accounted for a large share of global publishing. The topics most often covered are related to epidemiological and public health concepts, genetics, virology and respiratory diseases, such as pneumonia. However, it is essential to interpret these data with caution following the evolution of publications and their funding in the coming months.},
  file = {/home/gabriel/Dropbox/zotero-library/Lucas-Dominguez et al_2021_The sharing of research data facing the COVID-19 pandemic.pdf},
  journal = {Scientometrics},
  language = {en}
}

@article{lyu2021,
  title = {The Classification of Citing Motivations: A Meta-Synthesis},
  shorttitle = {The Classification of Citing Motivations},
  author = {Lyu, Dongqing and Ruan, Xuanmin and Xie, Juan and Cheng, Ying},
  year = {2021},
  month = apr,
  volume = {126},
  pages = {3243--3264},
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03908-z},
  abstract = {Citation analysis has been a prevalent method in the field of information science, especially research on bibliometrics and evaluation, but its validity relies heavily on how the citations are treated. It is essential to study authors' citing motivations to identify citations with different values and significance. This study applied a meta-synthesis approach to establish a new holistic classification of citation motivations based on previous studies. First, we used a four-step search strategy to identify related articles on authors' citing motivations. Thirty-eight primary studies were included after the inclusion and exclusion criteria were applied and appraised using the Evidence-based Librarianship checklist. Next, we decoded and recoded the citing motivations found in the included studies, following the standard procedures of meta-synthesis. Thirty-five descriptive concepts of citation motivations emerged, which were then synthesized into 13 analytic themes. As a result, we proposed a comprehensive classification, including two main categories of citing reasons, i.e., ``scientific motivations'' and ``tactical motivations.'' Generally, the citations driven by scientific motivations serve as a rhetorical function, while tactical motivations are social or benefit-oriented and not easily captured through text-parsing. Our synthesis contributes to bibliometric and scientific evaluation theory. The synthesized classification also provides a comprehensive and unified annotation schema for citation classification and helps identify the useful mentions of a reference in a citing paper to optimize citation- based measurements.},
  file = {/home/gabriel/Dropbox/zotero-library/Lyu et al_2021_The classification of citing motivations.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {4}
}

@article{merigo2017,
  title = {A Bibliometric Analysis of Operations Research and Management Science},
  author = {Merig{\'o}, Jos{\'e} M. and Yang, Jian-Bo},
  year = {2017},
  month = dec,
  volume = {73},
  pages = {37--48},
  issn = {03050483},
  doi = {10.1016/j.omega.2016.12.004},
  abstract = {Bibliometric analysis is the quantitative study of bibliographic material. It provides a general picture of a research field that can be classified by papers, authors and journals. This paper presents a bibliometric overview of research published in operations research and management science in recent decades. The main objective of this study is to identify some of the most relevant research in this field and some of the newest trends according to the information found in the Web of Science database. Several classifications are made, including an analysis of the most influential journals, the two hundred most cited papers of all time and the most productive and influential authors. The results obtained are in accordance with the common wisdom, although some variations are found.},
  file = {/home/gabriel/Dropbox/zotero-library/Omega/2017/Merigó_Yang_2017_A bibliometric analysis of operations research and management science.pdf},
  journal = {Omega},
  language = {en}
}

@article{mingers2015,
  title = {A Review of Theory and Practice in Scientometrics},
  author = {Mingers, John and Leydesdorff, Loet},
  year = {2015},
  month = oct,
  volume = {246},
  pages = {1--19},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2015.04.002},
  abstract = {Scientometrics is the study of the quantitative aspects of the process of science as a communication system. It is centrally, but not only, concerned with the analysis of citations in the academic literature. In recent years it has come to play a major role in the measurement and evaluation of research performance. In this review we consider: the historical development of scientometrics, sources of citation data, citation metrics and the ``laws'' of scientometrics, normalisation, journal impact factors and other journal metrics, visualising and mapping science, evaluation and policy, and future developments.},
  file = {/home/gabriel/Dropbox/zotero-library/Mingers_Leydesdorff_2015_A review of theory and practice in scientometrics.pdf},
  journal = {European Journal of Operational Research},
  keywords = {Altmetrics,Citations,H-index,Impact factor,Normalisation,todo},
  language = {en},
  number = {1}
}

@article{moed2002,
  title = {The Impact-Factors Debate: The {{ISI}}'s Uses and Limits},
  shorttitle = {The Impact-Factors Debate},
  author = {Moed, Henk F.},
  year = {2002},
  month = feb,
  volume = {415},
  pages = {731--732},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/415731a},
  file = {/home/gabriel/Dropbox/zotero-library/Moed_2002_The impact-factors debate.pdf},
  journal = {Nature},
  keywords = {todo},
  language = {en},
  number = {6873}
}

@book{moed2006,
  title = {Citation {{Analysis}} in {{Research Evaluation}}},
  author = {Moed, Henk F.},
  year = {2006},
  month = mar,
  publisher = {{Springer Science \& Business Media}},
  abstract = {This book is written for members of the scholarly research community, and for persons involved in research evaluation and research policy. More specifically, it is directed towards the following four main groups of readers: \textendash{} All scientists and scholars who have been or will be subjected to a quantitative assessment of research performance using citation analysis. \textendash{} Research policy makers and managers who wish to become conversant with the basic features of citation analysis, and about its potentialities and limitations. \textendash{} Members of peer review committees and other evaluators, who consider the use of citation analysis as a tool in their assessments. \textendash{} Practitioners and students in the field of quantitative science and technology studies, informetrics, and library and information science. Citation analysis involves the construction and application of a series of indicators of the `impact', `influence' or `quality' of scholarly work, derived from citation data, i.e. data on references cited in footnotes or bibliographies of scholarly research publications. Such indicators are applied both in the study of scholarly communication and in the assessment of research performance. The term `scholarly' comprises all domains of science and scholarship, including not only those fields that are normally denoted as science \textendash{} the natural and life sciences, mathematical and technical sciences \textendash{} but also social sciences and humanities.},
  file = {/home/gabriel/Dropbox/zotero-library/Moed_2006_Citation Analysis in Research Evaluation.pdf},
  googlebooks = {D9SaJ6awy4gC},
  isbn = {978-1-4020-3714-6},
  keywords = {Business \& Economics / Economics / General,Business \& Economics / General,Computers / Computer Science,Language Arts \& Disciplines / Library \& Information Science / General,Philosophy / General,Science / General,Social Science / General,Social Science / Research,Social Science / Statistics},
  language = {en}
}

@book{moed2017,
  title = {Applied {{Evaluative Informetrics}}},
  author = {Moed, Henk F.},
  year = {2017},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-60522-7},
  file = {/home/gabriel/Dropbox/zotero-library/Moed_2017_Applied Evaluative Informetrics.pdf},
  isbn = {978-3-319-60521-0 978-3-319-60522-7},
  series = {Qualitative and {{Quantitative Analysis}} of {{Scientific}} and {{Scholarly Communication}}}
}

@article{momeni2021,
  title = {What Happens When a Journal Converts to Open Access? {{A}} Bibliometric Analysis},
  shorttitle = {What Happens When a Journal Converts to Open Access?},
  author = {Momeni, Fakhri and Mayr, Philipp and Fraser, Nicholas and Peters, Isabella},
  year = {2021},
  month = apr,
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03972-5},
  abstract = {In recent years, increased stakeholder pressure to transition research to Open Access has led to many journals converting, or `flipping', from a closed access (CA) to an open access (OA) publishing model. Changing the publishing model can influence the decision of authors to submit their papers to a journal, and increased article accessibility may influence citation behaviour. In this paper we aimed to understand how flipping a journal to an OA model influences the journal's future publication volumes and citation impact. We analysed two independent sets of journals that had flipped to an OA model, one from the Directory of Open Access Journals (DOAJ) and one from the Open Access Directory (OAD), and compared their development with two respective control groups of similar journals. For bibliometric analyses, journals were matched to the Scopus database. We assessed changes in the number of articles published over time, as well as two citation metrics at the journal and article level: the normalised impact factor (IF) and the average relative citations (ARC), respectively. Our results show that overall, journals that flipped to an OA model increased their publication output compared to journals that remained closed. Mean normalised IF and ARC also generally increased following the flip to an OA model, at a greater rate than was observed in the control groups. However, the changes appear to vary largely by scientific discipline. Overall, these results indicate that flipping to an OA publishing model can bring positive changes to a journal.},
  file = {/home/gabriel/Dropbox/zotero-library/Momeni et al_2021_What happens when a journal converts to open access.pdf},
  journal = {Scientometrics},
  language = {en}
}

@article{moura2017,
  title = {Uses of {{Bibliometric Techniques}} in {{Public Health Research}}},
  author = {MOURA, Luana Kelle Batista and {de MESQUITA}, Rafael Fernandes and MOBIN, Mitra and MATOS, Francisca Tereza Coelho and MONTE, Thiago Lima and LAGO, Eliana Campelo and FALC{\~A}O, Carlos Alberto Monteiro and {de Ar{\^e}a Le{\~a}o FERRAZ}, Maria {\^A}ngela and SANTOS, Tanit Clementino and SOUSA, Laelson Rochelle Milan{\^e}s},
  year = {2017},
  month = oct,
  volume = {46},
  pages = {1435--1436},
  issn = {2251-6085},
  file = {/home/gabriel/Dropbox/zotero-library/Iranian Journal of Public Health/2017/MOURA et al_2017_Uses of Bibliometric Techniques in Public Health Research.pdf},
  journal = {Iranian Journal of Public Health},
  number = {10},
  pmcid = {PMC5750357},
  pmid = {29308389}
}

@article{mugnaini2004,
  title = {{Indicadores bibliom\'etricos da produ\c{c}\~ao cient\'ifica brasileira: uma an\'alise a partir da base Pascal}},
  shorttitle = {{Indicadores bibliom\'etricos da produ\c{c}\~ao cient\'ifica brasileira}},
  author = {Mugnaini, Rog{\'e}rio and Jannuzzi, Paulo de Martino and Quoniam, Luc},
  year = {2004},
  month = aug,
  volume = {33},
  pages = {123--131},
  publisher = {{Instituto Brasileiro de Informa\c{c}\~ao em{$<$}br{$>$}Ci\^encia e Tecnologia - IBICT}},
  issn = {0100-1965},
  doi = {10.1590/S0100-19652004000200013},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini et al_2004_Indicadores bibliométricos da produção científica brasileira.pdf},
  journal = {Ci\^encia da Informa\c{c}\~ao},
  language = {pt},
  number = {2}
}

@phdthesis{mugnaini2006,
  title = {{Caminhos para adequa\c{c}\~ao da avalia\c{c}\~ao da produ\c{c}\~ao cient\'ifica brasileira: impacto nacional versus internacional}},
  shorttitle = {{Caminhos para adequa\c{c}\~ao da avalia\c{c}\~ao da produ\c{c}\~ao cient\'ifica brasileira}},
  author = {Mugnaini, Rog{\'e}rio},
  year = {2006},
  month = nov,
  doi = {10.11606/T.27.2006.tde-11052007-091052},
  abstract = {Diversos indicadores bibliom\'etricos t\^em sido empregados na avalia\c{c}\~ao de desempenho de pesquisadores, universidades e pa\'ises. Indicadores de impacto, calculados a partir das cita\c{c}\~oes recebidas pelos artigos, t\^em sido objeto de muitos estudos constantes da pesquisa document\'aria. Dessa maneira, almeja-se apontar poss\'iveis formas de adequa\c{c}\~ao da an\'alise do impacto de revistas brasileiras com vistas ao aprimoramento dos crit\'erios de avalia\c{c}\~ao de produ\c{c}\~ao cient\'ifica no Brasil. Objetivos. A pesquisa foi conduzida de acordo com tr\^es objetivos: (1) Verificar se o uso exclusivo de indicadores internacionais deixa a pol\'itica cient\'ifica brasileira fora do contexto de sua realidade local, e se o acesso \`as revistas indexadas nas bases do ISI (Thomson Scientific) tem se justificado pelo uso ? o acesso gratuito aos textos completos \'e oferecido \`a comunidade cient\'ifica pela Capes (Coordena\c{c}\~ao de Aperfei\c{c}oamento de Pessoal de N\'ivel Superior). (2) Investigar se a base SciELO pode oferecer indicadores de impacto da produ\c{c}\~ao cient\'ifica brasileira com vistas ao aprimoramento da avalia\c{c}\~ao cient\'ifica nacional. (3) Buscar propor metodologias de indicadores mais adequadas \`a realidade da ci\^encia brasileira. Metodologia. Foi conduzido um estudo explorat\'orio quantitativo, baseados em caracter\'isticas qualitativas e quantitativas de revistas cient\'ificas provenientes de tr\^es fontes: revistas classificadas pela Avalia\c{c}\~ao Qualis (tri\^enio 2001/2003), revistas do Portal de Peri\'odicos da Capes e revistas indexadas na base SciELO (Scientific Electronic Library). Uma compara\c{c}\~ao do impacto nacional e internacional de um conjunto de revistas brasileiras indexadas na base SciELO foi realizada a partir das cita\c{c}\~oes recebidas pelas revistas em cada contexto (base SciELO e as bases do ISI). Uma metodologia de an\'alise de revistas foi apresentada aplicando-se t\'ecnicas de an\'alise estat\'istica multivariada a um conjunto de 42 indicadores. Resultados. A an\'alise da Avalia\c{c}\~ao Qualis mostrou que os crit\'erios definidos favorecem principalmente a publica\c{c}\~ao em revistas internacionais e fazem uso do Fator de Impacto do ISI. O Impacto P\'os-Portal, como foi denominado, indicou um efeito positivo, notado pelo aumento da m\'edia de cita\c{c}\~oes recebidas na SciELO, por aproximadamente 70\% das revistas da amostra (Ci\^encias da Vida), ap\'os o ano de publica\c{c}\~ao no Portal. A compara\c{c}\~ao do impacto nacional versus internacional das revistas SciELO mostrou que revistas indexadas tamb\'em no ISI s\~ao citadas com mais freq\"u\^encia naquela base, al\'em de receberem aproximadamente 72\% das cita\c{c}\~oes de revistas ISI de autores estrangeiros e terem os artigos em colabora\c{c}\~ao (nacional e internacional) mais citados que aqueles em autoria \'unica. Em rela\c{c}\~ao \`as revistas publicadas somente na SciELO, verificou-se que s\~ao citadas em quantidades similares naquela base e nas bases do ISI, recebem 68\% das cita\c{c}\~oes de revistas ISI de autores estrangeiros e t\^em seus artigos de autoria \'unica mais citados, seguidos daqueles em colabora\c{c}\~ao nacional. A an\'alise multivariada dos indicadores das revistas SciELO permitiu a identifica\c{c}\~ao de diferentes grupos de revistas, discriminados de acordo com as diferentes pr\'aticas de comunica\c{c}\~ao cient\'ifica. Conclus\~oes. A adequa\c{c}\~ao dos crit\'erios utilizados na avalia\c{c}\~ao da produ\c{c}\~ao cient\'ifica nacional pode ser conseguida considerando-se indicadores de impacto mensurados a partir de cita\c{c}\~oes provenientes das revistas nacionais, definindo crit\'erios que valorizem a publica\c{c}\~ao em revistas nacionais de qualidade reconhecida, o que permitir\'a a publica\c{c}\~ao de trabalhos importantes na l\'ingua portuguesa, e estimular\'a o processo de melhoria de qualidade das revistas nacionais.},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini_2006_Caminhos para adequação da avaliação da produção científica brasileira.pdf},
  language = {pt-br},
  school = {Universidade de S\~ao Paulo},
  type = {{text}}
}

@article{mugnaini2010,
  title = {Multidisciplinaridade e Especificidade Na Comunica\c{c}\~ao Cient\'ifica: Discuss\~ao Do Impacto Na Avalia\c{c}\~ao de Diferentes \'Areas},
  shorttitle = {Multidisciplinaridade e Especificidade Na Comunica\c{c}\~ao Cient\'ifica},
  author = {Mugnaini, Rog{\'e}rio and Poblaci{\'o}n, Dinah Apparecida de Melo Aguiar},
  year = {2010},
  volume = {4},
  issn = {1981-6278},
  doi = {10.3395/reciis.v4i5.533},
  abstract = {As refer\^encias bibliogr\'aficas podem revelar o perfil da ci\^encia publicada, oferecendo importantes informa\c{c}\~oes sobre a hist\'oria de uma revista. Ao identificar o impacto dos diferentes tipos de documentos citados por cinco revistas cient\'ificas de \'areas diversas, constatou-se que o livro \'e consideravelmente mais citado numa revista de Ci\^encias Sociais Aplicadas, enquanto a \'area de Sa\'ude Coletiva faz uso deste tipo de documento em propor\c{c}\~oes equipar\'aveis com os artigos cient\'ificos. Nas revistas de F\'isica e Medicina as cita\c{c}\~oes a revistas internacionais s\~ao muito mais prevalentes. E na revista de Veterin\'aria e de Ci\^encia da Informa\c{c}\~ao, destacam-se os anais e teses. Estas constata\c{c}\~oes s\~ao importantes para entender as culturas de comunica\c{c}\~ao cient\'ifica das \'areas, o que p\^ode ser observado tamb\'em ao analisar, tanto as classifica\c{c}\~oes das revistas no Qualis, quanto os crit\'erios constantes dos documentos de \'area. Indicadores bibliom\'etricos n\~ao restritos a um \'indice s\~ao capazes de oferecer par\^ametros para cooperar na defini\c{c}\~ao de crit\'erios para avalia\c{c}\~ao da produ\c{c}\~ao cient\'ifica brasileira, segundo as caracter\'isticas das diferentes \'areas do conhecimento.},
  copyright = {Direitos autorais},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini_Población_2010_Multidisciplinaridade e especificidade na comunicação científica.pdf},
  journal = {Revista Eletr\^onica de Comunica\c{c}\~ao, Informa\c{c}\~ao e Inova\c{c}\~ao em Sa\'ude},
  keywords = {Assessment,Avaliação,Bases de dados,Bibliometric indicators,Citação.,Citation,Comunicação científica,Database,Indicadores bibliométricos,Scientific publication},
  language = {en},
  number = {5}
}

@incollection{mugnaini2013,
  title = {40 Anos de {{Bibliometria}} No {{Brasil}}: Da Bibliografia Estat\'istica \`a Avalia\c{c}\~ao Da Produ\c{c}\~ao Cient\'ifica Nacional},
  shorttitle = {40 Anos de {{Bibliometria}} No {{Brasil}}},
  author = {Mugnaini, Rogerio},
  year = {2013},
  month = jan,
  pages = {37--58},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini_2013_40 anos de Bibliometria no Brasil.pdf},
  isbn = {978-85-7993-117-8},
  keywords = {done}
}

@article{mugnaini2014,
  title = {{Comunica\c{c}\~ao cient\'ifica no Brasil (1998-2012): indexa\c{c}\~ao, crescimento, fluxo e dispers\~ao}},
  shorttitle = {{Comunica\c{c}\~ao cient\'ifica no Brasil (1998-2012)}},
  author = {Mugnaini, Rog{\'e}rio and Digiampetri, Luciano Antonio and {Mena-Chalco}, Jes{\'u}s Pascual and Mugnaini, Rog{\'e}rio and Digiampetri, Luciano Antonio and {Mena-Chalco}, Jes{\'u}s Pascual},
  year = {2014},
  month = dec,
  volume = {26},
  pages = {239--252},
  publisher = {{Pontif\'icia Universidade Cat\'olica de Campinas}},
  issn = {0103-3786},
  doi = {10.1590/0103-3786201400030002},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini et al_2014_Comunicação científica no Brasil (1998-2012).pdf},
  journal = {Transinforma\c{c}\~ao},
  language = {pt},
  number = {3}
}

@article{mugnaini2017,
  title = {{Ciclo avaliativo de peri\'odicos no Brasil: caminho virtuoso ou colcha de retalhos?}},
  shorttitle = {{Ciclo avaliativo de peri\'odicos no Brasil}},
  author = {Mugnaini, Rogerio},
  year = {2017},
  month = mar,
  abstract = {This study investigated factors influencing the scholarly communication process, from the analysis of the quality criteria proposed between thematic areas in the exercise of evaluation ofscientific production in Brazil. Therefore, provided an overview based on the types of assessment criteria adopted in the Qualis-Peri\'odicos for the definition of strata A1, A2 and B1, from the documents of 2010-2012 period. It found that most hard sciences areas presents evaluation profile strictly based on bibliometric indicators, mainly JCR impact factor and in some cases, the h-index. Humanities, Social and Language, Literature and Arts areas, serve themselves believing at the selection process carried out by the databases, especially the citation indexes (Web of Science, Scopus and SciELO), and also in other databases as RedALyC and Latindex when defining criteria of journal indexing. Finally, periodic characteristics are primarily institutional diversity of authors and authors from foreign institutions, followed by journal periodicity and diversity of the editorial board. Thus configures itself an evaluation cycle, whose result appears to contribute to the improvement of the journals. On the other hand, although mounted, the process needs review, so that one can assess their effects on the scientific communication process.},
  annotation = {Accepted: 2017-03-06T20:28:32Z},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini_2017_Ciclo avaliativo de periódicos no Brasil.pdf},
  language = {pt\_BR}
}

@book{mugnaini2017a,
  title = {Bibliometria e Cientometria No {{Brasil}}: Infraestrutura Para Avalia\c{c}\~ao Da Pesquisa Cient\'ifica Na Era Do {{Big Data}} / {{Bibliometrics}} and Scientometrics in {{Brazil}}: Scientific Research Assessment Infrastructure in the Era of {{Big Data}}},
  shorttitle = {Bibliometria e Cientometria No {{Brasil}}},
  author = {Mugnaini, Rogerio and Fujino, Asa and Kobashi, Nair},
  year = {2017},
  month = mar,
  doi = {10.11606/9788572051705},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini et al_2017_Bibliometria e cientometria no Brasil.pdf},
  isbn = {978-85-7205-170-5},
  keywords = {fav,top5}
}

@article{mugnaini2021,
  title = {Openness Trends in {{Brazilian}} Citation Data: Factors Related to the Use of {{DOIs}}},
  shorttitle = {Openness Trends in {{Brazilian}} Citation Data},
  author = {Mugnaini, Rog{\'e}rio and Fraumann, Grischa and Tuesta, Esteban F. and Packer, Abel L.},
  year = {2021},
  month = mar,
  volume = {126},
  pages = {2523--2556},
  issn = {1588-2861},
  doi = {10.1007/s11192-020-03663-7},
  abstract = {Digital object identifiers (DOIs) are important metadata elements for indexing and interoperability, as well as for bibliometric studies in times of openness. This study analyses the use of DOIs in the cited references of articles by authors from Brazilian institutions, their possible influencing factors and differences among areas of knowledge. It measures the extent to which the citation datasets are open for reuse by others in terms of the availability of DOIs. 226,491 articles were retrieved from Web of Science (2012\textendash 2016), making a total of 8,707,120 cited references, 68\% of which include DOIs. The results showed that the hard sciences have higher percentages of DOIs in their cited references. The factor type of collaboration showed higher percentages when there is international collaboration, being significantly different from the other categories. However, when the analysis was conducted inside the areas, the international collaboration was found to be different particularly in the soft sciences and a couple of other areas. The articles with DOI attributed, as well as those with mention of research funding, had a significantly higher percentage, even in the interaction with the areas of knowledge. Among the open access routes the green routes showed the highest percentages, followed by golden (DOAJ and other) and Bronze, but green routes articles proved to be not significantly different from those not openly accessible. Finally, the principal collaborating countries also showed the greatest influence on the DOI attribution, with the exception of Peru and South Africa. Our findings provide evidence that studies on the availability and usability of DOIs can assist researchers, by underlining the importance of making greater use of this persistent identifier, as well as to provide consistency to citation analysis.},
  file = {/home/gabriel/Dropbox/zotero-library/Mugnaini et al_2021_Openness trends in Brazilian citation data.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {3}
}

@article{oh2009,
  title = {Is the Journal Impact Factor a Valid Indicator of Scientific Value?},
  author = {Oh, H. C. and Lim, J. F.},
  year = {2009},
  month = aug,
  volume = {50},
  pages = {749--751},
  issn = {0037-5675},
  file = {/home/gabriel/Dropbox/zotero-library/Oh_Lim_2009_Is the journal impact factor a valid indicator of scientific value.pdf},
  journal = {Singapore Medical Journal},
  keywords = {Bibliometrics,Biomedical Research,Databases; Bibliographic,Journal Impact Factor,Periodicals as Topic,Publishing,todo},
  language = {eng},
  number = {8},
  pmid = {19710968}
}

@article{pagano2014,
  title = {The Crisis of Intellectual Monopoly Capitalism},
  author = {Pagano, Ugo},
  year = {2014},
  month = nov,
  volume = {38},
  pages = {1409--1429},
  issn = {0309-166X},
  doi = {10.1093/cje/beu025},
  abstract = {The last three decades have witnessed the emergence of a new species of capitalism. In spite of marked differences between its national varieties, a common characteristic of this species can be found in the global monopolisation of knowledge. This monopolisation involves hierarchical relations among firms and between capital and labour, because the capital of some firms includes the exclusive ownership of much of the knowledge used in production. Since the 1994 Trade-Related Aspects of Intellectual Property Rights agreements, the growing commoditisation of knowledge has extended the role of closed science and closed markets at the expense of open science and open markets. The intrinsic long-term dynamics of this species of capitalism is increasingly characterised by financialisation, inequality and stagnation. In order to exit from the current crisis, we must change many features of intellectual monopoly capitalism and rely on an eclectic approach that draws insights from liberal, Keynesian and Marxian traditions.},
  file = {/home/gabriel/Dropbox/zotero-library/Pagano_2014_The crisis of intellectual monopoly capitalism.pdf},
  journal = {Cambridge Journal of Economics},
  keywords = {todo},
  number = {6}
}

@article{pan2018,
  title = {Examining the Usage, Citation, and Diffusion Patterns of Bibliometric Mapping Software: {{A}} Comparative Study of Three Tools},
  shorttitle = {Examining the Usage, Citation, and Diffusion Patterns of Bibliometric Mapping Software},
  author = {Pan, Xuelian and Yan, Erjia and Cui, Ming and Hua, Weina},
  year = {2018},
  month = may,
  volume = {12},
  pages = {481--493},
  issn = {1751-1577},
  doi = {10.1016/j.joi.2018.03.005},
  abstract = {This study investigates the use, citation and diffusion of three bibliometric mapping software tools (CiteSpace, HistCite and VOSviewer) in scientific papers. We first conduct a content analysis of a sample of 481 English core journal papers\textemdash i.e., papers from journals deemed central to their respective disciplines\textemdash in which at least one of these tools is mentioned. This allows us to understand the predominant mention and citation practices surrounding these tools. We then employ several diffusion indicators to gain insight into the diffusion patterns of the three software tools. Overall, we find that researchers mention and cite the tools in diverse ways, many of which fall short of a traditional formal citation. Our results further indicate a clear upward trend in the use of all three tools, though VOSviewer is more frequently used than CiteSpace or HistCite. We also find that these three software tools have seen the fastest and most widespread adoption in library and information science research, where the tools originated. They have since been gradually adopted in other areas of study, initially at a lower diffusion speed but afterward at a rapidly growing rate.},
  file = {/home/gabriel/Dropbox/zotero-library/Pan et al_2018_Examining the usage, citation, and diffusion patterns of bibliometric mapping.pdf},
  journal = {Journal of Informetrics},
  keywords = {Bibliometric mapping software,Bibliometrics,Knowledge diffusion,Scholarly communication,Software citation},
  language = {en},
  number = {2}
}

@article{pendlebury2009,
  title = {The Use and Misuse of Journal Metrics and Other Citation Indicators},
  author = {Pendlebury, David A.},
  year = {2009},
  month = feb,
  volume = {57},
  pages = {1--11},
  issn = {0004-069X, 1661-4917},
  doi = {10.1007/s00005-009-0008-y},
  abstract = {This article reviews the nature and use of the journal impact factor and other common bibliometric measures for assessing research in the sciences and social sciences based on data compiled by Thomson Reuters. Journal impact factors are frequently misused to assess the influence of individual papers and authors, but such uses were never intended. Thomson Reuters also employs other measures of journal influence, which are contrasted with the impact factor. Finally, the author comments on the proper use of citation data in general, often as a supplement to peer review. This review may help government policymakers, university administrators, and individual researchers become better acquainted with the potential benefits and limitations of bibliometrics in the evaluation of research.},
  file = {/home/gabriel/Dropbox/zotero-library/Archivum Immunologiae et Therapiae Experimentalis/2009/Pendlebury_2009_The use and misuse of journal metrics and other citation indicators.pdf},
  journal = {Archivum Immunologiae et Therapiae Experimentalis},
  keywords = {fav,top5},
  language = {en},
  number = {1}
}

@article{peoples2016,
  title = {Twitter {{Predicts Citation Rates}} of {{Ecological Research}}},
  author = {Peoples, Brandon K. and Midway, Stephen R. and Sackett, Dana and Lynch, Abigail and Cooney, Patrick B.},
  year = {2016},
  month = nov,
  volume = {11},
  pages = {e0166570},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0166570},
  abstract = {The relationship between traditional metrics of research impact (e.g., number of citations) and alternative metrics (altmetrics) such as Twitter activity are of great interest, but remain imprecisely quantified. We used generalized linear mixed modeling to estimate the relative effects of Twitter activity, journal impact factor, and time since publication on Web of Science citation rates of 1,599 primary research articles from 20 ecology journals published from 2012\textendash 2014. We found a strong positive relationship between Twitter activity (i.e., the number of unique tweets about an article) and number of citations. Twitter activity was a more important predictor of citation rates than 5-year journal impact factor. Moreover, Twitter activity was not driven by journal impact factor; the `highest-impact' journals were not necessarily the most discussed online. The effect of Twitter activity was only about a fifth as strong as time since publication; accounting for this confounding factor was critical for estimating the true effects of Twitter use. Articles in impactful journals can become heavily cited, but articles in journals with lower impact factors can generate considerable Twitter activity and also become heavily cited. Authors may benefit from establishing a strong social media presence, but should not expect research to become highly cited solely through social media promotion. Our research demonstrates that altmetrics and traditional metrics can be closely related, but not identical. We suggest that both altmetrics and traditional citation rates can be useful metrics of research impact.},
  file = {/home/gabriel/Dropbox/zotero-library/Peoples et al_2016_Twitter Predicts Citation Rates of Ecological Research.pdf},
  journal = {PLOS ONE},
  keywords = {Altmetrics,Bibliometrics,Citation analysis,Conservation science,Ecology,Scientific publishing,Social media,Twitter},
  language = {en},
  number = {11}
}

@article{robinson-garcia2019,
  title = {The Many Faces of Mobility: {{Using}} Bibliometric Data to Measure the Movement of Scientists},
  shorttitle = {The Many Faces of Mobility},
  author = {{Robinson-Garcia}, Nicol{\'a}s and Sugimoto, Cassidy R. and Murray, Dakota and {Yegros-Yegros}, Alfredo and Larivi{\`e}re, Vincent and Costas, Rodrigo},
  year = {2019},
  month = feb,
  volume = {13},
  pages = {50--63},
  issn = {1751-1577},
  doi = {10.1016/j.joi.2018.11.002},
  abstract = {This paper presents a methodological framework for developing scientific mobility indicators based on bibliometric data. We identify nearly 16 million individual authors from publications covered in the Web of Science for the 2008\textendash 2015 period. Based on the information provided across individuals' publication records, we propose a general classification for analyzing scientific mobility using institutional affiliation changes. We distinguish between migrants--authors who have ruptures with their country of origin--and travelers--authors who gain additional affiliations while maintaining affiliation with their country of origin. We find that 3.7\% of researchers who have published at least one paper over the period are mobile. Travelers represent 72.7\% of all mobile scholars, but migrants have higher scientific impact. We apply this classification at the country level, expanding the classification to incorporate the directionality of scientists' mobility (i.e., incoming and outgoing). We provide a brief analysis to highlight the utility of the proposed taxonomy to study scholarly mobility and discuss the implications for science policy.},
  file = {/home/gabriel/Dropbox/zotero-library/Robinson-Garcia et al_2019_The many faces of mobility.pdf},
  journal = {Journal of Informetrics},
  keywords = {Bibliometrics,Brain circulation,Brain drain,International mobility,Science policy,Scientific mobility},
  language = {en},
  number = {1}
}

@article{roldan-valadez2019,
  title = {Current Concepts on Bibliometrics: A Brief Review about Impact Factor, {{Eigenfactor}} Score, {{CiteScore}}, {{SCImago Journal Rank}}, {{Source}}-{{Normalised Impact}} per {{Paper}}, {{H}}-Index, and Alternative Metrics},
  shorttitle = {Current Concepts on Bibliometrics},
  author = {{Roldan-Valadez}, Ernesto and {Salazar-Ruiz}, Shirley Yoselin and {Ibarra-Contreras}, Rafael and Rios, Camilo},
  year = {2019},
  month = aug,
  volume = {188},
  pages = {939--951},
  issn = {0021-1265, 1863-4362},
  doi = {10.1007/s11845-018-1936-5},
  abstract = {Background Understanding the impact of a publication by using bibliometric indices becomes an essential activity not only for universities and research institutes but also for individual academicians. This paper aims to provide a brief review of the current bibliometric tools used by authors and editors and proposes an algorithm to assess the relevance of the most common bibliometric tools to help the researchers select the fittest journal and know the trends of published submissions by using self-evaluation. Methods We present a narrative review answering at least two related consecutive questions triggered by the topics mentioned above. How prestigious is a journal based on its most recent bibliometrics, so authors may choose it to submit their next manuscript? And, how can they self-evaluate/understand the impact of their whole publishing scientific life? Results We presented the main relevant definitions of each bibliometrics and grouped them in those oriented to evaluated journals or individuals. Also, we share with our readers our algorithm to assess journals before manuscript submission. Conclusions Since there is a journal performance market and an article performance market, each one with its patterns, an integrative use of these metrics, rather than just the impact factor alone, might represent the fairest and most legitimate approach to assess the influence and importance of an acceptable research issue, and not only a sound journal in their respective disciplines.},
  file = {/home/gabriel/Dropbox/zotero-library/Irish Journal of Medical Science (1971 -)/2019/Roldan-Valadez et al_2019_Current concepts on bibliometrics.pdf},
  journal = {Irish Journal of Medical Science (1971 -)},
  keywords = {fav,top5},
  language = {en},
  number = {3}
}

@book{rousseau2018,
  title = {Becoming {{Metric}}-{{Wise}}: {{A Bibliometric Guide}} for {{Researchers}}},
  shorttitle = {Becoming {{Metric}}-{{Wise}}},
  author = {Rousseau, Ronald and Egghe, Leo and Guns, Raf},
  year = {2018},
  month = jan,
  publisher = {{Chandos Publishing}},
  abstract = {Becoming Metric-Wise: A Bibliometric Guide for Researchers aims to inform researchers about metrics so that they become aware of the evaluative techniques being applied to their scientific output. Understanding these concepts will help them during their funding initiatives, and in hiring and tenure. The book not only describes what indicators do (or are designed to do, which is not always the same thing), but also gives precise mathematical formulae so that indicators can be properly understood and evaluated. Metrics have become a critical issue in science, with widespread international discussion taking place on the subject across scientific journals and organizations.  As researchers should know the publication-citation context, the mathematical formulae of indicators being used by evaluating committees and their consequences, and how such indicators might be misused, this book provides an ideal tome on the topic.Provides researchers with a detailed understanding of bibliometric indicators and their applicationsEmpowers researchers looking to understand the indicators relevant to their work and careersPresents an informed and rounded picture of bibliometrics, including the strengths and shortcomings of particular indicatorsSupplies the mathematics behind bibliometric indicators so they can be properly understoodWritten by authors with longstanding expertise who are considered global leaders in the field of bibliometrics},
  file = {/home/gabriel/Dropbox/zotero-library/Rousseau et al_2018_Becoming Metric-Wise.pdf},
  googlebooks = {AissDwAAQBAJ},
  isbn = {978-0-08-102475-1},
  keywords = {Language Arts \& Disciplines / Library \& Information Science / Administration \& Management,Language Arts \& Disciplines / Library \& Information Science / General},
  language = {en}
}

@article{salager-meyer2008,
  title = {Scientific Publishing in Developing Countries: {{Challenges}} for the Future},
  shorttitle = {Scientific Publishing in Developing Countries},
  author = {{Salager-Meyer}, Fran{\c c}oise},
  year = {2008},
  month = apr,
  volume = {7},
  pages = {121--132},
  issn = {1475-1585},
  doi = {10.1016/j.jeap.2008.03.009},
  abstract = {In this paper, I first refer to the center-periphery dichotomy in terms of scientific output, placing emphasis upon the relation that exists between science and technology development, on the one hand, and social and economic development, on the other. I then analyze the main problems faced by most peripheral journals and the role nation states play in scientific activities in developing countries. I then address issues such as the world power structures, the social organization of developing countries, growing North/South disparities and the question of collaborative research. The discursive (i.e., language related) and non-discursive problems faced by researchers in periphery countries and the main initiatives that have recently been taken to try to solve the stark disparities that exist in the world of scholarly publishing are also discussed. I finally present a proposal, the aim of which is to suggest ways that could help scientists in periphery countries become fully integrated members of the worldwide network of science and would also contribute to the promotion of scientific multilingualism, a means for science to be truly universal, as it should be. I~conclude by arguing that science, technology and publication form a triad which is essential for the survival of developing nations, and that, although the complete elimination of inequities in the world of scholarship is unlikely, progress could be achieved if there were a universal will (i.e., a worldwide will at the institutional, governmental and intergovernmental levels) to redress the current North/South imbalance.},
  file = {/home/gabriel/Dropbox/zotero-library/Salager-Meyer_2008_Scientific publishing in developing countries.pdf},
  journal = {Journal of English for Academic Purposes},
  keywords = {Linguistic imperialism,Local/small journals,NNES scientists,Periphery,Research,Scientific multilingualism,todo},
  language = {en},
  number = {2},
  series = {English for {{Research Publication Purposes}}}
}

@article{shelton2012,
  title = {Publish or Patent: {{Bibliometric}} Evidence for Empirical Trade-Offs in National Funding Strategies},
  shorttitle = {Publish or Patent},
  author = {Shelton, R. D. and Leydesdorff, Loet},
  year = {2012},
  volume = {63},
  pages = {498--511},
  issn = {1532-2890},
  doi = {10.1002/asi.21677},
  abstract = {Multivariate linear regression models suggest a trade-off in allocations of national research and development (R\&D). Government funding and spending in the higher education sector encourage publications as a long-term research benefit. Conversely, other components such as industrial funding and spending in the business sector encourage patenting. Our results help explain why the United States trails the European Union in publications: The focus in the United States is on industrial funding\textemdash some 70\% of its total R\&D investment. Likewise, our results also help explain why the European Union trails the United States in patenting, since its focus on government funding is less effective than industrial funding in predicting triadic patenting. Government funding contributes negatively to patenting in a multiple regression, and this relationship is significant in the case of triadic patenting. We provide new forecasts about the relationships of the United States, the European Union, and China for publishing; these results suggest much later dates for changes than previous forecasts because Chinese growth has been slowing down since 2003. Models for individual countries might be more successful than regression models whose parameters are averaged over a set of countries because nations can be expected to differ historically in terms of the institutional arrangements and funding schemes.},
  annotation = {\_eprint: https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.21677},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of the American Society for Information Science and Technology/2012/Shelton_Leydesdorff_2012_Publish or patent.pdf},
  journal = {Journal of the American Society for Information Science and Technology},
  language = {en},
  number = {3}
}

@article{subramanyam1983,
  title = {Bibliometric Studies of Research Collaboration: {{A}} Review},
  shorttitle = {Bibliometric Studies of Research Collaboration},
  author = {Subramanyam, K.},
  year = {1983},
  month = jan,
  volume = {6},
  pages = {33--38},
  issn = {0165-5515, 1741-6485},
  doi = {10.1177/016555158300600105},
  abstract = {Scientific research is becoming an increasingly collaborative endeavour. The nature and magnitude of collaboration vary from one discipline to another, and depend upon such factors as the nature of the research problem, the research environ ment, and demographic factors. Earlier studies have shown a high degree of correlation between collaboration and research productivity, and between collaboration and financial support for research. The extent of collaboration cannot be easily determined by traditional methods of survey and observation. Bibliometric methods offer a convenient and non-reactive tool for studying collaboration in research. In this paper, several types of collaboration have been identified, and earlier research on collaboration has been reviewed. Further research is needed to refine the methods of defining and assessing collaboration and its impact on the organization of research and communica tion in science.},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of Information Science/1983/Subramanyam_1983_Bibliometric studies of research collaboration.pdf},
  journal = {Journal of Information Science},
  keywords = {fav},
  language = {en},
  number = {1}
}

@article{tahamtan2016,
  title = {Factors Affecting Number of Citations: A Comprehensive Review of the Literature},
  shorttitle = {Factors Affecting Number of Citations},
  author = {Tahamtan, Iman and Safipour Afshar, Askar and Ahamdzadeh, Khadijeh},
  year = {2016},
  month = jun,
  volume = {107},
  pages = {1195--1225},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-016-1889-2},
  file = {/home/gabriel/Dropbox/zotero-library/Tahamtan et al_2016_Factors affecting number of citations.pdf},
  journal = {Scientometrics},
  keywords = {fav},
  language = {en},
  number = {3}
}

@article{thelwall2013,
  title = {Do {{Altmetrics Work}}? {{Twitter}} and {{Ten Other Social Web Services}}},
  shorttitle = {Do {{Altmetrics Work}}?},
  author = {Thelwall, Mike and Haustein, Stefanie and Larivi{\`e}re, Vincent and Sugimoto, Cassidy R.},
  year = {2013},
  month = may,
  volume = {8},
  pages = {e64841},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0064841},
  abstract = {Altmetric measurements derived from the social web are increasingly advocated and used as early indicators of article impact and usefulness. Nevertheless, there is a lack of systematic scientific evidence that altmetrics are valid proxies of either impact or utility although a few case studies have reported medium correlations between specific altmetrics and citation rates for individual journals or fields. To fill this gap, this study compares 11 altmetrics with Web of Science citations for 76 to 208,739 PubMed articles with at least one altmetric mention in each case and up to 1,891 journals per metric. It also introduces a simple sign test to overcome biases caused by different citation and usage windows. Statistically significant associations were found between higher metric scores and higher citations for articles with positive altmetric scores in all cases with sufficient evidence (Twitter, Facebook wall posts, research highlights, blogs, mainstream media and forums) except perhaps for Google+ posts. Evidence was insufficient for LinkedIn, Pinterest, question and answer sites, and Reddit, and no conclusions should be drawn about articles with zero altmetric scores or the strength of any correlation between altmetrics and citations. Nevertheless, comparisons between citations and metric values for articles published at different times, even within the same year, can remove or reverse this association and so publishers and scientometricians should consider the effect of time when using altmetrics to rank articles. Finally, the coverage of all the altmetrics except for Twitter seems to be low and so it is not clear if they are prevalent enough to be useful in practice.},
  file = {/home/gabriel/Dropbox/zotero-library/Thelwall et al_2013_Do Altmetrics Work.pdf},
  journal = {PLOS ONE},
  keywords = {Altmetrics,Bibliometrics,Citation analysis,Libraries,Medical journals,Scientific publishing,Social media,todo,Twitter},
  language = {en},
  number = {5}
}

@article{thompson2015,
  title = {A {{Descriptive}} and {{Historical Review}} of {{Bibliometrics}} with {{Applications}} to {{Medical Sciences}}},
  author = {Thompson, Dennis F. and Walker, Cheri K.},
  year = {2015},
  volume = {35},
  pages = {551--559},
  issn = {1875-9114},
  doi = {10.1002/phar.1586},
  abstract = {The discipline of bibliometrics involves the application of mathematical and statistical methods to scholarly publications. The first attempts at systematic data collection were provided by Alfred Lotka and Samuel Bradford, who subsequently established the foundational laws of bibliometrics. Eugene Garfield ushered in the modern era of bibliometrics with the routine use of citation analysis and systematized processing. Key elements of bibliometric analysis include database coverage, consistency and accuracy of the data, data fields, search options, and analysis and use of metrics. A number of bibliometric applications are currently being used in medical science and health care. Bibliometric parameters and indexes may be increasingly used by grant funding sources as measures of research success. Universities may build benchmarking standards from bibliometric data to determine academic achievement through promotion and tenure guidelines in the future. This article reviews the history, definition, laws, and elements of bibliometric principles and provides examples of bibliometric applications to the broader health care community. To accomplish this, the Medline (1966\textendash 2014) and Web of Science (1945\textendash 2014) databases were searched to identify relevant articles; select articles were also cross-referenced. Articles selected were those that provided background, history, descriptive analysis, and application of bibliometric principles and metrics to medical science and health care. No attempt was made to cover all areas exhaustively; rather, key articles were chosen that illustrate bibliometric concepts and enhance the reader's knowledge. It is important that faculty and researchers understand the limitations and appropriate uses of bibliometric data. Bibliometrics has considerable potential as a research area for health care scientists and practitioners that can be used to discover new information about academic trends, pharmacotherapy, disease, and broader health sciences trends.},
  annotation = {\_eprint: https://accpjournals.onlinelibrary.wiley.com/doi/pdf/10.1002/phar.1586},
  copyright = {\textcopyright{} 2015 Pharmacotherapy Publications, Inc.},
  file = {/home/gabriel/Dropbox/zotero-library/Thompson_Walker_2015_A Descriptive and Historical Review of Bibliometrics with Applications to.pdf},
  journal = {Pharmacotherapy: The Journal of Human Pharmacology and Drug Therapy},
  keywords = {application,bibliometrics,history,laws,medicine,pharmacy,publications},
  language = {en},
  number = {6}
}

@book{todeschini2016,
  title = {Handbook of Bibliometric Indicators: Quantitative Tools for Studying and Evaluating Research},
  shorttitle = {Handbook of Bibliometric Indicators},
  author = {Todeschini, Roberto and Baccini, Alberto},
  year = {2016},
  publisher = {{Wiley-VCH Verlag GmbH \& Co.KGaA}},
  address = {{Weinheim}},
  annotation = {OCLC: 953444623},
  file = {/home/gabriel/Dropbox/zotero-library/Todeschini_Baccini_2016_Handbook of bibliometric indicators.pdf},
  isbn = {978-3-527-33704-0 978-3-527-68194-5 978-3-527-68195-2 978-3-527-68193-8 978-3-527-68196-9},
  language = {eng}
}

@article{torre,
  title = {{Nuovi indicatori bibliometrici nella letteratura scientifica: un panorama in continua evoluzione}},
  author = {Torre, G La and Sciarra, I and Chiappetta, M and Monteduro, A},
  pages = {7},
  abstract = {Introduction. Bibliometrics is a science which evaluates the impact of the scientific work of a journal or of an author, using mathematical and statistical tools. Impact Factor (IF) is the first bibliometric parameter created, and after it many others have been progressively conceived in order to go beyond its limits. Currently bibliometric indexes are used for academic purposes, among them to evaluate the eligibility of a researcher to compete for the National Scientific Qualification, in order to access to competitive exams to become professor. Objective. Aim of this study is to identify the most relevant bibliometric indexes and to summarized their characteristics. Methods. A revision of bibliometric indexes as been conducted, starting from the classic ones and completing with the most recent ones. Results. The two most used bibliometric indexes are the IF, which measures the scientific impact of a periodical and bases on Web of Science citation database, and the h-index, which measures the impact of the scientific work of a researcher, basing on Scopus database. Besides them other indexes have been created more recently, such as the SCImago Journal Rank Indicator (SJR), the Source Normalised Impact per Paper (SNIP) and the CiteScore index. They are all based on Scopus database and evaluate, in different ways, the citational impact of a periodic. The i10-index instead is provided from Google Scholar database and allows to evaluate the impact of the scientific production of a researcher. Recently two softwares have been introduced: the first one, Publish or Perish, allows to evaluate the scientific work of a researcher, through the assessment of many indexes; the second one, Altmetric, measure the use in the Web of the academic papers, instead of measuring citations, by means of alternative metrics respect to the traditional ones. Conclusions. Each analized index shows advantages but also criticalities. Therefore the combined use of more than one indexes, citational and not, should be preferred, in order to correctly evaluate the work of reserchers and to finally improve the quality and the development of scientific research. Clin Ter 2017; 168(2):e65-71. doi: 10.7417/CT.2017.1985},
  file = {/home/gabriel/Dropbox/zotero-library/undefined/undefined/Torre et al_Nuovi indicatori bibliometrici nella letteratura scientifica.pdf},
  language = {it}
}

@article{tran2019,
  title = {Global {{Evolution}} of {{Research}} in {{Artificial Intelligence}} in {{Health}} and {{Medicine}}: {{A Bibliometric Study}}},
  shorttitle = {Global {{Evolution}} of {{Research}} in {{Artificial Intelligence}} in {{Health}} and {{Medicine}}},
  author = {Tran, Bach and Vu, Giang and Ha, Giang and Vuong, Quan-Hoang and Ho, Manh-Tung and Vuong, Thu-Trang and La, Viet-Phuong and Ho, Manh-Toan and Nghiem, Kien-Cuong and Nguyen, Huong and Latkin, Carl and Tam, Wilson and Cheung, Ngai-Man and Nguyen, Hong-Kong and Ho, Cyrus and Ho, Roger},
  year = {2019},
  month = mar,
  volume = {8},
  pages = {360},
  issn = {2077-0383},
  doi = {10.3390/jcm8030360},
  abstract = {The increasing application of Artificial Intelligence (AI) in health and medicine has attracted a great deal of research interest in recent decades. This study aims to provide a global and historical picture of research concerning AI in health and medicine. A total of 27,451 papers that were published between 1977 and 2018 (84.6\% were dated 2008\textendash 2018) were retrieved from the Web of Science platform. The descriptive analysis examined the publication volume, and authors and countries collaboration. A global network of authors' keywords and content analysis of related scientific literature highlighted major techniques, including Robotic, Machine learning, Artificial neural network, Artificial intelligence, Natural language process, and their most frequent applications in Clinical Prediction and Treatment. The number of cancer-related publications was the highest, followed by Heart Diseases and Stroke, Vision impairment, Alzheimer's, and Depression. Moreover, the shortage in the research of AI application to some high burden diseases suggests future directions in AI research. This study offers a first and comprehensive picture of the global efforts directed towards this increasingly important and prolific field of research and suggests the development of global and national protocols and regulations on the justification and adaptation of medical AI products.},
  file = {/home/gabriel/Dropbox/zotero-library/Journal of Clinical Medicine/2019/Tran et al_2019_Global Evolution of Research in Artificial Intelligence in Health and Medicine.pdf},
  journal = {Journal of Clinical Medicine},
  language = {en},
  number = {3}
}

@article{vanleeuwen2001,
  title = {The Use of Combined Bibliometric Methods in Research Funding Policy},
  author = {{van Leeuwen}, T N and {van der Wurff}, L J and {van Raan}, A F J},
  year = {2001},
  month = dec,
  volume = {10},
  pages = {195--201},
  issn = {09582029, 14715449},
  doi = {10.3152/147154401781777015},
  file = {/home/gabriel/Dropbox/zotero-library/van Leeuwen et al_2001_The use of combined bibliometric methods in research funding policy.pdf},
  journal = {Research Evaluation},
  language = {en},
  number = {3}
}

@incollection{vanraan2019a,
  title = {Measuring {{Science}}: {{Basic Principles}} and {{Application}} of {{Advanced Bibliometrics}}},
  shorttitle = {Measuring {{Science}}},
  booktitle = {Springer {{Handbook}} of {{Science}} and {{Technology Indicators}}},
  author = {{van Raan}, Anthony},
  editor = {Gl{\"a}nzel, Wolfgang and Moed, Henk F. and Schmoch, Ulrich and Thelwall, Mike},
  year = {2019},
  pages = {237--280},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-02511-3_10},
  abstract = {We begin with a short history of measuring science and discuss how the Science Citation Index has revolutionized the quantitative study of science and created a strong application potential. After reviewing the rationale of bibliometric analysis, we present the basic principle of the bibliometric methodology, with complex citation networks as a starting point. We show that the two main pillars of advanced bibliometric methods, citation-based analysis and science mapping, are both reducible to one and the same principle. From this basic principle we deduce a set of main indicators, particularly for the assessment of research output and international impact. Important elements include new approaches for identifying fields and research themes on the basis of a publication-level rather than a journal-level network; publication and citation counting; normalization of citation measures; the use of indicators based on averages versus those based on citation distributions; and weighting procedures and statistical reliability. In this account of the state of the art of advanced bibliometrics , we highlight in particular the developments in our Leiden institute, given its long-standing, extensive, and broad experience.The next part of this chapter deals with practical applications of indicators, particularly real-life examples of evaluation studies. We further discuss several crucial issues such as the use of journal impact factors and h-index; the relation between peer review judgment and bibliometric findings; definition and delimitation of fields; assignment of publications; the influence of open access; webometrics and altmetrics; ranking of universities; and general objections to bibliometric analysis.The second main pillar of the advanced bibliometric methodology is the development of science maps. We discuss the basic elements and the construction of both citation-relation and word-relation science maps. Further, we present a method to combine the two main pillars: the integration of citation analysis in science maps. This combined citation analysis and science mapping can be used to explore research related to socioeconomic problems. Recently developed bibliometric instruments enable tunable mapping, which opens up new analytical opportunities in monitoring scientific research. Finally, we contend that bibliometric indicators and maps are not just evaluation tools for science policymakers, research managers, and individual researchers, but also powerful instruments in the study of science.},
  file = {/home/gabriel/Zotero/storage/D5CSCDT4/van Raan - 2019 - Measuring Science Basic Principles and Applicatio},
  isbn = {978-3-030-02511-3},
  keywords = {advanced bibliometrics,bibliometric networks,guidelines for the proper use of bibliometric indicators,history of scientometrics,indicators of research output and impact,measuring science,methodological and technical issues in bibliometics,practical application of bibliometric indicators,ranking of universities,science maps},
  language = {en},
  series = {Springer {{Handbooks}}}
}

@article{wallin2005,
  title = {Bibliometric {{Methods}}: {{Pitfalls}} and {{Possibilities}}},
  shorttitle = {Bibliometric {{Methods}}},
  author = {Wallin, Johan A.},
  year = {2005},
  volume = {97},
  pages = {261--275},
  issn = {1742-7843},
  doi = {10.1111/j.1742-7843.2005.pto_139.x},
  abstract = {Abstract: Bibliometric studies are increasingly being used for research assessment. Bibliometric indicators are strongly methodology-dependent but for all of them, various types of data normalization are an indispensable requirement. Bibliometric studies have many pitfalls; technical skill, critical sense and a precise knowledge about the examined scientific domain are required to carry out and interpret bibliometric investigations correctly.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1742-7843.2005.pto\_139.x},
  file = {/home/gabriel/Dropbox/zotero-library/Basic & Clinical Pharmacology & Toxicology/2005/Wallin_2005_Bibliometric Methods.pdf},
  journal = {Basic \& Clinical Pharmacology \& Toxicology},
  keywords = {done},
  language = {en},
  number = {5}
}

@article{waltman2016,
  title = {A Review of the Literature on Citation Impact Indicators},
  author = {Waltman, Ludo},
  year = {2016},
  month = may,
  volume = {10},
  pages = {365--391},
  issn = {1751-1577},
  doi = {10.1016/j.joi.2016.02.007},
  abstract = {Citation impact indicators nowadays play an important role in research evaluation, and consequently these indicators have received a lot of attention in the bibliometric and scientometric literature. This paper provides an in-depth review of the literature on citation impact indicators. First, an overview is given of the literature on bibliographic databases that can be used to calculate citation impact indicators (Web of Science, Scopus, and Google Scholar). Next, selected topics in the literature on citation impact indicators are reviewed in detail. The first topic is the selection of publications and citations to be included in the calculation of citation impact indicators. The second topic is the normalization of citation impact indicators, in particular normalization for field differences. Counting methods for dealing with co-authored publications are the third topic, and citation impact indicators for journals are the last topic. The paper concludes by offering some recommendations for future research.},
  file = {/home/gabriel/Dropbox/zotero-library/Waltman_2016_A review of the literature on citation impact indicators.pdf},
  journal = {Journal of Informetrics},
  keywords = {Bibliographic database,Citation analysis,Citation impact indicator,Counting method,fav,Normalization,todo},
  language = {en},
  number = {2}
}

@article{wang2019,
  title = {Which Can Better Predict the Future Success of Articles? {{Bibliometric}} Indices or Alternative Metrics},
  shorttitle = {Which Can Better Predict the Future Success of Articles?},
  author = {Wang, Mingyang and Wang, Zhenyu and Chen, Guangsheng},
  year = {2019},
  month = jun,
  volume = {119},
  pages = {1575--1595},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-019-03052-9},
  abstract = {In this paper, we made a survey on the prediction capability of bibliometric indices and alternative metrics on the future success of articles by establishing a machine learning framework. Twenty-three bibliometric and alternative indices were collected to establish the feature space for the predication task. In order to eliminate the possible redundancy in feature space, three feature selection techniques of Relief-F, principal component analysis and entropy weighted method were used to rank the features according to their contribution to the original data set. Combining the fractal dimension of the data set, the intrinsic features which can better represent the original feature space were extracted. Three classifiers of Na\"ive Bayes, KNN and random forest were performed to detect the classification performance of these features. Experimental results show that both bibliometric indices and alternative metrics are beneficial to articles' growth. Early citation features, early Web usage statistics, as well as the reputation of the first author are the most valuable indicators in making an article more influential in the future.},
  file = {/home/gabriel/Dropbox/zotero-library/Scientometrics/2019/Wang et al_2019_Which can better predict the future success of articles.pdf},
  journal = {Scientometrics},
  keywords = {done},
  language = {en},
  number = {3}
}

@article{wang2021,
  title = {Identifying 'seed' Papers in Sciences},
  author = {Wang, Jean J. and Shao, Sarah X. and Ye, Fred Y.},
  year = {2021},
  month = apr,
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03980-5},
  abstract = {A concise quantitative method is established for identifying `seed' papers in sciences. The method is set up following h-type metrics based on co-citation network analysis. With defining original-seed (O-Seed) and dominant-seed (D-Seed) by measurable h-strength and second-order h-type degree centrality, O-seed resembles to be a `root' and D-seed develops to become `stem'. Using dataset from Web of Science (WoS), the `seed' papers in research fields of graphene, genome editing, and h-set studies are identified. Graphene D-Seed paper and genome editing D-Seed paper are representative outputs of the 2010 Nobel Prize in Physics and the 2020 Nobel Prize in Chemistry respectively. H-set O-Seed and D-Seed are the same paper that first proposed the concept of h-index. The `seed' papers are characterized by not only high citations, but also network structure and core function in sciences.},
  file = {/home/gabriel/Dropbox/zotero-library/Wang et al_2021_Identifying 'seed' papers in sciences.pdf},
  journal = {Scientometrics},
  language = {en}
}

@article{zancanaro2015,
  title = {A Bibliometric Mapping of Open Educational Resources},
  author = {Zancanaro, Airton and Todesco, Jos{\'e} Leomar and Ramos, Fernando},
  year = {2015},
  month = jan,
  volume = {16},
  issn = {1492-3831},
  doi = {10.19173/irrodl.v16i1.1960},
  abstract = {Open educational resources (OER) is a topic that has aroused increasing interest by researchers as a powerful contribution to improve the educational system quality and openness, both in face to face and distance education. The goal of this research is to map publications related to OER, dating from 2002 to 2013, and available through the Web of Science and Scopus scientific databases as well as in the OER Knowledge Cloud open repository. Data were used to explore relevant aspects related to the scientific production in OER, such as: (i) number of publications per year; (ii) most cited publications; (iii) authors with higher number of publications; (iv) institutions and countries with more publications and (v) most referenced bibliography by the authors. The analysis has included 544 papers, written by 843 authors, from 338 institutions, from 61 different countries. Moreover, the analysis has included the publications referenced and the author's keywords, considering 6,355 different publications and 929 different keywords. Besides presenting a bibliographic mapping of the research on OER, this paper also intends to contribute to consolidate the idea that OER is a promising field for researchers, in line with the spreading of the Open movement.},
  file = {/home/gabriel/Dropbox/zotero-library/The International Review of Research in Open and Distributed Learning/2015/Zancanaro et al_2015_A bibliometric mapping of open educational resources.pdf},
  journal = {The International Review of Research in Open and Distributed Learning},
  language = {en},
  number = {1}
}

@article{zhang2021,
  title = {Does Open Data Boost Journal Impact: Evidence from {{Chinese}} Economics},
  shorttitle = {Does Open Data Boost Journal Impact},
  author = {Zhang, Liwei and Ma, Liang},
  year = {2021},
  month = apr,
  volume = {126},
  pages = {3393--3419},
  issn = {1588-2861},
  doi = {10.1007/s11192-021-03897-z},
  abstract = {To encourage research transparency and replication, more and more journals have been requiring authors to share original datasets and analytic procedures supporting their publications. Does open data boost journal impact? In this article, we report one of the first empirical studies to assess the effects of open data on journal impact. China Industrial Economics (CIE) mandated authors to open their research data in the end of 2016, which is the first to embrace open data among Chinese journals and provides a natural experiment for policy evaluation. We use the data of 37 Chinese economics journals from 2001 to 2019 and apply synthetic control method to causally estimate the effects of open data, and our results show that open data has significantly increased the citations of journal articles. On average, the current- and second-year citations of articles published with CIE have increased by 1\,\textasciitilde\,4 times, and articles published before the open data policy also benefited from the spillover effect. Our findings suggest that journals can leverage compulsory open data to develop reputation and amplify academic impacts.},
  file = {/home/gabriel/Dropbox/zotero-library/Zhang_Ma_2021_Does open data boost journal impact.pdf},
  journal = {Scientometrics},
  language = {en},
  number = {4}
}

@misc{zotero-886,
  title = {{{SCIENTOMETRICS}}, {{TECHNIQUES}}, {{SOURCES AND THEIR KEY POINTS TO ANALYSIS OF LIS RESEARCH}}: {{AN OVERVIEW}}},
  file = {/home/gabriel/Dropbox/zotero-library/SCIENTOMETRICS, TECHNIQUES, SOURCES AND THEIR KEY POINTS TO ANALYSIS OF LIS.pdf},
  howpublished = {https://scholar.googleusercontent.com/scholar?q=cache:yGZ8PRdKnlUJ:scholar.google.com/+softwares+in+scientometrics+research\&hl=en\&as\_sdt=0,5\&as\_ylo=2017},
  keywords = {todo}
}

@misc{zotero-891,
  title = {Scientometrics of {{Scientometrics}}: {{Mapping Historical Footprint}} and {{Emerging Technologies}} in {{Scientometrics}} | {{IntechOpen}}},
  file = {/home/gabriel/Zotero/storage/JCW335CV/Scientometrics of Scientometrics Mapping Historic.pdf},
  howpublished = {https://www.intechopen.com/books/scientometrics/scientometrics-of-scientometrics-mapping-historical-footprint-and-emerging-technologies-in-scientome},
  keywords = {todo}
}

@misc{zotero-904,
  title = {Scientometrics - {{Google Books}}},
  howpublished = {https://books.google.com.br/books?hl=en\&lr=\&id=fy6RDwAAQBAJ\&oi=fnd\&pg=PA9\&dq=scientometrics+history\&ots=JdBDtDm1dv\&sig=q08qlZYAf0noJpk6IAZP125rdaQ\#v=onepage\&q=scientometrics\%20history\&f=false}
}

@misc{zotero-983,
  title = {{{BLDE University Journal}} of {{Health Sciences}} - {{Citation}} Impact: {{Manipulation}} and Monopoly : {{Download PDF}}},
  howpublished = {https://www.bldeujournalhs.in/downloadpdf.asp?issn=2468-838X;year=2017;volume=2;issue=2;spage=67;epage=68;aulast=Das;type=2}
}


